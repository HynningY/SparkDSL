11:46:48.496 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
11:46:48.735 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
11:46:48.832 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_customer_job
11:46:48.860 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: mx
11:46:48.861 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: mx
11:46:48.862 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
11:46:48.863 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
11:46:48.864 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(mx); groups with view permissions: Set(); users  with modify permissions: Set(mx); groups with modify permissions: Set()
11:46:49.587 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 62461.
11:46:49.602 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
11:46:49.617 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
11:46:49.620 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
11:46:49.620 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
11:46:49.630 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\mx\AppData\Local\Temp\blockmgr-ba9f3e3d-a4cf-4689-ac97-4a277f0307ce
11:46:49.653 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
11:46:49.696 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
11:46:49.785 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @3955ms
11:46:49.853 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
11:46:49.873 INFO  [main] org.spark_project.jetty.server.Server - Started @4044ms
11:46:49.894 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@39fc6b2c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
11:46:49.894 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
11:46:49.934 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67318f{/jobs,null,AVAILABLE,@Spark}
11:46:49.935 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2237bada{/jobs/json,null,AVAILABLE,@Spark}
11:46:49.936 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5710768a{/jobs/job,null,AVAILABLE,@Spark}
11:46:49.943 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64d7b720{/jobs/job/json,null,AVAILABLE,@Spark}
11:46:49.944 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bb3d42d{/stages,null,AVAILABLE,@Spark}
11:46:49.945 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/stages/json,null,AVAILABLE,@Spark}
11:46:49.945 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e7095ac{/stages/stage,null,AVAILABLE,@Spark}
11:46:49.946 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51671b08{/stages/stage/json,null,AVAILABLE,@Spark}
11:46:49.947 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1162410a{/stages/pool,null,AVAILABLE,@Spark}
11:46:49.947 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62df0ff3{/stages/pool/json,null,AVAILABLE,@Spark}
11:46:49.948 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62e8f862{/storage,null,AVAILABLE,@Spark}
11:46:49.949 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c49fab6{/storage/json,null,AVAILABLE,@Spark}
11:46:49.950 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74518890{/storage/rdd,null,AVAILABLE,@Spark}
11:46:49.951 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f3ddbd9{/storage/rdd/json,null,AVAILABLE,@Spark}
11:46:49.952 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c2d4cc6{/environment,null,AVAILABLE,@Spark}
11:46:49.952 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6134ac4a{/environment/json,null,AVAILABLE,@Spark}
11:46:49.953 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71b1a49c{/executors,null,AVAILABLE,@Spark}
11:46:49.954 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3773862a{/executors/json,null,AVAILABLE,@Spark}
11:46:49.955 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@589b028e{/executors/threadDump,null,AVAILABLE,@Spark}
11:46:49.956 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9fecdf1{/executors/threadDump/json,null,AVAILABLE,@Spark}
11:46:49.963 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b0f7d9d{/static,null,AVAILABLE,@Spark}
11:46:49.964 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6594402a{/,null,AVAILABLE,@Spark}
11:46:49.966 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@405325cf{/api,null,AVAILABLE,@Spark}
11:46:49.967 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@669d2b1b{/jobs/job/kill,null,AVAILABLE,@Spark}
11:46:49.967 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ea9f009{/stages/stage/kill,null,AVAILABLE,@Spark}
11:46:49.969 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://10.0.152.153:4040
11:46:50.080 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
11:46:50.169 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62482.
11:46:50.170 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 10.0.152.153:62482
11:46:50.173 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
11:46:50.197 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 10.0.152.153, 62482, None)
11:46:50.202 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 10.0.152.153:62482 with 1988.7 MB RAM, BlockManagerId(driver, 10.0.152.153, 62482, None)
11:46:50.209 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 10.0.152.153, 62482, None)
11:46:50.210 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 10.0.152.153, 62482, None)
11:46:50.524 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@415156bf{/metrics/json,null,AVAILABLE,@Spark}
11:46:50.560 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/Java/JetBrains/release_test/target/classes/hive-site.xml
11:46:50.593 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('hdfs://hdfsCluster/hive/db').
11:46:50.594 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'hdfs://hdfsCluster/hive/db'.
11:46:50.599 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5f303ecd{/SQL,null,AVAILABLE,@Spark}
11:46:50.599 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25a73de1{/SQL/json,null,AVAILABLE,@Spark}
11:46:50.600 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@260f2144{/SQL/execution,null,AVAILABLE,@Spark}
11:46:50.601 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51827393{/SQL/execution/json,null,AVAILABLE,@Spark}
11:46:50.603 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f96b85{/static/sql,null,AVAILABLE,@Spark}
11:46:51.492 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
11:46:52.086 WARN  [main] org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.metastore.local does not exist
11:46:52.207 INFO  [main] hive.metastore - Trying to connect to metastore with URI thrift://node242:9083
11:46:53.129 INFO  [main] hive.metastore - Connected to metastore.
11:46:57.418 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/mx/AppData/Local/Temp/33538692-f50d-47dd-aa50-a90a2bc1e1dd_resources
11:46:57.449 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/mx/33538692-f50d-47dd-aa50-a90a2bc1e1dd
11:46:57.498 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/mx/AppData/Local/Temp/mx/33538692-f50d-47dd-aa50-a90a2bc1e1dd
11:46:57.526 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/mx/33538692-f50d-47dd-aa50-a90a2bc1e1dd/_tmp_space.db
11:46:57.530 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is hdfs://hdfsCluster/hive/db
11:46:57.739 WARN  [main] org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.metastore.local does not exist
11:46:57.866 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/mx/AppData/Local/Temp/5a78144c-2a00-40bd-9240-ed1ab99cfc25_resources
11:46:57.888 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/mx/5a78144c-2a00-40bd-9240-ed1ab99cfc25
11:46:57.942 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/mx/AppData/Local/Temp/mx/5a78144c-2a00-40bd-9240-ed1ab99cfc25
11:46:58.000 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/mx/5a78144c-2a00-40bd-9240-ed1ab99cfc25/_tmp_space.db
11:46:58.004 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is hdfs://hdfsCluster/hive/db
11:46:58.028 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
11:46:59.610 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:47:00.193 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:00.206 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:00.207 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:00.207 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:00.207 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:00.208 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:00.208 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:00.208 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:00.209 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:00.209 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:47:00.224 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
11:47:00.556 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:47:00.571 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:47:00.571 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num 
11:47:00.573 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:47:00.573 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources  
11:47:00.573 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels  
11:47:00.574 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard 
11:47:00.586 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\d{6})(\d{4})',2)as int)) as age 
11:47:00.645 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\d{16})(\d{1})',2)as int) % 2 as gender 
11:47:00.649 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code 
11:47:00.650 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude 
11:47:00.651 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude 
11:47:00.652 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id 
11:47:00.653 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code 
11:47:00.654 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version 
11:47:00.654 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid 
11:47:00.655 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:47:00.655 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:47:03.538 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:03.538 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:03.539 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:03.539 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:03.539 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:03.539 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:03.539 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:03.539 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:03.539 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:03.540 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:47:03.851 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190614)
11:47:03.855 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
11:47:03.860 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:47:03.875 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
11:47:03.884 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:47:04.045 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
11:47:04.388 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 287.710769 ms
11:47:04.663 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 72.363077 ms
11:47:04.752 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 316.7 KB, free 1988.4 MB)
11:47:04.886 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.3 KB, free 1988.4 MB)
11:47:04.894 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 10.0.152.153:62482 (size: 27.3 KB, free: 1988.7 MB)
11:47:04.897 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DWReleaseCustomer.scala:48
11:47:04.905 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 117218374 bytes, open cost is considered as scanning 4194304 bytes.
11:47:05.016 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:48
11:47:05.030 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at DWReleaseCustomer.scala:48)
11:47:05.032 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DWReleaseCustomer.scala:48) with 1 output partitions
11:47:05.032 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DWReleaseCustomer.scala:48)
11:47:05.033 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
11:47:05.034 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
11:47:05.038 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DWReleaseCustomer.scala:48), which has no missing parents
11:47:05.155 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 23.8 KB, free 1988.3 MB)
11:47:05.158 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.3 KB, free 1988.3 MB)
11:47:05.159 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 10.0.152.153:62482 (size: 10.3 KB, free: 1988.7 MB)
11:47:05.160 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
11:47:05.177 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DWReleaseCustomer.scala:48) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:47:05.179 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 4 tasks
11:47:05.223 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5388 bytes)
11:47:05.224 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5388 bytes)
11:47:05.225 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5388 bytes)
11:47:05.225 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5388 bytes)
11:47:05.232 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
11:47:05.232 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
11:47:05.233 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
11:47:05.232 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
11:47:05.424 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 83.498256 ms
11:47:05.448 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hdfsCluster/data/release/ods/release_session/bdp_day=20190614/1560478440000m3cdfa4d, range: 117218374-230215221, partition values: [20190614]
11:47:05.448 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hdfsCluster/data/release/ods/release_session/bdp_day=20190614/1560478440000m3cdfa4d, range: 0-117218374, partition values: [20190614]
11:47:05.448 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hdfsCluster/data/release/ods/release_session/bdp_day=20190614/1560453600000ci726eb9, range: 0-117218374, partition values: [20190614]
11:47:05.455 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hdfsCluster/data/release/ods/release_session/bdp_day=20190614/1560453600000ci726eb9, range: 117218374-230269670, partition values: [20190614]
11:47:06.584 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:47:06.584 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:47:06.584 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:47:06.584 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:48:40.367 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1825 bytes result sent to driver
11:48:40.504 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 95278 ms on localhost (executor driver) (1/4)
11:48:41.573 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1782 bytes result sent to driver
11:48:41.576 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 96351 ms on localhost (executor driver) (2/4)
11:48:54.453 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1782 bytes result sent to driver
11:48:54.457 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 109244 ms on localhost (executor driver) (3/4)
11:49:08.299 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1782 bytes result sent to driver
11:49:08.304 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 123080 ms on localhost (executor driver) (4/4)
11:49:08.309 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (show at DWReleaseCustomer.scala:48) finished in 123.102 s
11:49:08.310 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:49:08.311 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:49:08.312 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
11:49:08.312 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:49:08.312 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
11:49:08.315 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:48), which has no missing parents
11:49:08.322 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1988.3 MB)
11:49:08.323 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.3 MB)
11:49:08.324 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 10.0.152.153:62482 (size: 2.2 KB, free: 1988.7 MB)
11:49:08.324 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
11:49:08.326 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:48) (first 15 tasks are for partitions Vector(0))
11:49:08.326 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
11:49:08.327 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 4, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:49:08.328 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 4)
11:49:08.343 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 non-empty blocks out of 4 blocks
11:49:08.344 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
11:49:08.396 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 4). 2285 bytes result sent to driver
11:49:08.397 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 4) in 71 ms on localhost (executor driver) (1/1)
11:49:08.397 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
11:49:08.397 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DWReleaseCustomer.scala:48) finished in 0.071 s
11:49:08.403 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DWReleaseCustomer.scala:48, took 123.387516 s
11:49:08.429 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
11:49:08.450 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:08.450 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:08.450 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:08.450 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:08.450 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:08.451 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:08.451 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:08.451 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:08.451 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
11:49:08.451 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:08.451 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:08.451 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:08.452 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:08.452 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:08.452 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:08.452 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:08.452 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:08.452 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:49:08.678 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hdfsCluster/data/release/dw/release_customer/.hive-staging_hive_2019-09-10_11-49-08_677_6563403111305353427-1
11:49:09.102 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:09.102 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:09.102 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:09.103 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:09.103 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:09.104 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:09.104 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:09.104 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:09.104 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:09.104 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:49:09.175 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190614)
11:49:09.176 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
11:49:09.176 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:49:09.176 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
11:49:09.176 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:49:09.236 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:49:09.240 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:49:09.443 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 34.910358 ms
11:49:09.473 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 316.7 KB, free 1988.0 MB)
11:49:09.521 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 27.3 KB, free 1988.0 MB)
11:49:09.522 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 10.0.152.153:62482 (size: 27.3 KB, free: 1988.6 MB)
11:49:09.523 INFO  [main] org.apache.spark.SparkContext - Created broadcast 3 from insertInto at SparkHelper.scala:36
11:49:09.523 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 117218374 bytes, open cost is considered as scanning 4194304 bytes.
11:49:09.667 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:36
11:49:09.668 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (insertInto at SparkHelper.scala:36)
11:49:09.668 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (insertInto at SparkHelper.scala:36) with 4 output partitions
11:49:09.668 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (insertInto at SparkHelper.scala:36)
11:49:09.668 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
11:49:09.669 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
11:49:09.669 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:36), which has no missing parents
11:49:09.675 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 23.8 KB, free 1988.0 MB)
11:49:09.679 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.3 KB, free 1988.0 MB)
11:49:09.681 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 10.0.152.153:62482 (size: 10.3 KB, free: 1988.6 MB)
11:49:09.681 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1004
11:49:09.686 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:36) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:49:09.686 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 4 tasks
11:49:09.688 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 5, localhost, executor driver, partition 0, ANY, 5388 bytes)
11:49:09.688 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 6, localhost, executor driver, partition 1, ANY, 5388 bytes)
11:49:09.688 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 2.0 (TID 7, localhost, executor driver, partition 2, ANY, 5388 bytes)
11:49:09.766 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 2.0 (TID 8, localhost, executor driver, partition 3, ANY, 5388 bytes)
11:49:09.793 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 5)
11:49:09.826 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hdfsCluster/data/release/ods/release_session/bdp_day=20190614/1560453600000ci726eb9, range: 0-117218374, partition values: [20190614]
11:49:09.826 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 6)
11:49:09.827 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 2.0 in stage 2.0 (TID 7)
11:49:09.828 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 3.0 in stage 2.0 (TID 8)
11:49:09.848 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hdfsCluster/data/release/ods/release_session/bdp_day=20190614/1560453600000ci726eb9, range: 117218374-230269670, partition values: [20190614]
11:49:09.890 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hdfsCluster/data/release/ods/release_session/bdp_day=20190614/1560478440000m3cdfa4d, range: 0-117218374, partition values: [20190614]
11:49:09.902 INFO  [Executor task launch worker for task 8] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hdfsCluster/data/release/ods/release_session/bdp_day=20190614/1560478440000m3cdfa4d, range: 117218374-230215221, partition values: [20190614]
11:49:09.926 INFO  [Executor task launch worker for task 6] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:49:09.928 INFO  [Executor task launch worker for task 8] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:49:09.935 INFO  [Executor task launch worker for task 5] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:49:09.942 INFO  [Executor task launch worker for task 7] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:50:53.132 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 10.0.152.153:62482 in memory (size: 2.2 KB, free: 1988.6 MB)
11:50:53.136 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 81
11:50:53.136 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 8
11:50:53.136 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 6
11:50:53.136 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 2
11:50:53.137 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 10.0.152.153:62482 in memory (size: 27.3 KB, free: 1988.7 MB)
11:50:53.137 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 1
11:50:53.138 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 4
11:50:53.140 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 0
11:50:53.140 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 5
11:50:53.140 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 3
11:50:53.142 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 10.0.152.153:62482 in memory (size: 10.3 KB, free: 1988.7 MB)
11:50:53.147 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 7
11:50:57.285 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 2.0 in stage 2.0 (TID 7). 1782 bytes result sent to driver
11:50:57.286 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 2.0 (TID 7) in 107598 ms on localhost (executor driver) (1/4)
11:50:57.918 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 3.0 in stage 2.0 (TID 8). 1739 bytes result sent to driver
11:50:57.918 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 2.0 (TID 8) in 108152 ms on localhost (executor driver) (2/4)
11:51:09.244 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 5). 1739 bytes result sent to driver
11:51:09.244 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 5) in 119557 ms on localhost (executor driver) (3/4)
11:51:12.691 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 1.0 in stage 2.0 (TID 6). 1739 bytes result sent to driver
11:51:12.692 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 6) in 123004 ms on localhost (executor driver) (4/4)
11:51:12.692 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
11:51:12.692 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (insertInto at SparkHelper.scala:36) finished in 123.005 s
11:51:12.692 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:51:12.692 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:51:12.692 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 3)
11:51:12.692 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:51:12.693 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[14] at insertInto at SparkHelper.scala:36), which has no missing parents
11:51:12.742 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 184.6 KB, free 1988.2 MB)
11:51:12.744 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 68.1 KB, free 1988.1 MB)
11:51:12.744 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 10.0.152.153:62482 (size: 68.1 KB, free: 1988.6 MB)
11:51:12.745 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1004
11:51:12.745 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at insertInto at SparkHelper.scala:36) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:51:12.745 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 4 tasks
11:51:12.746 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 9, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:51:12.746 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 10, localhost, executor driver, partition 1, ANY, 4726 bytes)
11:51:12.746 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 11, localhost, executor driver, partition 2, ANY, 4726 bytes)
11:51:12.747 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 3.0 (TID 12, localhost, executor driver, partition 3, ANY, 4726 bytes)
11:51:12.747 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 9)
11:51:12.747 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 10)
11:51:12.747 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 11)
11:51:12.747 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 3.0 in stage 3.0 (TID 12)
11:51:12.778 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 non-empty blocks out of 4 blocks
11:51:12.778 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:12.779 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 non-empty blocks out of 4 blocks
11:51:12.779 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:12.784 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 non-empty blocks out of 4 blocks
11:51:12.784 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:12.793 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 non-empty blocks out of 4 blocks
11:51:12.794 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:12.797 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.503795 ms
11:51:12.837 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 33.603692 ms
11:51:13.334 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 10.0.152.153:62482 in memory (size: 10.3 KB, free: 1988.6 MB)
11:51:13.485 INFO  [Executor task launch worker for task 12] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:13.486 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:13.489 INFO  [Executor task launch worker for task 9] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:13.502 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:13.513 INFO  [Executor task launch worker for task 11] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:13.513 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:13.522 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 25.634052 ms
11:51:13.533 INFO  [Executor task launch worker for task 10] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:13.533 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:13.566 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 22.553026 ms
11:51:13.586 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 17.676717 ms
11:51:13.681 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@25a641a7
11:51:13.682 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@31ae68d9
11:51:13.682 INFO  [Executor task launch worker for task 12] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@406a6b1f
11:51:13.682 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1d1a148e
11:51:13.693 INFO  [Executor task launch worker for task 10] org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
11:51:13.699 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:13.699 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:13.699 INFO  [Executor task launch worker for task 12] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:13.699 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:13.699 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hdfsCluster/data/release/dw/release_customer/.hive-staging_hive_2019-09-10_11-49-08_677_6563403111305353427-1/-ext-10000/_temporary/0/_temporary/attempt_20190910115113_0003_m_000001_0/bdp_day=20190614/part-00001-b3c419d2-b290-4896-9262-ea057a202e36.c000
11:51:13.699 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hdfsCluster/data/release/dw/release_customer/.hive-staging_hive_2019-09-10_11-49-08_677_6563403111305353427-1/-ext-10000/_temporary/0/_temporary/attempt_20190910115113_0003_m_000002_0/bdp_day=20190614/part-00002-b3c419d2-b290-4896-9262-ea057a202e36.c000
11:51:13.699 INFO  [Executor task launch worker for task 12] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hdfsCluster/data/release/dw/release_customer/.hive-staging_hive_2019-09-10_11-49-08_677_6563403111305353427-1/-ext-10000/_temporary/0/_temporary/attempt_20190910115113_0003_m_000003_0/bdp_day=20190614/part-00003-b3c419d2-b290-4896-9262-ea057a202e36.c000
11:51:13.699 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hdfsCluster/data/release/dw/release_customer/.hive-staging_hive_2019-09-10_11-49-08_677_6563403111305353427-1/-ext-10000/_temporary/0/_temporary/attempt_20190910115113_0003_m_000000_0/bdp_day=20190614/part-00000-b3c419d2-b290-4896-9262-ea057a202e36.c000
11:51:13.702 INFO  [Executor task launch worker for task 9] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:13.702 INFO  [Executor task launch worker for task 11] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:13.702 INFO  [Executor task launch worker for task 10] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:13.702 INFO  [Executor task launch worker for task 12] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:13.702 INFO  [Executor task launch worker for task 9] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:13.702 INFO  [Executor task launch worker for task 11] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:13.702 INFO  [Executor task launch worker for task 10] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:13.702 INFO  [Executor task launch worker for task 12] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:13.703 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:13.703 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:13.703 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:13.703 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:13.703 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:13.704 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:13.704 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:13.704 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:13.704 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:13.704 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:13.704 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:13.704 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:13.704 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:13.704 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:13.704 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:13.704 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:13.704 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:13.704 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:13.704 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:13.704 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:13.705 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:13.705 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:13.705 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:13.705 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:14.214 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@6ac35f0d
11:51:14.215 INFO  [Executor task launch worker for task 12] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@2f53602a
11:51:14.215 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@3f91b9d2
11:51:14.215 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@1eabdb68
11:51:16.672 INFO  [Executor task launch worker for task 12] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 15,984,387
11:51:16.695 INFO  [Executor task launch worker for task 10] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 15,985,045
11:51:16.707 INFO  [Executor task launch worker for task 11] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 15,985,790
11:51:16.734 INFO  [Executor task launch worker for task 9] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 15,985,408
11:51:17.547 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 5,785,108B for [release_session] BINARY: 214,244 values, 5,784,636B raw, 5,784,636B comp, 6 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:17.548 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 86B for [release_status] BINARY: 214,244 values, 24B raw, 24B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
11:51:17.553 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 5,785,081B for [release_session] BINARY: 214,243 values, 5,784,609B raw, 5,784,609B comp, 6 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:17.553 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 86B for [release_status] BINARY: 214,243 values, 24B raw, 24B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
11:51:17.593 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 5,785,027B for [release_session] BINARY: 214,241 values, 5,784,555B raw, 5,784,555B comp, 6 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:17.593 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 86B for [release_status] BINARY: 214,241 values, 24B raw, 24B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
11:51:17.667 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 5,785,135B for [release_session] BINARY: 214,245 values, 5,784,663B raw, 5,784,663B comp, 6 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:17.668 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 86B for [release_status] BINARY: 214,245 values, 24B raw, 24B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
11:51:19.880 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 2,142,565B for [device_num] BINARY: 214,241 values, 2,142,433B raw, 2,142,433B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:19.891 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 54,072B for [device_type] BINARY: 214,241 values, 54,009B raw, 54,009B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
11:51:19.901 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 2,142,605B for [device_num] BINARY: 214,245 values, 2,142,473B raw, 2,142,473B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:19.904 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 2,142,595B for [device_num] BINARY: 214,244 values, 2,142,463B raw, 2,142,463B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:19.905 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 54,070B for [device_type] BINARY: 214,244 values, 54,007B raw, 54,007B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
11:51:19.928 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 54,071B for [device_type] BINARY: 214,245 values, 54,008B raw, 54,008B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
11:51:19.931 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 80,927B for [sources] BINARY: 214,244 values, 80,797B raw, 80,797B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
11:51:19.932 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 86B for [channels] BINARY: 214,244 values, 24B raw, 24B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
11:51:19.942 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 2,142,585B for [device_num] BINARY: 214,243 values, 2,142,453B raw, 2,142,453B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:19.943 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 54,072B for [device_type] BINARY: 214,243 values, 54,009B raw, 54,009B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
11:51:19.945 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 80,927B for [sources] BINARY: 214,241 values, 80,797B raw, 80,797B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
11:51:19.946 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 86B for [channels] BINARY: 214,241 values, 24B raw, 24B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
11:51:19.962 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 80,930B for [sources] BINARY: 214,243 values, 80,800B raw, 80,800B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
11:51:19.963 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 86B for [channels] BINARY: 214,243 values, 24B raw, 24B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
11:51:19.977 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 80,926B for [sources] BINARY: 214,245 values, 80,796B raw, 80,796B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
11:51:19.977 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 86B for [channels] BINARY: 214,245 values, 24B raw, 24B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
11:51:23.639 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 4,713,685B for [idcard] BINARY: 214,241 values, 4,713,342B raw, 4,713,342B comp, 5 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:23.640 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 33B for [age] INT32: 214,241 values, 8B raw, 8B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:23.640 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 33B for [gender] BINARY: 214,241 values, 8B raw, 8B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:23.640 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 32,010B for [area_code] BINARY: 214,241 values, 31,884B raw, 31,884B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 20B raw, 2B comp}
11:51:23.675 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 241,702B for [longitude] BINARY: 214,241 values, 241,509B raw, 241,509B comp, 4 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 479 entries, 7,244B raw, 479B comp}
11:51:23.710 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 241,728B for [latitude] BINARY: 214,241 values, 241,500B raw, 241,500B comp, 4 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 339 entries, 5,248B raw, 339B comp}
11:51:23.716 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 107,639B for [matter_id] BINARY: 214,241 values, 107,569B raw, 107,569B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
11:51:23.737 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 107,642B for [model_code] BINARY: 214,241 values, 107,568B raw, 107,568B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 71B raw, 10B comp}
11:51:23.753 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 107,639B for [model_version] BINARY: 214,241 values, 107,569B raw, 107,569B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
11:51:23.763 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 80,866B for [aid] BINARY: 214,241 values, 80,788B raw, 80,788B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 5 entries, 40B raw, 5B comp}
11:51:23.814 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 295,130B for [ct] INT64: 214,241 values, 295,036B raw, 295,036B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
11:51:27.943 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 4,713,751B for [idcard] BINARY: 214,244 values, 4,713,408B raw, 4,713,408B comp, 5 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:27.943 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 33B for [age] INT32: 214,244 values, 8B raw, 8B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:27.943 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 33B for [gender] BINARY: 214,244 values, 8B raw, 8B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:27.943 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 32,047B for [area_code] BINARY: 214,244 values, 31,921B raw, 31,921B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 20B raw, 2B comp}
11:51:28.078 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 241,693B for [longitude] BINARY: 214,244 values, 241,500B raw, 241,500B comp, 4 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 479 entries, 7,244B raw, 479B comp}
11:51:28.215 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 241,739B for [latitude] BINARY: 214,244 values, 241,511B raw, 241,511B comp, 4 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 339 entries, 5,248B raw, 339B comp}
11:51:28.230 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 107,639B for [matter_id] BINARY: 214,244 values, 107,569B raw, 107,569B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
11:51:28.274 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 107,647B for [model_code] BINARY: 214,244 values, 107,573B raw, 107,573B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 71B raw, 10B comp}
11:51:28.386 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 107,639B for [model_version] BINARY: 214,244 values, 107,569B raw, 107,569B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
11:51:28.434 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 80,869B for [aid] BINARY: 214,244 values, 80,791B raw, 80,791B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 5 entries, 40B raw, 5B comp}
11:51:28.830 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 295,141B for [ct] INT64: 214,244 values, 295,047B raw, 295,047B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
11:51:29.359 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 4,713,773B for [idcard] BINARY: 214,245 values, 4,713,430B raw, 4,713,430B comp, 5 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:29.359 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 33B for [age] INT32: 214,245 values, 8B raw, 8B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:29.359 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 33B for [gender] BINARY: 214,245 values, 8B raw, 8B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:29.360 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 31,875B for [area_code] BINARY: 214,245 values, 31,749B raw, 31,749B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 20B raw, 2B comp}
11:51:29.641 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 4,713,729B for [idcard] BINARY: 214,243 values, 4,713,386B raw, 4,713,386B comp, 5 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:29.641 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 33B for [age] INT32: 214,243 values, 8B raw, 8B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:29.641 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 33B for [gender] BINARY: 214,243 values, 8B raw, 8B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:29.641 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 31,936B for [area_code] BINARY: 214,243 values, 31,810B raw, 31,810B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 20B raw, 2B comp}
11:51:29.692 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 241,693B for [longitude] BINARY: 214,245 values, 241,500B raw, 241,500B comp, 4 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 479 entries, 7,244B raw, 479B comp}
11:51:30.016 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 241,693B for [longitude] BINARY: 214,243 values, 241,500B raw, 241,500B comp, 4 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 479 entries, 7,244B raw, 479B comp}
11:51:30.197 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 241,729B for [latitude] BINARY: 214,245 values, 241,501B raw, 241,501B comp, 4 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 339 entries, 5,248B raw, 339B comp}
11:51:30.437 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 107,642B for [matter_id] BINARY: 214,245 values, 107,572B raw, 107,572B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
11:51:30.592 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 241,720B for [latitude] BINARY: 214,243 values, 241,492B raw, 241,492B comp, 4 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 339 entries, 5,248B raw, 339B comp}
11:51:30.629 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 107,647B for [model_code] BINARY: 214,245 values, 107,573B raw, 107,573B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 71B raw, 10B comp}
11:51:30.664 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 107,639B for [matter_id] BINARY: 214,243 values, 107,569B raw, 107,569B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
11:51:30.899 INFO  [Executor task launch worker for task 11] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190910115113_0003_m_000002_0' to hdfs://hdfsCluster/data/release/dw/release_customer/.hive-staging_hive_2019-09-10_11-49-08_677_6563403111305353427-1/-ext-10000/_temporary/0/task_20190910115113_0003_m_000002
11:51:30.900 INFO  [Executor task launch worker for task 11] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190910115113_0003_m_000002_0: Committed
11:51:30.902 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 11). 2615 bytes result sent to driver
11:51:30.903 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 11) in 18157 ms on localhost (executor driver) (1/4)
11:51:31.009 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 107,642B for [model_code] BINARY: 214,243 values, 107,568B raw, 107,568B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 71B raw, 10B comp}
11:51:31.018 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 107,639B for [model_version] BINARY: 214,245 values, 107,569B raw, 107,569B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
11:51:31.099 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 80,869B for [aid] BINARY: 214,245 values, 80,791B raw, 80,791B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 5 entries, 40B raw, 5B comp}
11:51:31.185 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 107,638B for [model_version] BINARY: 214,243 values, 107,568B raw, 107,568B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
11:51:31.261 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 80,869B for [aid] BINARY: 214,243 values, 80,791B raw, 80,791B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 5 entries, 40B raw, 5B comp}
11:51:31.726 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 295,141B for [ct] INT64: 214,245 values, 295,047B raw, 295,047B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
11:51:32.135 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 295,141B for [ct] INT64: 214,243 values, 295,047B raw, 295,047B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
11:51:33.793 INFO  [Executor task launch worker for task 9] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190910115113_0003_m_000000_0' to hdfs://hdfsCluster/data/release/dw/release_customer/.hive-staging_hive_2019-09-10_11-49-08_677_6563403111305353427-1/-ext-10000/_temporary/0/task_20190910115113_0003_m_000000
11:51:33.793 INFO  [Executor task launch worker for task 9] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190910115113_0003_m_000000_0: Committed
11:51:33.794 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 9). 2615 bytes result sent to driver
11:51:33.795 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 9) in 21049 ms on localhost (executor driver) (2/4)
11:51:33.955 INFO  [Executor task launch worker for task 12] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190910115113_0003_m_000003_0' to hdfs://hdfsCluster/data/release/dw/release_customer/.hive-staging_hive_2019-09-10_11-49-08_677_6563403111305353427-1/-ext-10000/_temporary/0/task_20190910115113_0003_m_000003
11:51:33.955 INFO  [Executor task launch worker for task 12] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190910115113_0003_m_000003_0: Committed
11:51:33.956 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 3.0 in stage 3.0 (TID 12). 2615 bytes result sent to driver
11:51:33.957 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 3.0 (TID 12) in 21211 ms on localhost (executor driver) (3/4)
11:51:34.580 INFO  [Executor task launch worker for task 10] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190910115113_0003_m_000001_0' to hdfs://hdfsCluster/data/release/dw/release_customer/.hive-staging_hive_2019-09-10_11-49-08_677_6563403111305353427-1/-ext-10000/_temporary/0/task_20190910115113_0003_m_000001
11:51:34.581 INFO  [Executor task launch worker for task 10] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190910115113_0003_m_000001_0: Committed
11:51:34.581 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 10). 2615 bytes result sent to driver
11:51:34.582 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 10) in 21836 ms on localhost (executor driver) (4/4)
11:51:34.582 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
11:51:34.583 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (insertInto at SparkHelper.scala:36) finished in 21.836 s
11:51:34.583 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: insertInto at SparkHelper.scala:36, took 144.916142 s
11:51:35.235 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:51:35.271 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.271 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.271 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.272 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.272 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.272 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.272 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.272 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.272 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
11:51:35.272 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.272 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.272 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.272 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.272 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.273 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.273 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.273 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.273 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:35.536 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hdfsCluster/data/release/dw/release_customer/bdp_day=20190614/part-00002-49dfc689-7682-4e94-b77d-bfe47faeaffb.c000
11:51:35.557 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:35.577 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hdfsCluster/data/release/dw/release_customer/bdp_day=20190614/part-00002-729b47af-965b-4fc9-a483-3af7c465320f.c000
11:51:35.582 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:35.609 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hdfsCluster/data/release/dw/release_customer/bdp_day=20190614/part-00005-729b47af-965b-4fc9-a483-3af7c465320f.c000
11:51:35.620 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:35.646 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hdfsCluster/data/release/dw/release_customer/bdp_day=20190614/part-00006-49dfc689-7682-4e94-b77d-bfe47faeaffb.c000
11:51:35.684 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:35.727 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hdfsCluster/data/release/dw/release_customer/bdp_day=20190614/part-00008-729b47af-965b-4fc9-a483-3af7c465320f.c000
11:51:35.773 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:35.821 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hdfsCluster/data/release/dw/release_customer/bdp_day=20190614/part-00010-49dfc689-7682-4e94-b77d-bfe47faeaffb.c000
11:51:35.850 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:35.889 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hdfsCluster/data/release/dw/release_customer/bdp_day=20190614/part-00011-729b47af-965b-4fc9-a483-3af7c465320f.c000
11:51:35.892 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:35.910 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hdfsCluster/data/release/dw/release_customer/bdp_day=20190614/part-00014-49dfc689-7682-4e94-b77d-bfe47faeaffb.c000
11:51:35.938 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:35.996 ERROR [main] org.apache.hadoop.hdfs.KeyProviderCache - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
11:51:36.028 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hdfsCluster/data/release/dw/release_customer/.hive-staging_hive_2019-09-10_11-49-08_677_6563403111305353427-1/-ext-10000/bdp_day=20190614/part-00000-b3c419d2-b290-4896-9262-ea057a202e36.c000, dest: hdfs://hdfsCluster/data/release/dw/release_customer/bdp_day=20190614/part-00000-b3c419d2-b290-4896-9262-ea057a202e36.c000, Status:true
11:51:36.139 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hdfsCluster/data/release/dw/release_customer/.hive-staging_hive_2019-09-10_11-49-08_677_6563403111305353427-1/-ext-10000/bdp_day=20190614/part-00001-b3c419d2-b290-4896-9262-ea057a202e36.c000, dest: hdfs://hdfsCluster/data/release/dw/release_customer/bdp_day=20190614/part-00001-b3c419d2-b290-4896-9262-ea057a202e36.c000, Status:true
11:51:36.219 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hdfsCluster/data/release/dw/release_customer/.hive-staging_hive_2019-09-10_11-49-08_677_6563403111305353427-1/-ext-10000/bdp_day=20190614/part-00002-b3c419d2-b290-4896-9262-ea057a202e36.c000, dest: hdfs://hdfsCluster/data/release/dw/release_customer/bdp_day=20190614/part-00002-b3c419d2-b290-4896-9262-ea057a202e36.c000, Status:true
11:51:36.810 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hdfsCluster/data/release/dw/release_customer/.hive-staging_hive_2019-09-10_11-49-08_677_6563403111305353427-1/-ext-10000/bdp_day=20190614/part-00003-b3c419d2-b290-4896-9262-ea057a202e36.c000, dest: hdfs://hdfsCluster/data/release/dw/release_customer/bdp_day=20190614/part-00003-b3c419d2-b290-4896-9262-ea057a202e36.c000, Status:true
11:51:37.074 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hdfsCluster/data/release/dw/release_customer/.hive-staging_hive_2019-09-10_11-49-08_677_6563403111305353427-1/-ext-10000/bdp_day=20190614 with partSpec {bdp_day=20190614}
11:51:37.133 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_customer`
11:51:37.347 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:37.347 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:37.347 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:37.347 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:37.347 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:37.348 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:37.348 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:37.348 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:37.348 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
11:51:37.348 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:37.348 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:37.348 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:37.349 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:37.349 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:37.349 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:37.349 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:37.350 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:37.350 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:37.388 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@39fc6b2c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
11:51:37.415 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.152.153:4040
11:51:37.426 INFO  [dispatcher-event-loop-0] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
11:51:37.709 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
11:51:37.709 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
11:51:37.710 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
11:51:37.713 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
11:51:37.716 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
11:51:37.719 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
11:51:37.720 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\mx\AppData\Local\Temp\spark-357e1d36-6f82-405b-bd15-a9527cbe0c7b
15:51:31.314 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
15:51:32.607 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_customer_job
15:51:33.100 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: YongningY,root
15:51:33.101 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: YongningY,root
15:51:33.115 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
15:51:33.117 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
15:51:33.122 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(YongningY, root); groups with view permissions: Set(); users  with modify permissions: Set(YongningY, root); groups with modify permissions: Set()
15:51:34.535 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 5238.
15:51:34.663 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
15:51:34.796 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
15:51:34.817 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
15:51:34.818 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
15:51:34.839 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\Yongning Y\AppData\Local\Temp\blockmgr-4b07c50f-8e3b-4ad7-97c1-28e633d2c4d9
15:51:35.009 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1445.7 MB
15:51:35.194 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
15:51:35.610 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @8968ms
15:51:35.871 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
15:51:35.901 INFO  [main] org.spark_project.jetty.server.Server - Started @9268ms
15:51:35.938 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@3fc08eec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
15:51:35.939 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
15:51:36.007 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@492fc69e{/jobs,null,AVAILABLE,@Spark}
15:51:36.007 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d2260db{/jobs/json,null,AVAILABLE,@Spark}
15:51:36.008 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49bf29c6{/jobs/job,null,AVAILABLE,@Spark}
15:51:36.009 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7668d560{/jobs/job/json,null,AVAILABLE,@Spark}
15:51:36.010 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@126be319{/stages,null,AVAILABLE,@Spark}
15:51:36.011 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c371e13{/stages/json,null,AVAILABLE,@Spark}
15:51:36.014 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e34c607{/stages/stage,null,AVAILABLE,@Spark}
15:51:36.018 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9257031{/stages/stage/json,null,AVAILABLE,@Spark}
15:51:36.020 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7726e185{/stages/pool,null,AVAILABLE,@Spark}
15:51:36.022 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@282308c3{/stages/pool/json,null,AVAILABLE,@Spark}
15:51:36.025 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1db0ec27{/storage,null,AVAILABLE,@Spark}
15:51:36.030 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d4ab71a{/storage/json,null,AVAILABLE,@Spark}
15:51:36.032 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1af05b03{/storage/rdd,null,AVAILABLE,@Spark}
15:51:36.034 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ad777f{/storage/rdd/json,null,AVAILABLE,@Spark}
15:51:36.038 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@438bad7c{/environment,null,AVAILABLE,@Spark}
15:51:36.039 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4fdf8f12{/environment/json,null,AVAILABLE,@Spark}
15:51:36.040 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54f5f647{/executors,null,AVAILABLE,@Spark}
15:51:36.044 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a6d5a8f{/executors/json,null,AVAILABLE,@Spark}
15:51:36.045 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@315ba14a{/executors/threadDump,null,AVAILABLE,@Spark}
15:51:36.055 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27f0ad19{/executors/threadDump/json,null,AVAILABLE,@Spark}
15:51:36.076 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38d5b107{/static,null,AVAILABLE,@Spark}
15:51:36.078 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77e2a6e2{/,null,AVAILABLE,@Spark}
15:51:36.082 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@199e4c2b{/api,null,AVAILABLE,@Spark}
15:51:36.088 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/job/kill,null,AVAILABLE,@Spark}
15:51:36.090 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e7095ac{/stages/stage/kill,null,AVAILABLE,@Spark}
15:51:36.107 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.145.1:4040
15:51:36.464 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
15:51:36.556 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5260.
15:51:36.558 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.145.1:5260
15:51:36.571 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
15:51:36.632 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.145.1, 5260, None)
15:51:36.647 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.145.1:5260 with 1445.7 MB RAM, BlockManagerId(driver, 192.168.145.1, 5260, None)
15:51:36.654 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.145.1, 5260, None)
15:51:36.655 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.145.1, 5260, None)
15:51:36.973 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@aa004a0{/metrics/json,null,AVAILABLE,@Spark}
15:51:37.147 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/H:/IdeaProject/release_test/target/classes/hive-site.xml
15:51:37.198 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('hdfs://hdfsCluster/hive/db').
15:51:37.200 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'hdfs://hdfsCluster/hive/db'.
15:51:37.222 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49dbaaf3{/SQL,null,AVAILABLE,@Spark}
15:51:37.223 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@736f3e9e{/SQL/json,null,AVAILABLE,@Spark}
15:51:37.224 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1698d7c0{/SQL/execution,null,AVAILABLE,@Spark}
15:51:37.225 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@87abc48{/SQL/execution/json,null,AVAILABLE,@Spark}
15:51:37.230 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13047d7d{/static/sql,null,AVAILABLE,@Spark}
15:51:39.388 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
15:51:40.117 WARN  [main] org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.metastore.local does not exist
15:51:40.670 INFO  [main] hive.metastore - Trying to connect to metastore with URI thrift://node242:9083
15:51:42.992 WARN  [main] hive.metastore - Failed to connect to the MetaStore Server...
15:51:42.993 INFO  [main] hive.metastore - Waiting 1 seconds before next connection attempt.
15:51:43.994 INFO  [main] hive.metastore - Trying to connect to metastore with URI thrift://node242:9083
15:51:43.995 WARN  [main] hive.metastore - Failed to connect to the MetaStore Server...
15:51:43.996 INFO  [main] hive.metastore - Waiting 1 seconds before next connection attempt.
15:51:44.997 INFO  [main] hive.metastore - Trying to connect to metastore with URI thrift://node242:9083
15:51:44.999 WARN  [main] hive.metastore - Failed to connect to the MetaStore Server...
15:51:45.000 INFO  [main] hive.metastore - Waiting 1 seconds before next connection attempt.
15:51:46.162 WARN  [main] hive.ql.metadata.Hive - Failed to access metastore. This class should not accessed in runtime.
org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1236)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.SparkSession.udf(SparkSession.scala:207)
	at com.qf.bigdata.release.util.SparkHelper$.registerFun(SparkHelper.scala:60)
	at com.qf.bigdata.release.util.SparkHelper$.createSpark(SparkHelper.scala:51)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$.handleJobs(DWReleaseCustomer.scala:79)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$.main(DWReleaseCustomer.scala:104)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer.main(DWReleaseCustomer.scala)
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	... 37 common frames omitted
Caused by: java.lang.reflect.InvocationTargetException: null
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	... 43 common frames omitted
Caused by: org.apache.hadoop.hive.metastore.api.MetaException: Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.UnknownHostException: node242
	at org.apache.thrift.transport.TSocket.open(TSocket.java:226)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:420)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.SparkSession.udf(SparkSession.scala:207)
	at com.qf.bigdata.release.util.SparkHelper$.registerFun(SparkHelper.scala:60)
	at com.qf.bigdata.release.util.SparkHelper$.createSpark(SparkHelper.scala:51)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$.handleJobs(DWReleaseCustomer.scala:79)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$.main(DWReleaseCustomer.scala:104)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer.main(DWReleaseCustomer.scala)
Caused by: java.net.UnknownHostException: node242
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:221)
	... 51 more

	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:466)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	... 48 common frames omitted
15:51:46.169 INFO  [main] hive.metastore - Trying to connect to metastore with URI thrift://node242:9083
15:51:46.170 WARN  [main] hive.metastore - Failed to connect to the MetaStore Server...
15:51:46.171 INFO  [main] hive.metastore - Waiting 1 seconds before next connection attempt.
15:51:47.171 INFO  [main] hive.metastore - Trying to connect to metastore with URI thrift://node242:9083
15:51:47.172 WARN  [main] hive.metastore - Failed to connect to the MetaStore Server...
15:51:47.173 INFO  [main] hive.metastore - Waiting 1 seconds before next connection attempt.
15:51:48.174 INFO  [main] hive.metastore - Trying to connect to metastore with URI thrift://node242:9083
15:51:48.176 WARN  [main] hive.metastore - Failed to connect to the MetaStore Server...
15:51:48.177 INFO  [main] hive.metastore - Waiting 1 seconds before next connection attempt.
15:51:49.194 ERROR [main] com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$ - Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':
java.lang.IllegalArgumentException: Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1062)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.SparkSession.udf(SparkSession.scala:207)
	at com.qf.bigdata.release.util.SparkHelper$.registerFun(SparkHelper.scala:60)
	at com.qf.bigdata.release.util.SparkHelper$.createSpark(SparkHelper.scala:51)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$.handleJobs(DWReleaseCustomer.scala:79)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$.main(DWReleaseCustomer.scala:104)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer.main(DWReleaseCustomer.scala)
Caused by: org.apache.spark.sql.AnalysisException: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:106)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	... 11 common frames omitted
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	... 20 common frames omitted
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	... 34 common frames omitted
Caused by: java.lang.reflect.InvocationTargetException: null
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	... 40 common frames omitted
Caused by: org.apache.hadoop.hive.metastore.api.MetaException: Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.UnknownHostException: node242
	at org.apache.thrift.transport.TSocket.open(TSocket.java:226)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:420)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.SparkSession.udf(SparkSession.scala:207)
	at com.qf.bigdata.release.util.SparkHelper$.registerFun(SparkHelper.scala:60)
	at com.qf.bigdata.release.util.SparkHelper$.createSpark(SparkHelper.scala:51)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$.handleJobs(DWReleaseCustomer.scala:79)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$.main(DWReleaseCustomer.scala:104)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer.main(DWReleaseCustomer.scala)
Caused by: java.net.UnknownHostException: node242
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:221)
	... 48 more

	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:466)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	... 45 common frames omitted
15:51:49.242 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
15:51:49.279 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@3fc08eec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
15:51:49.293 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.145.1:4040
15:51:49.359 INFO  [dispatcher-event-loop-2] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
15:51:49.382 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
15:51:49.383 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
15:51:49.431 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
15:51:49.439 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
15:51:49.443 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
15:51:49.446 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
15:51:49.448 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\Yongning Y\AppData\Local\Temp\spark-7ab4b6a3-4b9b-4a4b-8be7-23a5c32fe702
15:57:08.488 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
15:57:08.910 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_customer_job
15:57:08.930 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: YongningY,root
15:57:08.931 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: YongningY,root
15:57:08.932 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
15:57:08.933 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
15:57:08.933 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(YongningY, root); groups with view permissions: Set(); users  with modify permissions: Set(YongningY, root); groups with modify permissions: Set()
15:57:09.607 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 5312.
15:57:09.629 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
15:57:09.646 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
15:57:09.650 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
15:57:09.651 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
15:57:09.660 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\Yongning Y\AppData\Local\Temp\blockmgr-0226a05a-b819-45e4-a84e-cc5572841ba8
15:57:09.678 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1445.7 MB
15:57:09.724 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
15:57:09.819 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @4048ms
15:57:09.916 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
15:57:09.935 INFO  [main] org.spark_project.jetty.server.Server - Started @4167ms
15:57:09.966 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@3fc08eec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
15:57:09.966 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
15:57:09.993 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@492fc69e{/jobs,null,AVAILABLE,@Spark}
15:57:09.994 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d2260db{/jobs/json,null,AVAILABLE,@Spark}
15:57:09.995 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49bf29c6{/jobs/job,null,AVAILABLE,@Spark}
15:57:09.996 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7668d560{/jobs/job/json,null,AVAILABLE,@Spark}
15:57:09.997 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@126be319{/stages,null,AVAILABLE,@Spark}
15:57:09.998 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c371e13{/stages/json,null,AVAILABLE,@Spark}
15:57:09.999 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e34c607{/stages/stage,null,AVAILABLE,@Spark}
15:57:10.001 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9257031{/stages/stage/json,null,AVAILABLE,@Spark}
15:57:10.002 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7726e185{/stages/pool,null,AVAILABLE,@Spark}
15:57:10.003 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@282308c3{/stages/pool/json,null,AVAILABLE,@Spark}
15:57:10.004 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1db0ec27{/storage,null,AVAILABLE,@Spark}
15:57:10.005 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d4ab71a{/storage/json,null,AVAILABLE,@Spark}
15:57:10.007 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1af05b03{/storage/rdd,null,AVAILABLE,@Spark}
15:57:10.008 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ad777f{/storage/rdd/json,null,AVAILABLE,@Spark}
15:57:10.009 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@438bad7c{/environment,null,AVAILABLE,@Spark}
15:57:10.010 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4fdf8f12{/environment/json,null,AVAILABLE,@Spark}
15:57:10.011 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54f5f647{/executors,null,AVAILABLE,@Spark}
15:57:10.014 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a6d5a8f{/executors/json,null,AVAILABLE,@Spark}
15:57:10.015 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@315ba14a{/executors/threadDump,null,AVAILABLE,@Spark}
15:57:10.015 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27f0ad19{/executors/threadDump/json,null,AVAILABLE,@Spark}
15:57:10.033 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38d5b107{/static,null,AVAILABLE,@Spark}
15:57:10.036 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77e2a6e2{/,null,AVAILABLE,@Spark}
15:57:10.040 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@199e4c2b{/api,null,AVAILABLE,@Spark}
15:57:10.042 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/job/kill,null,AVAILABLE,@Spark}
15:57:10.043 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e7095ac{/stages/stage/kill,null,AVAILABLE,@Spark}
15:57:10.045 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.145.1:4040
15:57:10.201 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
15:57:10.230 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5333.
15:57:10.231 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.145.1:5333
15:57:10.232 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
15:57:10.283 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.145.1, 5333, None)
15:57:10.288 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.145.1:5333 with 1445.7 MB RAM, BlockManagerId(driver, 192.168.145.1, 5333, None)
15:57:10.291 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.145.1, 5333, None)
15:57:10.292 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.145.1, 5333, None)
15:57:10.562 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@dffa30b{/metrics/json,null,AVAILABLE,@Spark}
15:57:10.618 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/H:/IdeaProject/release_test/target/classes/hive-site.xml
15:57:10.662 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
15:57:10.663 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/user/hive/warehouse'.
15:57:10.672 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f443fae{/SQL,null,AVAILABLE,@Spark}
15:57:10.672 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79ab34c1{/SQL/json,null,AVAILABLE,@Spark}
15:57:10.673 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@782168b7{/SQL/execution,null,AVAILABLE,@Spark}
15:57:10.674 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7435a578{/SQL/execution/json,null,AVAILABLE,@Spark}
15:57:10.676 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b214b94{/static/sql,null,AVAILABLE,@Spark}
15:57:11.629 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
15:57:12.527 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
15:57:12.571 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
15:57:14.634 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
15:57:16.536 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
15:57:16.545 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
15:57:16.919 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
15:57:16.923 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
15:57:17.009 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
15:57:17.213 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
15:57:17.220 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
15:57:17.264 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=azkaban pat=*
15:57:17.264 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=azkaban pat=*	
15:57:17.318 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dim pat=*
15:57:17.318 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dim pat=*	
15:57:17.322 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dm pat=*
15:57:17.322 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dm pat=*	
15:57:17.326 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dwd pat=*
15:57:17.326 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dwd pat=*	
15:57:17.331 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dws pat=*
15:57:17.331 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dws pat=*	
15:57:17.335 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_ods pat=*
15:57:17.335 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_ods pat=*	
15:57:17.339 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
15:57:17.339 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
15:57:17.343 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_release pat=*
15:57:17.343 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_release pat=*	
15:57:17.346 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_shop pat=*
15:57:17.347 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_shop pat=*	
15:57:17.350 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
15:57:17.350 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
15:57:17.354 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_shop pat=*
15:57:17.354 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_shop pat=*	
15:57:17.357 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
15:57:17.358 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
15:57:17.361 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_shop pat=*
15:57:17.362 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_shop pat=*	
15:57:17.365 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_release pat=*
15:57:17.366 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_release pat=*	
15:57:17.369 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_shop pat=*
15:57:17.369 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_shop pat=*	
15:57:17.373 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
15:57:17.374 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
15:57:17.377 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_shop pat=*
15:57:17.378 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_shop pat=*	
15:57:17.382 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dim pat=*
15:57:17.382 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dim pat=*	
15:57:17.386 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dm pat=*
15:57:17.386 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dm pat=*	
15:57:17.390 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dw pat=*
15:57:17.390 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dw pat=*	
15:57:17.393 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_rds pat=*
15:57:17.393 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_rds pat=*	
15:57:17.397 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=te pat=*
15:57:17.397 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=te pat=*	
15:57:17.400 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test pat=*
15:57:17.401 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test pat=*	
15:57:17.405 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test4 pat=*
15:57:17.405 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test4 pat=*	
15:57:19.934 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/affbb996-95c9-4b33-8198-2e1dbe571e5b_resources
15:57:19.978 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/affbb996-95c9-4b33-8198-2e1dbe571e5b
15:57:19.984 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/affbb996-95c9-4b33-8198-2e1dbe571e5b
15:57:19.993 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/affbb996-95c9-4b33-8198-2e1dbe571e5b/_tmp_space.db
15:57:20.009 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
15:57:20.166 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for root: Unknown error.
15:57:20.236 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:57:20.236 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
15:57:20.242 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
15:57:20.243 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
15:57:20.246 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
15:57:20.622 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/8635987e-9a2e-4575-b485-00e14deac8dc_resources
15:57:20.631 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/8635987e-9a2e-4575-b485-00e14deac8dc
15:57:20.634 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/8635987e-9a2e-4575-b485-00e14deac8dc
15:57:20.645 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/8635987e-9a2e-4575-b485-00e14deac8dc/_tmp_space.db
15:57:20.647 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
15:57:20.800 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
15:57:23.152 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
15:57:23.552 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
15:57:23.552 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
15:57:23.576 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
15:57:23.576 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
15:57:23.830 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
15:57:23.830 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
15:57:23.885 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:23.911 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:23.912 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:23.912 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:23.912 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:23.913 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:23.913 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:23.913 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:23.913 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:23.914 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
15:57:23.977 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
15:57:24.750 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Inferring case-sensitive schema for table ods_release.ods_01_release_session (inference mode: INFER_AND_SAVE)
15:57:24.763 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
15:57:24.763 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
15:57:24.768 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
15:57:24.768 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
15:57:24.795 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
15:57:24.796 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
15:57:24.825 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:24.826 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:24.826 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:24.826 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:24.826 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:24.827 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:24.827 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:24.827 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:24.828 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:57:24.828 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
15:57:24.917 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=ods_release tbl=ods_01_release_session
15:57:24.917 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_partitions : db=ods_release tbl=ods_01_release_session	
15:57:51.317 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
15:57:52.063 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_customer_job
15:57:52.088 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: YongningY,root
15:57:52.089 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: YongningY,root
15:57:52.089 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
15:57:52.090 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
15:57:52.091 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(YongningY, root); groups with view permissions: Set(); users  with modify permissions: Set(YongningY, root); groups with modify permissions: Set()
15:57:52.736 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 5371.
15:57:52.753 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
15:57:52.770 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
15:57:52.773 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
15:57:52.774 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
15:57:52.782 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\Yongning Y\AppData\Local\Temp\blockmgr-ad8ac1b5-d18d-492d-badd-aa38d80ced2b
15:57:52.799 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1445.7 MB
15:57:52.851 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
15:57:52.951 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @4196ms
15:57:53.039 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
15:57:53.062 INFO  [main] org.spark_project.jetty.server.Server - Started @4309ms
15:57:53.086 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@6d1112cc{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
15:57:53.087 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
15:57:53.108 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60b85ba1{/jobs,null,AVAILABLE,@Spark}
15:57:53.109 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b718392{/jobs/json,null,AVAILABLE,@Spark}
15:57:53.110 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f2d2181{/jobs/job,null,AVAILABLE,@Spark}
15:57:53.111 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fcdcf{/jobs/job/json,null,AVAILABLE,@Spark}
15:57:53.112 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46292372{/stages,null,AVAILABLE,@Spark}
15:57:53.113 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c44052e{/stages/json,null,AVAILABLE,@Spark}
15:57:53.114 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@530a8454{/stages/stage,null,AVAILABLE,@Spark}
15:57:53.116 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31198ceb{/stages/stage/json,null,AVAILABLE,@Spark}
15:57:53.117 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75201592{/stages/pool,null,AVAILABLE,@Spark}
15:57:53.118 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@aa5455e{/stages/pool/json,null,AVAILABLE,@Spark}
15:57:53.119 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5dda14d0{/storage,null,AVAILABLE,@Spark}
15:57:53.119 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d9fc57a{/storage/json,null,AVAILABLE,@Spark}
15:57:53.120 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b4ef7{/storage/rdd,null,AVAILABLE,@Spark}
15:57:53.121 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5987e932{/storage/rdd/json,null,AVAILABLE,@Spark}
15:57:53.123 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bbbdd4b{/environment,null,AVAILABLE,@Spark}
15:57:53.123 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25230246{/environment/json,null,AVAILABLE,@Spark}
15:57:53.124 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a8b5227{/executors,null,AVAILABLE,@Spark}
15:57:53.125 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6979efad{/executors/json,null,AVAILABLE,@Spark}
15:57:53.127 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67318f{/executors/threadDump,null,AVAILABLE,@Spark}
15:57:53.128 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17f9344b{/executors/threadDump/json,null,AVAILABLE,@Spark}
15:57:53.143 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54e81b21{/static,null,AVAILABLE,@Spark}
15:57:53.144 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2237bada{/,null,AVAILABLE,@Spark}
15:57:53.146 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5710768a{/api,null,AVAILABLE,@Spark}
15:57:53.147 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bf61e67{/jobs/job/kill,null,AVAILABLE,@Spark}
15:57:53.148 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b273a59{/stages/stage/kill,null,AVAILABLE,@Spark}
15:57:53.150 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.145.1:4040
15:57:53.293 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
15:57:53.322 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5392.
15:57:53.323 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.145.1:5392
15:57:53.325 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
15:57:53.355 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.145.1, 5392, None)
15:57:53.359 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.145.1:5392 with 1445.7 MB RAM, BlockManagerId(driver, 192.168.145.1, 5392, None)
15:57:53.365 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.145.1, 5392, None)
15:57:53.366 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.145.1, 5392, None)
15:57:53.631 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f02251{/metrics/json,null,AVAILABLE,@Spark}
15:57:53.695 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/H:/IdeaProject/release_test/target/classes/hive-site.xml
15:57:53.738 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
15:57:53.739 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/user/hive/warehouse'.
15:57:53.749 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@bdc8014{/SQL,null,AVAILABLE,@Spark}
15:57:53.750 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73ba6fe6{/SQL/json,null,AVAILABLE,@Spark}
15:57:53.751 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28d79cba{/SQL/execution,null,AVAILABLE,@Spark}
15:57:53.752 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29f0c4f2{/SQL/execution/json,null,AVAILABLE,@Spark}
15:57:53.754 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bfe3203{/static/sql,null,AVAILABLE,@Spark}
15:57:54.661 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
15:57:55.394 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
15:57:55.425 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
15:57:56.358 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
15:57:57.771 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
15:57:57.773 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
15:57:58.017 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
15:57:58.021 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
15:57:58.080 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
15:57:58.182 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
15:57:58.184 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
15:57:58.197 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=azkaban pat=*
15:57:58.197 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=azkaban pat=*	
15:57:58.263 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dim pat=*
15:57:58.263 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dim pat=*	
15:57:58.267 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dm pat=*
15:57:58.267 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dm pat=*	
15:57:58.272 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dwd pat=*
15:57:58.273 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dwd pat=*	
15:57:58.279 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dws pat=*
15:57:58.279 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dws pat=*	
15:57:58.283 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_ods pat=*
15:57:58.283 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_ods pat=*	
15:57:58.286 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
15:57:58.287 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
15:57:58.292 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_release pat=*
15:57:58.292 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_release pat=*	
15:57:58.296 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_shop pat=*
15:57:58.296 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_shop pat=*	
15:57:58.299 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
15:57:58.299 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
15:57:58.303 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_shop pat=*
15:57:58.303 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_shop pat=*	
15:57:58.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
15:57:58.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
15:57:58.312 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_shop pat=*
15:57:58.313 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_shop pat=*	
15:57:58.316 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_release pat=*
15:57:58.317 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_release pat=*	
15:57:58.320 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_shop pat=*
15:57:58.320 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_shop pat=*	
15:57:58.363 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
15:57:58.364 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
15:57:58.368 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_shop pat=*
15:57:58.368 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_shop pat=*	
15:57:58.371 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dim pat=*
15:57:58.372 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dim pat=*	
15:57:58.375 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dm pat=*
15:57:58.375 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dm pat=*	
15:57:58.379 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dw pat=*
15:57:58.379 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dw pat=*	
15:57:58.383 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_rds pat=*
15:57:58.383 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_rds pat=*	
15:57:58.388 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=te pat=*
15:57:58.388 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=te pat=*	
15:57:58.391 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test pat=*
15:57:58.391 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test pat=*	
15:57:58.396 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test4 pat=*
15:57:58.396 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test4 pat=*	
15:57:59.055 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/4bdaae47-c87f-455b-a530-23fbee1b0d30_resources
15:57:59.070 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/4bdaae47-c87f-455b-a530-23fbee1b0d30
15:57:59.073 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/4bdaae47-c87f-455b-a530-23fbee1b0d30
15:57:59.077 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/4bdaae47-c87f-455b-a530-23fbee1b0d30/_tmp_space.db
15:57:59.081 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
15:57:59.092 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for root: Unknown error.
15:57:59.104 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:57:59.104 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
15:57:59.110 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
15:57:59.111 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
15:57:59.114 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
15:57:59.317 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/6bcad9a3-ecf7-4009-88b3-59024fc7e6c0_resources
15:57:59.329 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/6bcad9a3-ecf7-4009-88b3-59024fc7e6c0
15:57:59.336 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/6bcad9a3-ecf7-4009-88b3-59024fc7e6c0
15:57:59.351 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/6bcad9a3-ecf7-4009-88b3-59024fc7e6c0/_tmp_space.db
15:57:59.357 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
15:57:59.396 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
15:58:00.933 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
15:58:01.122 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
15:58:01.122 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
15:58:01.128 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
15:58:01.129 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
15:58:01.240 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
15:58:01.240 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
15:58:01.274 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:01.288 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:01.288 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:01.288 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:01.289 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:01.289 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:01.289 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:01.289 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:01.289 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:01.290 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
15:58:01.304 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
15:58:01.430 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Inferring case-sensitive schema for table ods_release.ods_01_release_session (inference mode: INFER_AND_SAVE)
15:58:01.431 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
15:58:01.431 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
15:58:01.437 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
15:58:01.437 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
15:58:01.473 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
15:58:01.474 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
15:58:01.510 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:01.510 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:01.511 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:01.511 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:01.511 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:01.512 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:01.512 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:01.512 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:01.512 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:01.513 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
15:58:01.543 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=ods_release tbl=ods_01_release_session
15:58:01.543 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_partitions : db=ods_release tbl=ods_01_release_session	
15:58:26.738 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
15:58:27.308 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_customer_job
15:58:27.334 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: YongningY,root
15:58:27.334 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: YongningY,root
15:58:27.335 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
15:58:27.336 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
15:58:27.336 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(YongningY, root); groups with view permissions: Set(); users  with modify permissions: Set(YongningY, root); groups with modify permissions: Set()
15:58:28.014 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 5430.
15:58:28.031 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
15:58:28.049 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
15:58:28.052 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
15:58:28.053 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
15:58:28.062 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\Yongning Y\AppData\Local\Temp\blockmgr-74b4c715-10e4-4275-858d-bd4a598028d8
15:58:28.081 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1445.7 MB
15:58:28.140 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
15:58:28.218 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @4029ms
15:58:28.305 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
15:58:28.324 INFO  [main] org.spark_project.jetty.server.Server - Started @4136ms
15:58:28.350 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@6545c4a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
15:58:28.350 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
15:58:28.378 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60b85ba1{/jobs,null,AVAILABLE,@Spark}
15:58:28.379 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b718392{/jobs/json,null,AVAILABLE,@Spark}
15:58:28.379 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f2d2181{/jobs/job,null,AVAILABLE,@Spark}
15:58:28.380 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fcdcf{/jobs/job/json,null,AVAILABLE,@Spark}
15:58:28.381 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46292372{/stages,null,AVAILABLE,@Spark}
15:58:28.382 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c44052e{/stages/json,null,AVAILABLE,@Spark}
15:58:28.383 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@530a8454{/stages/stage,null,AVAILABLE,@Spark}
15:58:28.384 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31198ceb{/stages/stage/json,null,AVAILABLE,@Spark}
15:58:28.385 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75201592{/stages/pool,null,AVAILABLE,@Spark}
15:58:28.386 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@aa5455e{/stages/pool/json,null,AVAILABLE,@Spark}
15:58:28.387 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5dda14d0{/storage,null,AVAILABLE,@Spark}
15:58:28.388 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d9fc57a{/storage/json,null,AVAILABLE,@Spark}
15:58:28.389 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b4ef7{/storage/rdd,null,AVAILABLE,@Spark}
15:58:28.390 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5987e932{/storage/rdd/json,null,AVAILABLE,@Spark}
15:58:28.391 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bbbdd4b{/environment,null,AVAILABLE,@Spark}
15:58:28.393 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25230246{/environment/json,null,AVAILABLE,@Spark}
15:58:28.394 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a8b5227{/executors,null,AVAILABLE,@Spark}
15:58:28.395 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6979efad{/executors/json,null,AVAILABLE,@Spark}
15:58:28.395 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67318f{/executors/threadDump,null,AVAILABLE,@Spark}
15:58:28.397 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17f9344b{/executors/threadDump/json,null,AVAILABLE,@Spark}
15:58:28.414 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54e81b21{/static,null,AVAILABLE,@Spark}
15:58:28.416 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2237bada{/,null,AVAILABLE,@Spark}
15:58:28.417 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5710768a{/api,null,AVAILABLE,@Spark}
15:58:28.419 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bf61e67{/jobs/job/kill,null,AVAILABLE,@Spark}
15:58:28.420 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b273a59{/stages/stage/kill,null,AVAILABLE,@Spark}
15:58:28.423 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.145.1:4040
15:58:28.571 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
15:58:28.608 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5451.
15:58:28.609 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.145.1:5451
15:58:28.610 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
15:58:28.652 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.145.1, 5451, None)
15:58:28.655 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.145.1:5451 with 1445.7 MB RAM, BlockManagerId(driver, 192.168.145.1, 5451, None)
15:58:28.659 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.145.1, 5451, None)
15:58:28.660 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.145.1, 5451, None)
15:58:28.874 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f02251{/metrics/json,null,AVAILABLE,@Spark}
15:58:28.930 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/H:/IdeaProject/release_test/target/classes/hive-site.xml
15:58:28.978 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
15:58:28.979 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/user/hive/warehouse'.
15:58:28.990 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@bdc8014{/SQL,null,AVAILABLE,@Spark}
15:58:28.992 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73ba6fe6{/SQL/json,null,AVAILABLE,@Spark}
15:58:28.994 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28d79cba{/SQL/execution,null,AVAILABLE,@Spark}
15:58:29.001 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29f0c4f2{/SQL/execution/json,null,AVAILABLE,@Spark}
15:58:29.004 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bfe3203{/static/sql,null,AVAILABLE,@Spark}
15:58:29.927 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
15:58:30.737 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
15:58:30.766 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
15:58:31.858 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
15:58:33.207 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
15:58:33.209 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
15:58:33.460 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
15:58:33.463 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
15:58:33.520 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
15:58:33.631 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
15:58:33.632 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
15:58:33.647 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=azkaban pat=*
15:58:33.647 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=azkaban pat=*	
15:58:33.710 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dim pat=*
15:58:33.710 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dim pat=*	
15:58:33.714 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dm pat=*
15:58:33.714 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dm pat=*	
15:58:33.720 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dwd pat=*
15:58:33.720 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dwd pat=*	
15:58:33.726 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dws pat=*
15:58:33.726 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dws pat=*	
15:58:33.730 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_ods pat=*
15:58:33.730 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_ods pat=*	
15:58:33.733 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
15:58:33.733 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
15:58:33.738 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_release pat=*
15:58:33.738 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_release pat=*	
15:58:33.742 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_shop pat=*
15:58:33.742 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_shop pat=*	
15:58:33.745 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
15:58:33.745 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
15:58:33.749 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_shop pat=*
15:58:33.749 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_shop pat=*	
15:58:33.753 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
15:58:33.753 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
15:58:33.756 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_shop pat=*
15:58:33.756 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_shop pat=*	
15:58:33.761 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_release pat=*
15:58:33.761 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_release pat=*	
15:58:33.764 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_shop pat=*
15:58:33.764 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_shop pat=*	
15:58:33.769 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
15:58:33.769 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
15:58:33.772 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_shop pat=*
15:58:33.773 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_shop pat=*	
15:58:33.776 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dim pat=*
15:58:33.776 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dim pat=*	
15:58:33.779 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dm pat=*
15:58:33.779 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dm pat=*	
15:58:33.783 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dw pat=*
15:58:33.783 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dw pat=*	
15:58:33.786 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_rds pat=*
15:58:33.786 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_rds pat=*	
15:58:33.789 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=te pat=*
15:58:33.790 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=te pat=*	
15:58:33.792 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test pat=*
15:58:33.793 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test pat=*	
15:58:33.797 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test4 pat=*
15:58:33.797 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test4 pat=*	
15:58:34.464 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/cea9a9c8-d526-4434-92f5-69963211583a_resources
15:58:34.481 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/cea9a9c8-d526-4434-92f5-69963211583a
15:58:34.484 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/cea9a9c8-d526-4434-92f5-69963211583a
15:58:34.490 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/cea9a9c8-d526-4434-92f5-69963211583a/_tmp_space.db
15:58:34.493 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
15:58:34.503 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for root: Unknown error.
15:58:34.515 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:58:34.515 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
15:58:34.521 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
15:58:34.522 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
15:58:34.525 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
15:58:34.698 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/dd0689e6-a263-4f23-a669-787afceda4d7_resources
15:58:34.714 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/dd0689e6-a263-4f23-a669-787afceda4d7
15:58:34.720 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/dd0689e6-a263-4f23-a669-787afceda4d7
15:58:34.730 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/dd0689e6-a263-4f23-a669-787afceda4d7/_tmp_space.db
15:58:34.736 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
15:58:34.783 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
15:58:36.407 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
15:58:36.624 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
15:58:36.624 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
15:58:36.631 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
15:58:36.631 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
15:58:36.734 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
15:58:36.735 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
15:58:36.765 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:36.779 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:36.780 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:36.780 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:36.780 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:36.780 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:36.781 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:36.781 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:36.781 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:36.781 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
15:58:36.795 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
15:58:36.937 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Inferring case-sensitive schema for table ods_release.ods_01_release_session (inference mode: INFER_AND_SAVE)
15:58:36.939 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
15:58:36.939 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
15:58:36.944 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
15:58:36.945 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
15:58:36.972 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
15:58:36.972 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
15:58:36.998 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:36.999 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:36.999 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:37.000 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:37.000 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:37.000 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:37.000 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:37.001 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:37.001 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:37.001 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
15:58:37.022 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=ods_release tbl=ods_01_release_session
15:58:37.022 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_partitions : db=ods_release tbl=ods_01_release_session	
15:58:38.004 INFO  [main] org.apache.spark.SparkContext - Starting job: table at SparkHelper.scala:25
15:58:38.089 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (table at SparkHelper.scala:25) with 1 output partitions
15:58:38.091 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (table at SparkHelper.scala:25)
15:58:38.093 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
15:58:38.096 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
15:58:38.153 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[1] at table at SparkHelper.scala:25), which has no missing parents
15:58:38.455 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 75.1 KB, free 1445.6 MB)
15:58:42.780 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.3 KB, free 1445.6 MB)
15:58:42.783 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.145.1:5451 (size: 27.3 KB, free: 1445.7 MB)
15:58:42.800 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1004
15:58:42.844 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at table at SparkHelper.scala:25) (first 15 tasks are for partitions Vector(0))
15:58:42.846 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
15:58:42.949 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5016 bytes)
15:58:42.966 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
15:58:44.378 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1824 bytes result sent to driver
15:58:44.417 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1500 ms on localhost (executor driver) (1/1)
15:58:44.424 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
15:58:44.441 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (table at SparkHelper.scala:25) finished in 1.547 s
15:58:44.445 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: table at SparkHelper.scala:25, took 6.440481 s
15:58:44.500 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Saving case-sensitive schema for table ods_release.ods_01_release_session
15:58:44.501 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
15:58:44.501 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
15:58:44.506 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
15:58:44.507 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
15:58:44.532 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
15:58:44.533 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
15:58:44.558 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:44.558 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:44.558 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:44.558 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:44.558 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:44.559 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:44.559 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:44.559 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:44.559 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:44.560 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
15:58:44.591 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
15:58:44.592 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
15:58:44.622 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
15:58:44.622 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
15:58:44.656 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:44.657 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:44.657 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:44.657 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:44.658 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:44.658 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:44.659 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:44.659 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:44.660 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:44.660 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
15:58:44.778 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
15:58:44.778 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
15:58:45.084 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_table: db=ods_release tbl=ods_01_release_session newtbl=ods_01_release_session
15:58:45.084 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=ods_release tbl=ods_01_release_session newtbl=ods_01_release_session	
15:58:45.553 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
15:58:45.570 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
15:58:45.571 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num 
15:58:45.574 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
15:58:45.575 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources  
15:58:45.575 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels  
15:58:45.577 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard 
15:58:45.609 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\d{6})(\d{4})',2)as int)) as age 
15:58:45.644 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\d{16})(\d{1})',2)as int) % 2 as gender 
15:58:45.648 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code 
15:58:45.648 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude 
15:58:45.649 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude 
15:58:45.650 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id 
15:58:45.653 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code 
15:58:45.654 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version 
15:58:45.655 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid 
15:58:45.656 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
15:58:45.656 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
15:58:45.665 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:58:45.665 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
15:58:45.673 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:58:45.673 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
15:58:45.679 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:58:45.679 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
15:58:45.685 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:58:45.685 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
15:58:45.690 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:58:45.691 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
15:58:45.696 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:58:45.696 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
15:58:45.702 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:58:45.702 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
15:58:45.707 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:58:45.708 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
15:58:45.713 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:58:45.713 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
15:58:45.718 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:58:45.718 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
15:58:45.724 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:58:45.724 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
15:58:45.730 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:58:45.730 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
15:58:45.736 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:58:45.736 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
15:58:45.741 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
15:58:45.742 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
15:58:46.577 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
15:58:46.577 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
15:58:46.582 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
15:58:46.582 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
15:58:46.626 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
15:58:46.627 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
15:58:46.656 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:46.656 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:46.657 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:46.657 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:46.657 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:46.657 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:46.658 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:46.658 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:46.658 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
15:58:46.658 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
15:58:46.717 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
15:58:46.717 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
15:58:46.718 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
15:58:46.718 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
15:58:47.125 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190906)
15:58:47.128 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
15:58:47.131 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
15:58:47.233 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
15:58:47.253 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
15:58:48.583 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 514.6379 ms
15:58:48.738 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 72.9571 ms
15:58:48.883 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 310.9 KB, free 1445.3 MB)
15:58:49.042 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 26.7 KB, free 1445.3 MB)
15:58:49.043 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.145.1:5451 (size: 26.7 KB, free: 1445.6 MB)
15:58:49.044 INFO  [main] org.apache.spark.SparkContext - Created broadcast 1 from show at DWReleaseCustomer.scala:48
15:58:49.088 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 192.168.145.1:5451 in memory (size: 27.3 KB, free: 1445.7 MB)
15:58:49.119 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 48
15:58:49.137 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
15:58:49.246 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:48
15:58:49.250 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 6 (show at DWReleaseCustomer.scala:48)
15:58:49.253 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (show at DWReleaseCustomer.scala:48) with 1 output partitions
15:58:49.253 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (show at DWReleaseCustomer.scala:48)
15:58:49.253 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 1)
15:58:49.266 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
15:58:49.266 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[8] at show at DWReleaseCustomer.scala:48), which has no missing parents
15:58:49.277 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1445.4 MB)
15:58:49.279 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1445.4 MB)
15:58:49.280 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.145.1:5451 (size: 2.2 KB, free: 1445.7 MB)
15:58:49.280 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
15:58:49.281 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at DWReleaseCustomer.scala:48) (first 15 tasks are for partitions Vector(0))
15:58:49.281 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
15:58:49.283 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
15:58:49.284 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 1)
15:58:49.343 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
15:58:49.348 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 14 ms
15:58:49.378 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 1). 1182 bytes result sent to driver
15:58:49.380 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 1) in 98 ms on localhost (executor driver) (1/1)
15:58:49.380 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
15:58:49.382 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (show at DWReleaseCustomer.scala:48) finished in 0.099 s
15:58:49.382 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: show at DWReleaseCustomer.scala:48, took 0.135058 s
15:58:49.409 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:48
15:58:49.410 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 2 (show at DWReleaseCustomer.scala:48) with 3 output partitions
15:58:49.410 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 4 (show at DWReleaseCustomer.scala:48)
15:58:49.410 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 3)
15:58:49.413 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
15:58:49.413 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 4 (MapPartitionsRDD[8] at show at DWReleaseCustomer.scala:48), which has no missing parents
15:58:49.426 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.7 KB, free 1445.4 MB)
15:58:49.433 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1445.4 MB)
15:58:49.433 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.145.1:5451 (size: 2.2 KB, free: 1445.7 MB)
15:58:49.434 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1004
15:58:49.435 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 4 (MapPartitionsRDD[8] at show at DWReleaseCustomer.scala:48) (first 15 tasks are for partitions Vector(1, 2, 3))
15:58:49.435 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 3 tasks
15:58:49.436 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 2, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
15:58:49.437 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 4.0 (TID 3, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
15:58:49.438 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 4.0 (TID 4, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
15:58:49.439 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 2)
15:58:49.443 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 2.0 in stage 4.0 (TID 4)
15:58:49.443 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 1.0 in stage 4.0 (TID 3)
15:58:49.449 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
15:58:49.449 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
15:58:49.450 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
15:58:49.451 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
15:58:49.452 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 2). 1182 bytes result sent to driver
15:58:49.452 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 2.0 in stage 4.0 (TID 4). 1182 bytes result sent to driver
15:58:49.455 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
15:58:49.455 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 4.0 (TID 4) in 18 ms on localhost (executor driver) (1/3)
15:58:49.455 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
15:58:49.455 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 2) in 19 ms on localhost (executor driver) (2/3)
15:58:49.456 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 1.0 in stage 4.0 (TID 3). 1225 bytes result sent to driver
15:58:49.458 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 4.0 (TID 3) in 21 ms on localhost (executor driver) (3/3)
15:58:49.458 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
15:58:49.459 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 4 (show at DWReleaseCustomer.scala:48) finished in 0.024 s
15:58:49.461 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 2 finished: show at DWReleaseCustomer.scala:48, took 0.052403 s
15:58:49.494 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@6545c4a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
15:58:49.497 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.145.1:4040
15:58:49.509 INFO  [dispatcher-event-loop-3] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
15:58:49.519 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
15:58:49.519 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
15:58:49.520 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
15:58:49.522 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
15:58:49.525 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
15:58:49.528 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
15:58:49.529 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\Yongning Y\AppData\Local\Temp\spark-088c086f-27de-475e-ab4a-fc291e95747e
16:01:20.523 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
16:01:21.133 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_customer_job
16:01:21.167 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: YongningY,root
16:01:21.168 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: YongningY,root
16:01:21.169 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
16:01:21.170 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
16:01:21.170 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(YongningY, root); groups with view permissions: Set(); users  with modify permissions: Set(YongningY, root); groups with modify permissions: Set()
16:01:21.832 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 5503.
16:01:21.850 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
16:01:21.869 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
16:01:21.874 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
16:01:21.875 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
16:01:21.885 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\Yongning Y\AppData\Local\Temp\blockmgr-a0b9317e-3a89-42da-89de-0f6318f7e947
16:01:21.903 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1445.7 MB
16:01:21.970 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
16:01:22.082 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @4197ms
16:01:22.168 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
16:01:22.185 INFO  [main] org.spark_project.jetty.server.Server - Started @4302ms
16:01:22.210 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@3fc08eec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:01:22.210 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
16:01:22.235 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@492fc69e{/jobs,null,AVAILABLE,@Spark}
16:01:22.235 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d2260db{/jobs/json,null,AVAILABLE,@Spark}
16:01:22.236 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49bf29c6{/jobs/job,null,AVAILABLE,@Spark}
16:01:22.237 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7668d560{/jobs/job/json,null,AVAILABLE,@Spark}
16:01:22.238 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@126be319{/stages,null,AVAILABLE,@Spark}
16:01:22.239 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c371e13{/stages/json,null,AVAILABLE,@Spark}
16:01:22.240 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e34c607{/stages/stage,null,AVAILABLE,@Spark}
16:01:22.242 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9257031{/stages/stage/json,null,AVAILABLE,@Spark}
16:01:22.243 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7726e185{/stages/pool,null,AVAILABLE,@Spark}
16:01:22.245 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@282308c3{/stages/pool/json,null,AVAILABLE,@Spark}
16:01:22.246 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1db0ec27{/storage,null,AVAILABLE,@Spark}
16:01:22.247 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d4ab71a{/storage/json,null,AVAILABLE,@Spark}
16:01:22.248 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1af05b03{/storage/rdd,null,AVAILABLE,@Spark}
16:01:22.249 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ad777f{/storage/rdd/json,null,AVAILABLE,@Spark}
16:01:22.250 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@438bad7c{/environment,null,AVAILABLE,@Spark}
16:01:22.251 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4fdf8f12{/environment/json,null,AVAILABLE,@Spark}
16:01:22.252 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54f5f647{/executors,null,AVAILABLE,@Spark}
16:01:22.254 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a6d5a8f{/executors/json,null,AVAILABLE,@Spark}
16:01:22.255 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@315ba14a{/executors/threadDump,null,AVAILABLE,@Spark}
16:01:22.256 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27f0ad19{/executors/threadDump/json,null,AVAILABLE,@Spark}
16:01:22.272 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38d5b107{/static,null,AVAILABLE,@Spark}
16:01:22.273 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77e2a6e2{/,null,AVAILABLE,@Spark}
16:01:22.274 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@199e4c2b{/api,null,AVAILABLE,@Spark}
16:01:22.276 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/job/kill,null,AVAILABLE,@Spark}
16:01:22.277 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e7095ac{/stages/stage/kill,null,AVAILABLE,@Spark}
16:01:22.280 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.145.1:4040
16:01:22.466 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
16:01:22.492 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5524.
16:01:22.494 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.145.1:5524
16:01:22.495 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
16:01:22.524 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.145.1, 5524, None)
16:01:22.527 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.145.1:5524 with 1445.7 MB RAM, BlockManagerId(driver, 192.168.145.1, 5524, None)
16:01:22.530 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.145.1, 5524, None)
16:01:22.531 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.145.1, 5524, None)
16:01:22.816 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@dffa30b{/metrics/json,null,AVAILABLE,@Spark}
16:01:22.870 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/H:/IdeaProject/release_test/target/classes/hive-site.xml
16:01:22.906 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
16:01:22.908 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/user/hive/warehouse'.
16:01:22.917 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f443fae{/SQL,null,AVAILABLE,@Spark}
16:01:22.917 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79ab34c1{/SQL/json,null,AVAILABLE,@Spark}
16:01:22.918 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@782168b7{/SQL/execution,null,AVAILABLE,@Spark}
16:01:22.919 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7435a578{/SQL/execution/json,null,AVAILABLE,@Spark}
16:01:22.923 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b214b94{/static/sql,null,AVAILABLE,@Spark}
16:01:23.894 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
16:01:24.759 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16:01:24.792 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
16:01:25.853 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
16:01:27.202 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
16:01:27.205 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
16:01:27.460 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
16:01:27.464 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
16:01:27.520 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
16:01:27.629 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
16:01:27.631 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
16:01:27.645 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=azkaban pat=*
16:01:27.645 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=azkaban pat=*	
16:01:27.714 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dim pat=*
16:01:27.714 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dim pat=*	
16:01:27.719 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dm pat=*
16:01:27.719 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dm pat=*	
16:01:27.722 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dwd pat=*
16:01:27.723 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dwd pat=*	
16:01:27.727 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dws pat=*
16:01:27.728 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dws pat=*	
16:01:27.732 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_ods pat=*
16:01:27.732 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_ods pat=*	
16:01:27.736 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
16:01:27.736 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
16:01:27.740 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_release pat=*
16:01:27.740 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_release pat=*	
16:01:27.744 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_shop pat=*
16:01:27.744 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_shop pat=*	
16:01:27.747 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
16:01:27.747 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
16:01:27.751 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_shop pat=*
16:01:27.752 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_shop pat=*	
16:01:27.761 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
16:01:27.761 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
16:01:27.767 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_shop pat=*
16:01:27.767 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_shop pat=*	
16:01:27.772 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_release pat=*
16:01:27.772 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_release pat=*	
16:01:27.776 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_shop pat=*
16:01:27.776 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_shop pat=*	
16:01:27.781 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
16:01:27.781 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
16:01:27.785 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_shop pat=*
16:01:27.785 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_shop pat=*	
16:01:27.789 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dim pat=*
16:01:27.789 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dim pat=*	
16:01:27.792 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dm pat=*
16:01:27.793 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dm pat=*	
16:01:27.796 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dw pat=*
16:01:27.797 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dw pat=*	
16:01:27.800 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_rds pat=*
16:01:27.800 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_rds pat=*	
16:01:27.803 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=te pat=*
16:01:27.804 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=te pat=*	
16:01:27.807 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test pat=*
16:01:27.807 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test pat=*	
16:01:27.813 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test4 pat=*
16:01:27.813 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test4 pat=*	
16:01:28.568 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/40b3d79b-c71b-4180-bbe4-3dbff3b7712c_resources
16:01:28.585 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/40b3d79b-c71b-4180-bbe4-3dbff3b7712c
16:01:28.588 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/40b3d79b-c71b-4180-bbe4-3dbff3b7712c
16:01:28.593 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/40b3d79b-c71b-4180-bbe4-3dbff3b7712c/_tmp_space.db
16:01:28.597 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
16:01:28.609 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for root: Unknown error.
16:01:28.627 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:01:28.627 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:01:28.635 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
16:01:28.636 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
16:01:28.639 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
16:01:28.862 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/e4f31189-04d1-434a-9e9c-229ced2f292d_resources
16:01:28.889 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/e4f31189-04d1-434a-9e9c-229ced2f292d
16:01:28.893 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/e4f31189-04d1-434a-9e9c-229ced2f292d
16:01:28.900 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/e4f31189-04d1-434a-9e9c-229ced2f292d/_tmp_space.db
16:01:28.903 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
16:01:28.936 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
16:01:30.513 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
16:01:30.708 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
16:01:30.708 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
16:01:30.715 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:01:30.715 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:01:30.815 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:01:30.815 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:01:30.847 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:01:30.859 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:01:30.860 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:01:30.860 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:01:30.860 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:01:30.860 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:01:30.861 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:01:30.861 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:01:30.861 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:01:30.862 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:01:30.877 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
16:01:31.204 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
16:01:31.218 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
16:01:31.219 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num 
16:01:31.221 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
16:01:31.222 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources  
16:01:31.222 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels  
16:01:31.223 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard 
16:01:31.245 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2)as int)) as age 
16:01:31.287 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})',2)as int) % 2 as gender 
16:01:31.298 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code 
16:01:31.300 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude 
16:01:31.301 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude 
16:01:31.302 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id 
16:01:31.303 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code 
16:01:31.304 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version 
16:01:31.305 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid 
16:01:31.306 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
16:01:31.307 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
16:01:31.312 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:01:31.312 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:01:31.319 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:01:31.319 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:01:31.325 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:01:31.325 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:01:31.330 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:01:31.330 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:01:31.335 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:01:31.336 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:01:31.341 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:01:31.341 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:01:31.346 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:01:31.347 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:01:31.354 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:01:31.354 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:01:31.362 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:01:31.365 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:01:31.374 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:01:31.374 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:01:31.382 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:01:31.383 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:01:31.389 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:01:31.390 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:01:31.396 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:01:31.396 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:01:31.427 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:01:31.427 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:01:31.800 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
16:01:31.801 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
16:01:31.805 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:01:31.806 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:01:31.834 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:01:31.835 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:01:31.858 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:01:31.859 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:01:31.859 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:01:31.859 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:01:31.859 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:01:31.860 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:01:31.860 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:01:31.860 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:01:31.860 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:01:31.861 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:01:31.887 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
16:01:31.887 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
16:01:31.891 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
16:01:31.891 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
16:01:32.003 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190906)
16:01:32.005 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
16:01:32.008 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
16:01:32.023 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
16:01:32.030 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
16:01:32.543 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 315.9283 ms
16:01:32.575 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
16:01:32.842 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 71.2813 ms
16:01:32.959 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 310.9 KB, free 1445.4 MB)
16:01:33.056 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.7 KB, free 1445.4 MB)
16:01:33.061 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.145.1:5524 (size: 26.7 KB, free: 1445.7 MB)
16:01:33.065 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DWReleaseCustomer.scala:48
16:01:33.075 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
16:01:33.183 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:48
16:01:33.199 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at DWReleaseCustomer.scala:48)
16:01:33.202 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DWReleaseCustomer.scala:48) with 1 output partitions
16:01:33.203 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DWReleaseCustomer.scala:48)
16:01:33.203 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
16:01:33.205 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
16:01:33.209 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:48), which has no missing parents
16:01:33.250 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 1445.4 MB)
16:01:33.255 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1445.4 MB)
16:01:33.256 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.145.1:5524 (size: 2.2 KB, free: 1445.7 MB)
16:01:33.257 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
16:01:33.280 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:48) (first 15 tasks are for partitions Vector(0))
16:01:33.281 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
16:01:33.324 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
16:01:33.336 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
16:01:33.423 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
16:01:33.425 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 6 ms
16:01:33.445 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 1268 bytes result sent to driver
16:01:33.454 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 141 ms on localhost (executor driver) (1/1)
16:01:33.459 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
16:01:33.471 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DWReleaseCustomer.scala:48) finished in 0.168 s
16:01:33.480 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DWReleaseCustomer.scala:48, took 0.295489 s
16:01:33.490 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:48
16:01:33.492 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (show at DWReleaseCustomer.scala:48) with 3 output partitions
16:01:33.492 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (show at DWReleaseCustomer.scala:48)
16:01:33.492 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
16:01:33.492 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
16:01:33.494 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:48), which has no missing parents
16:01:33.505 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1445.4 MB)
16:01:33.532 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1445.4 MB)
16:01:33.538 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.145.1:5524 (size: 2.2 KB, free: 1445.7 MB)
16:01:33.539 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
16:01:33.545 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 3 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:48) (first 15 tasks are for partitions Vector(1, 2, 3))
16:01:33.545 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 3 tasks
16:01:33.549 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
16:01:33.550 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
16:01:33.551 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
16:01:33.552 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 1)
16:01:33.564 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
16:01:33.565 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
16:01:33.565 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 2)
16:01:33.565 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 3)
16:01:33.576 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 1). 1225 bytes result sent to driver
16:01:33.579 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
16:01:33.579 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:01:33.581 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
16:01:33.581 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
16:01:33.583 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 2). 1182 bytes result sent to driver
16:01:33.585 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 3). 1182 bytes result sent to driver
16:01:33.586 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 1) in 39 ms on localhost (executor driver) (1/3)
16:01:33.587 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 2) in 38 ms on localhost (executor driver) (2/3)
16:01:33.591 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 3) in 41 ms on localhost (executor driver) (3/3)
16:01:33.591 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
16:01:33.591 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (show at DWReleaseCustomer.scala:48) finished in 0.044 s
16:01:33.592 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: show at DWReleaseCustomer.scala:48, took 0.101423 s
16:01:33.614 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@3fc08eec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:01:33.622 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.145.1:4040
16:01:33.635 INFO  [dispatcher-event-loop-0] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
16:01:33.646 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
16:01:33.646 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
16:01:33.655 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
16:01:33.660 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
16:01:33.662 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
16:01:33.665 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
16:01:33.666 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\Yongning Y\AppData\Local\Temp\spark-296634fb-285c-4a81-a143-2a943c59c015
16:02:48.093 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
16:02:48.620 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_customer_job
16:02:48.646 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: YongningY,root
16:02:48.647 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: YongningY,root
16:02:48.648 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
16:02:48.648 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
16:02:48.649 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(YongningY, root); groups with view permissions: Set(); users  with modify permissions: Set(YongningY, root); groups with modify permissions: Set()
16:02:49.308 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 5567.
16:02:49.327 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
16:02:49.346 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
16:02:49.349 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
16:02:49.350 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
16:02:49.358 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\Yongning Y\AppData\Local\Temp\blockmgr-5a30e765-b210-46f0-a63e-ee1d6040b6b0
16:02:49.377 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1445.7 MB
16:02:49.443 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
16:02:49.559 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @4115ms
16:02:49.663 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
16:02:49.688 INFO  [main] org.spark_project.jetty.server.Server - Started @4246ms
16:02:49.727 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@35ce1810{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:02:49.728 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
16:02:49.760 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60b85ba1{/jobs,null,AVAILABLE,@Spark}
16:02:49.762 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b718392{/jobs/json,null,AVAILABLE,@Spark}
16:02:49.775 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f2d2181{/jobs/job,null,AVAILABLE,@Spark}
16:02:49.777 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fcdcf{/jobs/job/json,null,AVAILABLE,@Spark}
16:02:49.778 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46292372{/stages,null,AVAILABLE,@Spark}
16:02:49.780 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c44052e{/stages/json,null,AVAILABLE,@Spark}
16:02:49.782 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@530a8454{/stages/stage,null,AVAILABLE,@Spark}
16:02:49.786 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31198ceb{/stages/stage/json,null,AVAILABLE,@Spark}
16:02:49.787 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75201592{/stages/pool,null,AVAILABLE,@Spark}
16:02:49.792 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@aa5455e{/stages/pool/json,null,AVAILABLE,@Spark}
16:02:49.797 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5dda14d0{/storage,null,AVAILABLE,@Spark}
16:02:49.799 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d9fc57a{/storage/json,null,AVAILABLE,@Spark}
16:02:49.800 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b4ef7{/storage/rdd,null,AVAILABLE,@Spark}
16:02:49.801 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5987e932{/storage/rdd/json,null,AVAILABLE,@Spark}
16:02:49.803 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bbbdd4b{/environment,null,AVAILABLE,@Spark}
16:02:49.813 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25230246{/environment/json,null,AVAILABLE,@Spark}
16:02:49.814 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a8b5227{/executors,null,AVAILABLE,@Spark}
16:02:49.815 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6979efad{/executors/json,null,AVAILABLE,@Spark}
16:02:49.826 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67318f{/executors/threadDump,null,AVAILABLE,@Spark}
16:02:49.827 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17f9344b{/executors/threadDump/json,null,AVAILABLE,@Spark}
16:02:49.836 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54e81b21{/static,null,AVAILABLE,@Spark}
16:02:49.840 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2237bada{/,null,AVAILABLE,@Spark}
16:02:49.842 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5710768a{/api,null,AVAILABLE,@Spark}
16:02:49.843 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bf61e67{/jobs/job/kill,null,AVAILABLE,@Spark}
16:02:49.844 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b273a59{/stages/stage/kill,null,AVAILABLE,@Spark}
16:02:49.846 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.145.1:4040
16:02:50.001 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
16:02:50.066 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5588.
16:02:50.067 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.145.1:5588
16:02:50.071 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
16:02:50.112 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.145.1, 5588, None)
16:02:50.118 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.145.1:5588 with 1445.7 MB RAM, BlockManagerId(driver, 192.168.145.1, 5588, None)
16:02:50.123 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.145.1, 5588, None)
16:02:50.124 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.145.1, 5588, None)
16:02:50.386 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f02251{/metrics/json,null,AVAILABLE,@Spark}
16:02:50.445 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/H:/IdeaProject/release_test/target/classes/hive-site.xml
16:02:50.471 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
16:02:50.472 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/user/hive/warehouse'.
16:02:50.479 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64138b0c{/SQL,null,AVAILABLE,@Spark}
16:02:50.480 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22d9c961{/SQL/json,null,AVAILABLE,@Spark}
16:02:50.481 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79ab34c1{/SQL/execution,null,AVAILABLE,@Spark}
16:02:50.482 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@281f23f2{/SQL/execution/json,null,AVAILABLE,@Spark}
16:02:50.484 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6093d508{/static/sql,null,AVAILABLE,@Spark}
16:02:51.408 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
16:02:52.228 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16:02:52.259 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
16:02:53.263 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
16:02:54.778 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
16:02:54.785 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
16:02:55.047 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
16:02:55.051 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
16:02:55.107 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
16:02:55.225 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
16:02:55.227 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
16:02:55.241 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=azkaban pat=*
16:02:55.241 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=azkaban pat=*	
16:02:55.313 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dim pat=*
16:02:55.313 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dim pat=*	
16:02:55.317 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dm pat=*
16:02:55.317 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dm pat=*	
16:02:55.322 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dwd pat=*
16:02:55.322 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dwd pat=*	
16:02:55.326 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dws pat=*
16:02:55.327 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dws pat=*	
16:02:55.330 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_ods pat=*
16:02:55.331 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_ods pat=*	
16:02:55.334 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
16:02:55.334 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
16:02:55.338 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_release pat=*
16:02:55.339 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_release pat=*	
16:02:55.342 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_shop pat=*
16:02:55.342 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_shop pat=*	
16:02:55.346 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
16:02:55.347 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
16:02:55.350 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_shop pat=*
16:02:55.350 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_shop pat=*	
16:02:55.354 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
16:02:55.354 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
16:02:55.358 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_shop pat=*
16:02:55.358 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_shop pat=*	
16:02:55.362 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_release pat=*
16:02:55.363 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_release pat=*	
16:02:55.366 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_shop pat=*
16:02:55.366 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_shop pat=*	
16:02:55.371 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
16:02:55.371 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
16:02:55.374 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_shop pat=*
16:02:55.375 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_shop pat=*	
16:02:55.378 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dim pat=*
16:02:55.378 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dim pat=*	
16:02:55.381 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dm pat=*
16:02:55.381 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dm pat=*	
16:02:55.384 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dw pat=*
16:02:55.384 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dw pat=*	
16:02:55.387 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_rds pat=*
16:02:55.388 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_rds pat=*	
16:02:55.391 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=te pat=*
16:02:55.391 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=te pat=*	
16:02:55.393 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test pat=*
16:02:55.394 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test pat=*	
16:02:55.397 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test4 pat=*
16:02:55.397 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test4 pat=*	
16:02:56.066 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/cbb42aaa-970d-43e2-9cdb-90295db1c44e_resources
16:02:56.083 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/cbb42aaa-970d-43e2-9cdb-90295db1c44e
16:02:56.084 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/cbb42aaa-970d-43e2-9cdb-90295db1c44e
16:02:56.090 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/cbb42aaa-970d-43e2-9cdb-90295db1c44e/_tmp_space.db
16:02:56.093 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
16:02:56.105 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for root: Unknown error.
16:02:56.116 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:02:56.116 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:02:56.122 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
16:02:56.123 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
16:02:56.126 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
16:02:56.346 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/1f1f2e1c-6b13-4931-bdd7-f2c1275e54c2_resources
16:02:56.361 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/1f1f2e1c-6b13-4931-bdd7-f2c1275e54c2
16:02:56.364 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/1f1f2e1c-6b13-4931-bdd7-f2c1275e54c2
16:02:56.414 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/1f1f2e1c-6b13-4931-bdd7-f2c1275e54c2/_tmp_space.db
16:02:56.425 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
16:02:56.457 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
16:02:57.959 ERROR [main] com.qf.bigdata.release.util.SparkHelper$ - Text '2019-09-06' could not be parsed at index 4
java.time.format.DateTimeParseException: Text '2019-09-06' could not be parsed at index 4
	at java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1949)
	at java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1851)
	at java.time.LocalDate.parse(LocalDate.java:400)
	at com.qf.bigdata.release.util.DateUtil$.dateFormat4String(DateUtil.scala:17)
	at com.qf.bigdata.release.util.SparkHelper$.rangeDates(SparkHelper.scala:69)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$.handleJobs(DWReleaseCustomer.scala:81)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$.main(DWReleaseCustomer.scala:104)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer.main(DWReleaseCustomer.scala)
16:02:57.966 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@35ce1810{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:02:57.968 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.145.1:4040
16:02:57.978 INFO  [dispatcher-event-loop-3] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
16:02:57.985 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
16:02:57.985 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
16:02:57.991 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
16:02:57.994 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
16:02:57.996 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
16:02:57.999 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
16:02:58.000 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\Yongning Y\AppData\Local\Temp\spark-67d41d5a-ca68-4d79-ab5c-47b2e082319d
16:03:27.546 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
16:03:27.901 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_customer_job
16:03:27.921 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: YongningY,root
16:03:27.921 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: YongningY,root
16:03:27.922 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
16:03:27.922 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
16:03:27.923 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(YongningY, root); groups with view permissions: Set(); users  with modify permissions: Set(YongningY, root); groups with modify permissions: Set()
16:03:28.514 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 5624.
16:03:28.535 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
16:03:28.552 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
16:03:28.555 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
16:03:28.555 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
16:03:28.563 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\Yongning Y\AppData\Local\Temp\blockmgr-216f5092-bfd0-463c-8c9a-faac5d95381b
16:03:28.582 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1445.7 MB
16:03:28.647 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
16:03:28.753 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @3840ms
16:03:28.862 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
16:03:28.883 INFO  [main] org.spark_project.jetty.server.Server - Started @3971ms
16:03:28.917 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@5472a23a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:03:28.917 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
16:03:28.949 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60b85ba1{/jobs,null,AVAILABLE,@Spark}
16:03:28.949 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b718392{/jobs/json,null,AVAILABLE,@Spark}
16:03:28.950 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f2d2181{/jobs/job,null,AVAILABLE,@Spark}
16:03:28.951 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fcdcf{/jobs/job/json,null,AVAILABLE,@Spark}
16:03:28.952 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46292372{/stages,null,AVAILABLE,@Spark}
16:03:28.953 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c44052e{/stages/json,null,AVAILABLE,@Spark}
16:03:28.954 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@530a8454{/stages/stage,null,AVAILABLE,@Spark}
16:03:28.966 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31198ceb{/stages/stage/json,null,AVAILABLE,@Spark}
16:03:28.968 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75201592{/stages/pool,null,AVAILABLE,@Spark}
16:03:28.969 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@aa5455e{/stages/pool/json,null,AVAILABLE,@Spark}
16:03:28.970 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5dda14d0{/storage,null,AVAILABLE,@Spark}
16:03:28.971 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d9fc57a{/storage/json,null,AVAILABLE,@Spark}
16:03:28.972 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b4ef7{/storage/rdd,null,AVAILABLE,@Spark}
16:03:28.973 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5987e932{/storage/rdd/json,null,AVAILABLE,@Spark}
16:03:28.974 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bbbdd4b{/environment,null,AVAILABLE,@Spark}
16:03:28.975 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25230246{/environment/json,null,AVAILABLE,@Spark}
16:03:28.976 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a8b5227{/executors,null,AVAILABLE,@Spark}
16:03:28.982 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6979efad{/executors/json,null,AVAILABLE,@Spark}
16:03:28.983 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67318f{/executors/threadDump,null,AVAILABLE,@Spark}
16:03:28.984 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17f9344b{/executors/threadDump/json,null,AVAILABLE,@Spark}
16:03:29.007 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54e81b21{/static,null,AVAILABLE,@Spark}
16:03:29.008 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2237bada{/,null,AVAILABLE,@Spark}
16:03:29.009 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5710768a{/api,null,AVAILABLE,@Spark}
16:03:29.010 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bf61e67{/jobs/job/kill,null,AVAILABLE,@Spark}
16:03:29.011 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b273a59{/stages/stage/kill,null,AVAILABLE,@Spark}
16:03:29.020 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.145.1:4040
16:03:29.147 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
16:03:29.249 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5645.
16:03:29.253 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.145.1:5645
16:03:29.255 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
16:03:29.308 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.145.1, 5645, None)
16:03:29.312 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.145.1:5645 with 1445.7 MB RAM, BlockManagerId(driver, 192.168.145.1, 5645, None)
16:03:29.315 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.145.1, 5645, None)
16:03:29.316 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.145.1, 5645, None)
16:03:29.552 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d4a65c6{/metrics/json,null,AVAILABLE,@Spark}
16:03:29.604 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/H:/IdeaProject/release_test/target/classes/hive-site.xml
16:03:29.630 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
16:03:29.631 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/user/hive/warehouse'.
16:03:29.637 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d74c3ba{/SQL,null,AVAILABLE,@Spark}
16:03:29.637 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64138b0c{/SQL/json,null,AVAILABLE,@Spark}
16:03:29.638 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f443fae{/SQL/execution,null,AVAILABLE,@Spark}
16:03:29.639 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79ab34c1{/SQL/execution/json,null,AVAILABLE,@Spark}
16:03:29.641 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29f0c4f2{/static/sql,null,AVAILABLE,@Spark}
16:03:30.714 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
16:03:31.507 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16:03:31.542 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
16:03:32.564 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
16:03:33.972 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
16:03:33.975 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
16:03:34.245 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
16:03:34.248 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
16:03:34.302 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
16:03:34.421 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
16:03:34.423 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
16:03:34.437 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=azkaban pat=*
16:03:34.437 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=azkaban pat=*	
16:03:34.496 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dim pat=*
16:03:34.496 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dim pat=*	
16:03:34.501 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dm pat=*
16:03:34.501 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dm pat=*	
16:03:34.506 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dwd pat=*
16:03:34.506 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dwd pat=*	
16:03:34.510 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dws pat=*
16:03:34.511 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dws pat=*	
16:03:34.524 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_ods pat=*
16:03:34.525 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_ods pat=*	
16:03:34.530 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
16:03:34.530 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
16:03:34.534 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_release pat=*
16:03:34.534 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_release pat=*	
16:03:34.537 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_shop pat=*
16:03:34.538 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_shop pat=*	
16:03:34.542 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
16:03:34.542 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
16:03:34.547 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_shop pat=*
16:03:34.547 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_shop pat=*	
16:03:34.550 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
16:03:34.551 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
16:03:34.554 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_shop pat=*
16:03:34.554 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_shop pat=*	
16:03:34.558 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_release pat=*
16:03:34.558 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_release pat=*	
16:03:34.561 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_shop pat=*
16:03:34.562 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_shop pat=*	
16:03:34.566 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
16:03:34.566 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
16:03:34.569 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_shop pat=*
16:03:34.570 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_shop pat=*	
16:03:34.573 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dim pat=*
16:03:34.573 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dim pat=*	
16:03:34.576 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dm pat=*
16:03:34.577 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dm pat=*	
16:03:34.580 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dw pat=*
16:03:34.581 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dw pat=*	
16:03:34.583 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_rds pat=*
16:03:34.584 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_rds pat=*	
16:03:34.587 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=te pat=*
16:03:34.587 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=te pat=*	
16:03:34.589 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test pat=*
16:03:34.590 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test pat=*	
16:03:34.593 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test4 pat=*
16:03:34.593 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test4 pat=*	
16:03:35.284 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/e6865d9a-a39e-4b4e-8a40-ad5896c6c5a5_resources
16:03:35.300 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/e6865d9a-a39e-4b4e-8a40-ad5896c6c5a5
16:03:35.302 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/e6865d9a-a39e-4b4e-8a40-ad5896c6c5a5
16:03:35.307 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/e6865d9a-a39e-4b4e-8a40-ad5896c6c5a5/_tmp_space.db
16:03:35.311 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
16:03:35.322 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for root: Unknown error.
16:03:35.334 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:03:35.335 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:03:35.341 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
16:03:35.341 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
16:03:35.344 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
16:03:35.530 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/0e47bc0b-b666-4423-a467-50b680930a34_resources
16:03:35.567 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/0e47bc0b-b666-4423-a467-50b680930a34
16:03:35.570 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/0e47bc0b-b666-4423-a467-50b680930a34
16:03:35.588 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/0e47bc0b-b666-4423-a467-50b680930a34/_tmp_space.db
16:03:35.591 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
16:03:35.627 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
16:03:37.233 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
16:03:37.446 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
16:03:37.447 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
16:03:37.454 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:03:37.454 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:03:37.555 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:03:37.555 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:03:37.587 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:03:37.598 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:03:37.599 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:03:37.599 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:03:37.599 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:03:37.599 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:03:37.600 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:03:37.600 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:03:37.600 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:03:37.600 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:03:37.615 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
16:03:37.960 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
16:03:37.974 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
16:03:37.975 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num 
16:03:37.977 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
16:03:37.978 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources  
16:03:37.979 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels  
16:03:37.979 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard 
16:03:37.996 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2)as int)) as age 
16:03:38.032 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})',2)as int) % 2 as gender 
16:03:38.042 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code 
16:03:38.043 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude 
16:03:38.044 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude 
16:03:38.045 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id 
16:03:38.046 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code 
16:03:38.047 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version 
16:03:38.048 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid 
16:03:38.049 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
16:03:38.049 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
16:03:38.054 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:03:38.055 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:03:38.063 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:03:38.063 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:03:38.069 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:03:38.069 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:03:38.074 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:03:38.075 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:03:38.081 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:03:38.082 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:03:38.089 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:03:38.089 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:03:38.096 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:03:38.097 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:03:38.102 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:03:38.102 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:03:38.107 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:03:38.107 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:03:38.114 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:03:38.114 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:03:38.119 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:03:38.119 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:03:38.124 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:03:38.124 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:03:38.129 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:03:38.130 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:03:38.135 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:03:38.136 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
16:03:38.564 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
16:03:38.564 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
16:03:38.568 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:03:38.568 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:03:38.594 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:03:38.594 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:03:38.619 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:03:38.619 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:03:38.619 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:03:38.620 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:03:38.620 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:03:38.620 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:03:38.620 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:03:38.620 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:03:38.620 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:03:38.620 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:03:38.640 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
16:03:38.641 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
16:03:38.644 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
16:03:38.644 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
16:03:38.912 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 2019-09-06)
16:03:38.914 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
16:03:38.917 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
16:03:38.928 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
16:03:38.933 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
16:03:39.255 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
16:03:39.447 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 342.0256 ms
16:03:39.709 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 51.969 ms
16:03:39.828 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 310.9 KB, free 1445.4 MB)
16:03:39.929 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.7 KB, free 1445.4 MB)
16:03:39.932 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.145.1:5645 (size: 26.7 KB, free: 1445.7 MB)
16:03:39.936 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DWReleaseCustomer.scala:48
16:03:39.945 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 11968796 bytes, open cost is considered as scanning 4194304 bytes.
16:03:40.206 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:48
16:03:40.228 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at DWReleaseCustomer.scala:48)
16:03:40.232 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DWReleaseCustomer.scala:48) with 1 output partitions
16:03:40.232 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DWReleaseCustomer.scala:48)
16:03:40.233 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
16:03:40.236 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
16:03:40.243 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DWReleaseCustomer.scala:48), which has no missing parents
16:03:40.571 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 24.0 KB, free 1445.3 MB)
16:03:40.575 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.4 KB, free 1445.3 MB)
16:03:40.575 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.145.1:5645 (size: 10.4 KB, free: 1445.7 MB)
16:03:40.576 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
16:03:40.599 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DWReleaseCustomer.scala:48) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
16:03:40.600 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 4 tasks
16:03:40.665 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5390 bytes)
16:03:40.668 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5390 bytes)
16:03:40.668 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5390 bytes)
16:03:40.669 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5390 bytes)
16:03:40.676 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
16:03:40.676 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
16:03:40.676 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
16:03:40.676 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
16:03:40.974 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 43.4463 ms
16:03:41.082 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-06]
16:03:41.082 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-06]
16:03:41.082 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-06]
16:03:41.082 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-06]
16:03:42.071 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:03:42.071 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:03:42.137 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:03:42.365 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1568 bytes result sent to driver
16:03:42.365 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1568 bytes result sent to driver
16:03:42.621 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1939 ms on localhost (executor driver) (1/4)
16:03:42.628 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 1959 ms on localhost (executor driver) (2/4)
16:03:44.356 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:03:44.953 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1611 bytes result sent to driver
16:03:44.958 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 4290 ms on localhost (executor driver) (3/4)
16:04:05.017 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1826 bytes result sent to driver
16:04:05.029 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 24362 ms on localhost (executor driver) (4/4)
16:04:06.187 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (show at DWReleaseCustomer.scala:48) finished in 24.388 s
16:04:06.360 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
16:04:06.862 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
16:04:07.462 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
16:04:08.628 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
16:04:09.520 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
16:04:14.988 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:48), which has no missing parents
16:04:17.897 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1445.3 MB)
16:04:17.900 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1445.3 MB)
16:04:17.901 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.145.1:5645 (size: 2.2 KB, free: 1445.7 MB)
16:04:17.904 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
16:04:18.729 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:48) (first 15 tasks are for partitions Vector(0))
16:04:18.729 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
16:04:18.738 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 4, localhost, executor driver, partition 0, ANY, 4726 bytes)
16:04:18.739 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 4)
16:04:29.957 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
16:04:31.318 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 6000 ms
16:04:40.404 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 4). 2779 bytes result sent to driver
16:04:40.408 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 4) in 21674 ms on localhost (executor driver) (1/1)
16:04:40.408 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
16:04:40.410 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DWReleaseCustomer.scala:48) finished in 21.677 s
16:04:40.511 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DWReleaseCustomer.scala:48, took 60.304340 s
16:04:40.958 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@5472a23a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:04:40.995 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.145.1:4040
16:04:41.294 INFO  [dispatcher-event-loop-3] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
16:04:41.462 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
16:04:41.468 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
16:04:41.469 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
16:04:41.634 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
16:04:41.666 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
16:04:41.804 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
16:04:41.937 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\Yongning Y\AppData\Local\Temp\spark-c7ba6977-8ee4-4ae0-ab41-7c6a4c352b87
18:57:40.202 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
18:57:42.237 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_customer_job
18:57:42.992 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: YongningY,root
18:57:42.998 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: YongningY,root
18:57:43.007 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
18:57:43.010 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
18:57:43.048 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(YongningY, root); groups with view permissions: Set(); users  with modify permissions: Set(YongningY, root); groups with modify permissions: Set()
18:57:44.861 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 7467.
18:57:44.978 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
18:57:45.122 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
18:57:45.144 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18:57:45.145 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
18:57:45.196 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\Yongning Y\AppData\Local\Temp\blockmgr-82da41ae-ab38-401c-a17f-05bed95ac369
18:57:45.303 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1445.7 MB
18:57:45.430 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
18:57:45.721 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @10533ms
18:57:45.907 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
18:57:45.924 INFO  [main] org.spark_project.jetty.server.Server - Started @10738ms
18:57:46.003 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@3fc08eec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
18:57:46.004 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
18:57:46.047 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@492fc69e{/jobs,null,AVAILABLE,@Spark}
18:57:46.048 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d2260db{/jobs/json,null,AVAILABLE,@Spark}
18:57:46.049 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49bf29c6{/jobs/job,null,AVAILABLE,@Spark}
18:57:46.050 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7668d560{/jobs/job/json,null,AVAILABLE,@Spark}
18:57:46.050 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@126be319{/stages,null,AVAILABLE,@Spark}
18:57:46.051 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c371e13{/stages/json,null,AVAILABLE,@Spark}
18:57:46.052 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e34c607{/stages/stage,null,AVAILABLE,@Spark}
18:57:46.054 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9257031{/stages/stage/json,null,AVAILABLE,@Spark}
18:57:46.055 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7726e185{/stages/pool,null,AVAILABLE,@Spark}
18:57:46.056 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@282308c3{/stages/pool/json,null,AVAILABLE,@Spark}
18:57:46.057 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1db0ec27{/storage,null,AVAILABLE,@Spark}
18:57:46.058 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d4ab71a{/storage/json,null,AVAILABLE,@Spark}
18:57:46.059 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1af05b03{/storage/rdd,null,AVAILABLE,@Spark}
18:57:46.060 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ad777f{/storage/rdd/json,null,AVAILABLE,@Spark}
18:57:46.060 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@438bad7c{/environment,null,AVAILABLE,@Spark}
18:57:46.061 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4fdf8f12{/environment/json,null,AVAILABLE,@Spark}
18:57:46.062 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54f5f647{/executors,null,AVAILABLE,@Spark}
18:57:46.063 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a6d5a8f{/executors/json,null,AVAILABLE,@Spark}
18:57:46.064 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@315ba14a{/executors/threadDump,null,AVAILABLE,@Spark}
18:57:46.065 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27f0ad19{/executors/threadDump/json,null,AVAILABLE,@Spark}
18:57:46.084 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38d5b107{/static,null,AVAILABLE,@Spark}
18:57:46.086 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77e2a6e2{/,null,AVAILABLE,@Spark}
18:57:46.087 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@199e4c2b{/api,null,AVAILABLE,@Spark}
18:57:46.088 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/job/kill,null,AVAILABLE,@Spark}
18:57:46.089 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e7095ac{/stages/stage/kill,null,AVAILABLE,@Spark}
18:57:46.109 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.145.1:4040
18:57:46.579 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
18:57:46.701 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7488.
18:57:46.740 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.145.1:7488
18:57:46.752 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18:57:46.801 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.145.1, 7488, None)
18:57:46.816 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.145.1:7488 with 1445.7 MB RAM, BlockManagerId(driver, 192.168.145.1, 7488, None)
18:57:46.824 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.145.1, 7488, None)
18:57:46.825 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.145.1, 7488, None)
18:57:47.068 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@dffa30b{/metrics/json,null,AVAILABLE,@Spark}
18:57:47.229 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/H:/IdeaProject/release_test/target/classes/hive-site.xml
18:57:47.285 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
18:57:47.285 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/user/hive/warehouse'.
18:57:47.327 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f443fae{/SQL,null,AVAILABLE,@Spark}
18:57:47.328 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79ab34c1{/SQL/json,null,AVAILABLE,@Spark}
18:57:47.330 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@782168b7{/SQL/execution,null,AVAILABLE,@Spark}
18:57:47.330 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7435a578{/SQL/execution/json,null,AVAILABLE,@Spark}
18:57:47.340 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b214b94{/static/sql,null,AVAILABLE,@Spark}
18:57:49.687 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
18:57:50.733 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
18:57:50.792 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
18:57:52.267 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
18:57:53.974 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
18:57:53.976 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
18:57:54.396 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
18:57:54.401 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
18:57:54.469 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
18:57:54.622 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
18:57:54.624 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
18:57:54.637 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=azkaban pat=*
18:57:54.638 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=azkaban pat=*	
18:57:54.696 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dim pat=*
18:57:54.696 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dim pat=*	
18:57:54.700 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dm pat=*
18:57:54.701 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dm pat=*	
18:57:54.705 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dwd pat=*
18:57:54.706 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dwd pat=*	
18:57:54.710 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dws pat=*
18:57:54.710 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dws pat=*	
18:57:54.714 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_ods pat=*
18:57:54.714 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_ods pat=*	
18:57:54.718 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
18:57:54.718 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
18:57:54.722 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_release pat=*
18:57:54.722 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_release pat=*	
18:57:54.725 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_shop pat=*
18:57:54.725 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_shop pat=*	
18:57:54.729 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
18:57:54.729 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
18:57:54.733 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_shop pat=*
18:57:54.733 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_shop pat=*	
18:57:54.737 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
18:57:54.737 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
18:57:54.741 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_shop pat=*
18:57:54.741 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_shop pat=*	
18:57:54.744 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_release pat=*
18:57:54.744 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_release pat=*	
18:57:54.748 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_shop pat=*
18:57:54.748 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_shop pat=*	
18:57:54.753 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
18:57:54.753 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
18:57:54.756 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_shop pat=*
18:57:54.757 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_shop pat=*	
18:57:54.761 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dim pat=*
18:57:54.761 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dim pat=*	
18:57:54.764 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dm pat=*
18:57:54.764 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dm pat=*	
18:57:54.768 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dw pat=*
18:57:54.768 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dw pat=*	
18:57:54.771 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_rds pat=*
18:57:54.772 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_rds pat=*	
18:57:54.775 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=te pat=*
18:57:54.775 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=te pat=*	
18:57:54.777 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test pat=*
18:57:54.777 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test pat=*	
18:57:54.781 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test4 pat=*
18:57:54.781 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test4 pat=*	
18:57:57.786 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/ccc692ba-83f1-41ab-9c31-22949975396b_resources
18:57:57.810 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/ccc692ba-83f1-41ab-9c31-22949975396b
18:57:57.812 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/ccc692ba-83f1-41ab-9c31-22949975396b
18:57:57.819 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/ccc692ba-83f1-41ab-9c31-22949975396b/_tmp_space.db
18:57:57.822 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
18:57:57.858 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for root: Unknown error.
18:57:57.911 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:57:57.911 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
18:57:57.917 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
18:57:57.918 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
18:57:57.921 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
18:57:58.283 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/bb4fdb5b-f555-4386-a146-ad9f690f54c6_resources
18:57:58.290 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/bb4fdb5b-f555-4386-a146-ad9f690f54c6
18:57:58.292 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/bb4fdb5b-f555-4386-a146-ad9f690f54c6
18:57:58.300 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/bb4fdb5b-f555-4386-a146-ad9f690f54c6/_tmp_space.db
18:57:58.303 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
18:57:58.604 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
18:58:00.533 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
18:58:00.900 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
18:58:00.900 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
18:58:00.908 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
18:58:00.908 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
18:58:01.180 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
18:58:01.180 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
18:58:01.222 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:01.248 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:01.249 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:01.249 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:01.249 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:01.249 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:01.250 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:01.250 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:01.250 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:01.250 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
18:58:01.326 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
18:58:02.376 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
18:58:02.390 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
18:58:02.391 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num 
18:58:02.393 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
18:58:02.394 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources  
18:58:02.395 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels  
18:58:02.395 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard 
18:58:02.444 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2)as int)) as age 
18:58:02.474 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})',2)as int) % 2 as gender 
18:58:02.477 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code 
18:58:02.478 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude 
18:58:02.479 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude 
18:58:02.480 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id 
18:58:02.481 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code 
18:58:02.482 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version 
18:58:02.483 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid 
18:58:02.483 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
18:58:02.484 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
18:58:02.511 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:58:02.512 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
18:58:02.519 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:58:02.520 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
18:58:02.525 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:58:02.525 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
18:58:02.531 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:58:02.531 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
18:58:02.537 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:58:02.537 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
18:58:02.543 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:58:02.543 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
18:58:02.549 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:58:02.550 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
18:58:02.557 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:58:02.557 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
18:58:02.563 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:58:02.563 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
18:58:02.569 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:58:02.570 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
18:58:02.575 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:58:02.576 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
18:58:02.581 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:58:02.582 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
18:58:02.590 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:58:02.590 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
18:58:02.600 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:58:02.600 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
18:58:03.424 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
18:58:03.424 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
18:58:03.428 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
18:58:03.429 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
18:58:03.455 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
18:58:03.455 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
18:58:03.479 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:03.479 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:03.479 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:03.480 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:03.480 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:03.480 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:03.480 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:03.480 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:03.480 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:03.480 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
18:58:03.546 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
18:58:03.547 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
18:58:03.557 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
18:58:03.557 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
18:58:04.574 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 2019-09-06)
18:58:04.577 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
18:58:04.579 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
18:58:04.640 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
18:58:04.656 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
18:58:05.488 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 304.1118 ms
18:58:05.931 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 67.5345 ms
18:58:06.289 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 310.9 KB, free 1445.4 MB)
18:58:07.280 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.7 KB, free 1445.4 MB)
18:58:07.289 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.145.1:7488 (size: 26.7 KB, free: 1445.7 MB)
18:58:07.312 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DWReleaseCustomer.scala:49
18:58:07.373 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 11968796 bytes, open cost is considered as scanning 4194304 bytes.
18:58:07.732 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:49
18:58:07.877 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at DWReleaseCustomer.scala:49)
18:58:07.890 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DWReleaseCustomer.scala:49) with 1 output partitions
18:58:07.892 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DWReleaseCustomer.scala:49)
18:58:07.894 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
18:58:07.905 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
18:58:07.973 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DWReleaseCustomer.scala:49), which has no missing parents
18:58:08.269 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 24.0 KB, free 1445.3 MB)
18:58:08.272 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.4 KB, free 1445.3 MB)
18:58:08.273 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.145.1:7488 (size: 10.4 KB, free: 1445.7 MB)
18:58:08.274 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
18:58:08.304 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DWReleaseCustomer.scala:49) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
18:58:08.305 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 4 tasks
18:58:08.402 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5390 bytes)
18:58:08.406 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5390 bytes)
18:58:08.408 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5390 bytes)
18:58:08.409 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5390 bytes)
18:58:08.446 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
18:58:08.446 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
18:58:08.446 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
18:58:08.446 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
18:58:08.769 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
18:58:08.802 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 49.9835 ms
18:58:08.869 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-06]
18:58:08.869 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-06]
18:58:08.870 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-06]
18:58:08.870 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-06]
18:58:10.408 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
18:58:10.408 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
18:58:10.408 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
18:58:10.472 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
18:58:10.900 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1654 bytes result sent to driver
18:58:10.900 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1654 bytes result sent to driver
18:58:10.900 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1654 bytes result sent to driver
18:58:10.918 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 2509 ms on localhost (executor driver) (1/4)
18:58:10.921 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 2512 ms on localhost (executor driver) (2/4)
18:58:10.921 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 2549 ms on localhost (executor driver) (3/4)
18:58:14.213 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1740 bytes result sent to driver
18:58:14.215 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 5809 ms on localhost (executor driver) (4/4)
18:58:14.229 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
18:58:14.230 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (show at DWReleaseCustomer.scala:49) finished in 5.877 s
18:58:14.231 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
18:58:14.231 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
18:58:14.243 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
18:58:14.246 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
18:58:14.260 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:49), which has no missing parents
18:58:14.270 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1445.3 MB)
18:58:14.272 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1445.3 MB)
18:58:14.274 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.145.1:7488 (size: 2.2 KB, free: 1445.7 MB)
18:58:14.275 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
18:58:14.277 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:49) (first 15 tasks are for partitions Vector(0))
18:58:14.277 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
18:58:14.280 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 4, localhost, executor driver, partition 0, ANY, 4726 bytes)
18:58:14.280 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 4)
18:58:14.490 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
18:58:14.493 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 38 ms
18:58:14.561 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 4). 2521 bytes result sent to driver
18:58:14.561 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 4) in 284 ms on localhost (executor driver) (1/1)
18:58:14.562 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
18:58:14.562 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DWReleaseCustomer.scala:49) finished in 0.285 s
18:58:14.587 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DWReleaseCustomer.scala:49, took 6.853628 s
18:58:14.844 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
18:58:14.855 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
18:58:14.855 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
18:58:14.882 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:14.883 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:14.883 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:14.883 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:14.883 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:14.883 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:14.883 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:14.883 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:14.884 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
18:58:14.884 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:14.884 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:14.884 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:14.884 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:14.884 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:14.884 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:14.884 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:14.884 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:14.885 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
18:58:15.060 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://mycluster/data/release/dw/release_customer/.hive-staging_hive_2019-09-10_18-58-15_057_8790561570673403795-1
18:58:15.475 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
18:58:15.475 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
18:58:15.483 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
18:58:15.483 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
18:58:15.510 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
18:58:15.510 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
18:58:15.541 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:15.541 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:15.542 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:15.542 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:15.542 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:15.542 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:15.542 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:15.543 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:15.543 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:15.543 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
18:58:15.546 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
18:58:15.546 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
18:58:15.546 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
18:58:15.546 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
18:58:15.601 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 2019-09-06)
18:58:15.602 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
18:58:15.603 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
18:58:15.603 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
18:58:15.604 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
18:58:15.705 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
18:58:15.707 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
18:58:15.802 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 54.8662 ms
18:58:15.818 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 310.9 KB, free 1445.0 MB)
18:58:15.841 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 26.7 KB, free 1445.0 MB)
18:58:15.841 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.145.1:7488 (size: 26.7 KB, free: 1445.6 MB)
18:58:15.843 INFO  [main] org.apache.spark.SparkContext - Created broadcast 3 from insertInto at SparkHelper.scala:36
18:58:15.844 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 11968796 bytes, open cost is considered as scanning 4194304 bytes.
18:58:15.963 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:36
18:58:15.965 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (insertInto at SparkHelper.scala:36)
18:58:15.965 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (insertInto at SparkHelper.scala:36) with 4 output partitions
18:58:15.965 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (insertInto at SparkHelper.scala:36)
18:58:15.965 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
18:58:15.965 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
18:58:15.967 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:36), which has no missing parents
18:58:15.975 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 24.0 KB, free 1445.0 MB)
18:58:15.979 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.4 KB, free 1445.0 MB)
18:58:15.979 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.145.1:7488 (size: 10.4 KB, free: 1445.6 MB)
18:58:15.980 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1004
18:58:15.981 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:36) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
18:58:15.981 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 4 tasks
18:58:15.983 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 5, localhost, executor driver, partition 0, ANY, 5390 bytes)
18:58:15.984 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 6, localhost, executor driver, partition 1, ANY, 5390 bytes)
18:58:15.985 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 2.0 (TID 7, localhost, executor driver, partition 2, ANY, 5390 bytes)
18:58:15.985 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 2.0 (TID 8, localhost, executor driver, partition 3, ANY, 5390 bytes)
18:58:15.985 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 5)
18:58:15.985 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 6)
18:58:15.985 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 2.0 in stage 2.0 (TID 7)
18:58:15.986 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 3.0 in stage 2.0 (TID 8)
18:58:16.003 INFO  [Executor task launch worker for task 8] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-06]
18:58:16.019 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-06]
18:58:16.027 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-06]
18:58:16.034 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-06]
18:58:16.051 INFO  [Executor task launch worker for task 8] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
18:58:16.069 INFO  [Executor task launch worker for task 5] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
18:58:16.088 INFO  [Executor task launch worker for task 7] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
18:58:16.089 INFO  [Executor task launch worker for task 6] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
18:58:16.110 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 5). 1611 bytes result sent to driver
18:58:16.111 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 5) in 129 ms on localhost (executor driver) (1/4)
18:58:16.112 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 3.0 in stage 2.0 (TID 8). 1568 bytes result sent to driver
18:58:16.113 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 2.0 (TID 8) in 128 ms on localhost (executor driver) (2/4)
18:58:16.139 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 2.0 in stage 2.0 (TID 7). 1568 bytes result sent to driver
18:58:16.145 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 81
18:58:16.147 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 2.0 (TID 7) in 163 ms on localhost (executor driver) (3/4)
18:58:16.213 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.145.1:7488 in memory (size: 2.2 KB, free: 1445.6 MB)
18:58:17.244 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 7
18:58:17.245 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 3
18:58:17.247 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 192.168.145.1:7488 in memory (size: 26.7 KB, free: 1445.7 MB)
18:58:17.250 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 8
18:58:17.250 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 1
18:58:17.250 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 2
18:58:17.254 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.145.1:7488 in memory (size: 10.4 KB, free: 1445.7 MB)
18:58:17.260 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 6
18:58:17.260 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 5
18:58:17.303 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 0
18:58:17.303 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 4
18:58:18.777 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 1.0 in stage 2.0 (TID 6). 1740 bytes result sent to driver
18:58:18.778 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 6) in 2794 ms on localhost (executor driver) (4/4)
18:58:18.779 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
18:58:18.780 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (insertInto at SparkHelper.scala:36) finished in 2.797 s
18:58:18.780 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
18:58:18.780 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
18:58:18.780 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 3)
18:58:18.780 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
18:58:18.780 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[14] at insertInto at SparkHelper.scala:36), which has no missing parents
18:58:18.816 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 181.6 KB, free 1445.2 MB)
18:58:18.818 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 67.0 KB, free 1445.1 MB)
18:58:18.819 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.145.1:7488 (size: 67.0 KB, free: 1445.6 MB)
18:58:18.819 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1004
18:58:18.820 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at insertInto at SparkHelper.scala:36) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
18:58:18.820 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 4 tasks
18:58:18.821 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 9, localhost, executor driver, partition 0, ANY, 4726 bytes)
18:58:18.821 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 10, localhost, executor driver, partition 1, ANY, 4726 bytes)
18:58:18.821 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 11, localhost, executor driver, partition 2, ANY, 4726 bytes)
18:58:18.822 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 3.0 (TID 12, localhost, executor driver, partition 3, ANY, 4726 bytes)
18:58:18.822 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 10)
18:58:18.822 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 9)
18:58:18.822 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 11)
18:58:18.822 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 3.0 in stage 3.0 (TID 12)
18:58:18.870 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
18:58:18.870 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
18:58:18.879 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
18:58:18.879 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
18:58:18.891 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
18:58:18.892 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
18:58:18.893 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
18:58:18.893 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
18:58:18.933 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.1923 ms
18:58:18.961 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.2927 ms
18:58:19.164 INFO  [Executor task launch worker for task 11] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
18:58:19.165 INFO  [Executor task launch worker for task 9] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
18:58:19.171 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
18:58:19.172 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
18:58:19.174 INFO  [Executor task launch worker for task 12] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
18:58:19.175 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
18:58:19.176 INFO  [Executor task launch worker for task 10] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
18:58:19.177 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
18:58:19.195 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 9.3854 ms
18:58:19.250 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 28.4294 ms
18:58:19.278 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 24.6906 ms
18:58:19.548 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@397ed48b
18:58:19.549 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2674cf83
18:58:19.549 INFO  [Executor task launch worker for task 12] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7a7a1905
18:58:19.549 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@12d78f80
18:58:19.590 INFO  [Executor task launch worker for task 9] org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
18:58:19.611 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
18:58:19.611 INFO  [Executor task launch worker for task 12] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
18:58:19.611 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
18:58:19.611 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
18:58:19.612 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://mycluster/data/release/dw/release_customer/.hive-staging_hive_2019-09-10_18-58-15_057_8790561570673403795-1/-ext-10000/_temporary/0/_temporary/attempt_20190910185819_0003_m_000001_0/bdp_day=2019-09-06/part-00001-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000
18:58:19.612 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://mycluster/data/release/dw/release_customer/.hive-staging_hive_2019-09-10_18-58-15_057_8790561570673403795-1/-ext-10000/_temporary/0/_temporary/attempt_20190910185819_0003_m_000000_0/bdp_day=2019-09-06/part-00000-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000
18:58:19.612 INFO  [Executor task launch worker for task 12] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://mycluster/data/release/dw/release_customer/.hive-staging_hive_2019-09-10_18-58-15_057_8790561570673403795-1/-ext-10000/_temporary/0/_temporary/attempt_20190910185819_0003_m_000003_0/bdp_day=2019-09-06/part-00003-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000
18:58:19.612 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://mycluster/data/release/dw/release_customer/.hive-staging_hive_2019-09-10_18-58-15_057_8790561570673403795-1/-ext-10000/_temporary/0/_temporary/attempt_20190910185819_0003_m_000002_0/bdp_day=2019-09-06/part-00002-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000
18:58:19.620 INFO  [Executor task launch worker for task 12] parquet.hadoop.codec.CodecConfig - Compression set to false
18:58:19.620 INFO  [Executor task launch worker for task 9] parquet.hadoop.codec.CodecConfig - Compression set to false
18:58:19.620 INFO  [Executor task launch worker for task 10] parquet.hadoop.codec.CodecConfig - Compression set to false
18:58:19.620 INFO  [Executor task launch worker for task 11] parquet.hadoop.codec.CodecConfig - Compression set to false
18:58:19.620 INFO  [Executor task launch worker for task 10] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
18:58:19.620 INFO  [Executor task launch worker for task 12] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
18:58:19.620 INFO  [Executor task launch worker for task 9] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
18:58:19.620 INFO  [Executor task launch worker for task 11] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
18:58:19.621 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
18:58:19.621 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
18:58:19.621 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
18:58:19.621 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
18:58:19.622 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
18:58:19.622 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
18:58:19.622 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
18:58:19.622 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
18:58:19.622 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
18:58:19.622 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
18:58:19.622 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
18:58:19.622 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
18:58:19.622 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Dictionary is on
18:58:19.622 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Dictionary is on
18:58:19.622 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Dictionary is on
18:58:19.622 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Dictionary is on
18:58:19.622 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Validation is off
18:58:19.622 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Validation is off
18:58:19.622 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Validation is off
18:58:19.622 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Validation is off
18:58:19.622 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
18:58:19.622 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
18:58:19.625 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
18:58:19.625 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
18:58:20.318 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@55617d79
18:58:20.318 INFO  [Executor task launch worker for task 12] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@2b500c40
18:58:20.319 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@5be18f71
18:58:20.319 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@2278508
18:58:20.974 INFO  [Executor task launch worker for task 9] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,797
18:58:20.980 INFO  [Executor task launch worker for task 10] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,740,125
18:58:20.992 INFO  [Executor task launch worker for task 12] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,862
18:58:20.995 INFO  [Executor task launch worker for task 11] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,277
18:58:21.257 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
18:58:21.258 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
18:58:21.259 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 579,154B for [release_session] BINARY: 21,447 values, 579,077B raw, 579,077B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
18:58:21.259 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
18:58:21.264 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
18:58:21.266 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
18:58:21.266 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
18:58:21.266 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,447 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
18:58:21.266 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
18:58:21.267 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 5,445B for [device_type] BINARY: 21,446 values, 5,414B raw, 5,414B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
18:58:21.267 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 5,445B for [device_type] BINARY: 21,446 values, 5,414B raw, 5,414B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
18:58:21.267 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 214,521B for [device_num] BINARY: 21,447 values, 214,478B raw, 214,478B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
18:58:21.267 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
18:58:21.268 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 5,447B for [device_type] BINARY: 21,447 values, 5,416B raw, 5,416B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
18:58:21.268 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
18:58:21.268 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,447 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
18:58:21.268 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
18:58:21.268 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
18:58:21.272 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
18:58:21.278 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
18:58:21.279 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
18:58:21.280 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 41 entries, 164B raw, 41B comp}
18:58:21.280 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
18:58:21.281 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 41 entries, 164B raw, 41B comp}
18:58:21.283 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 2,790B for [gender] BINARY: 21,446 values, 2,759B raw, 2,759B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 10B raw, 2B comp}
18:58:21.283 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 5,445B for [device_type] BINARY: 21,446 values, 5,414B raw, 5,414B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
18:58:21.283 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 2,792B for [gender] BINARY: 21,446 values, 2,761B raw, 2,761B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 10B raw, 2B comp}
18:58:21.284 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 3,206B for [area_code] BINARY: 21,446 values, 3,165B raw, 3,165B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 20B raw, 2B comp}
18:58:21.284 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
18:58:21.285 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 479 entries, 7,244B raw, 479B comp}
18:58:21.286 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 339 entries, 5,248B raw, 339B comp}
18:58:21.286 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
18:58:21.268 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,447 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
18:58:21.286 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 3,279B for [area_code] BINARY: 21,446 values, 3,238B raw, 3,238B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 20B raw, 2B comp}
18:58:21.287 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
18:58:21.287 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 10 entries, 61B raw, 10B comp}
18:58:21.288 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 479 entries, 7,244B raw, 479B comp}
18:58:21.291 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 41 entries, 164B raw, 41B comp}
18:58:21.291 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 10 entries, 71B raw, 10B comp}
18:58:21.291 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 339 entries, 5,248B raw, 339B comp}
18:58:21.292 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 2,794B for [gender] BINARY: 21,446 values, 2,763B raw, 2,763B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 10B raw, 2B comp}
18:58:21.292 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 10 entries, 61B raw, 10B comp}
18:58:21.292 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 10 entries, 61B raw, 10B comp}
18:58:21.292 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 3,208B for [area_code] BINARY: 21,446 values, 3,167B raw, 3,167B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 20B raw, 2B comp}
18:58:21.292 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 10 entries, 71B raw, 10B comp}
18:58:21.292 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 479 entries, 7,244B raw, 479B comp}
18:58:21.292 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 10 entries, 61B raw, 10B comp}
18:58:21.293 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 339 entries, 5,248B raw, 339B comp}
18:58:21.293 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 5 entries, 40B raw, 5B comp}
18:58:21.293 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1,357 entries, 10,856B raw, 1,357B comp}
18:58:21.294 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 5 entries, 40B raw, 5B comp}
18:58:21.294 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 10 entries, 61B raw, 10B comp}
18:58:21.295 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 471,909B for [idcard] BINARY: 21,447 values, 471,842B raw, 471,842B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
18:58:21.297 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1,357 entries, 10,856B raw, 1,357B comp}
18:58:21.297 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 10 entries, 71B raw, 10B comp}
18:58:21.297 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,447 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 41 entries, 164B raw, 41B comp}
18:58:21.297 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 10 entries, 61B raw, 10B comp}
18:58:21.298 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 2,803B for [gender] BINARY: 21,447 values, 2,772B raw, 2,772B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 10B raw, 2B comp}
18:58:21.298 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 5 entries, 40B raw, 5B comp}
18:58:21.298 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 3,272B for [area_code] BINARY: 21,447 values, 3,231B raw, 3,231B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 20B raw, 2B comp}
18:58:21.298 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1,357 entries, 10,856B raw, 1,357B comp}
18:58:21.299 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,447 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 479 entries, 7,244B raw, 479B comp}
18:58:21.299 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,447 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 339 entries, 5,248B raw, 339B comp}
18:58:21.299 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 10 entries, 61B raw, 10B comp}
18:58:21.299 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 10 entries, 71B raw, 10B comp}
18:58:21.300 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 10 entries, 61B raw, 10B comp}
18:58:21.300 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,447 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 5 entries, 40B raw, 5B comp}
18:58:21.300 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,447 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1,357 entries, 10,856B raw, 1,357B comp}
18:58:23.645 INFO  [Executor task launch worker for task 12] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190910185819_0003_m_000003_0' to hdfs://mycluster/data/release/dw/release_customer/.hive-staging_hive_2019-09-10_18-58-15_057_8790561570673403795-1/-ext-10000/_temporary/0/task_20190910185819_0003_m_000003
18:58:23.645 INFO  [Executor task launch worker for task 9] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190910185819_0003_m_000000_0' to hdfs://mycluster/data/release/dw/release_customer/.hive-staging_hive_2019-09-10_18-58-15_057_8790561570673403795-1/-ext-10000/_temporary/0/task_20190910185819_0003_m_000000
18:58:23.646 INFO  [Executor task launch worker for task 9] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190910185819_0003_m_000000_0: Committed
18:58:23.646 INFO  [Executor task launch worker for task 12] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190910185819_0003_m_000003_0: Committed
18:58:23.650 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 9). 2660 bytes result sent to driver
18:58:23.651 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 3.0 in stage 3.0 (TID 12). 2660 bytes result sent to driver
18:58:23.652 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 9) in 4831 ms on localhost (executor driver) (1/4)
18:58:23.653 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 3.0 (TID 12) in 4832 ms on localhost (executor driver) (2/4)
18:58:23.751 INFO  [Executor task launch worker for task 11] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190910185819_0003_m_000002_0' to hdfs://mycluster/data/release/dw/release_customer/.hive-staging_hive_2019-09-10_18-58-15_057_8790561570673403795-1/-ext-10000/_temporary/0/task_20190910185819_0003_m_000002
18:58:23.751 INFO  [Executor task launch worker for task 11] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190910185819_0003_m_000002_0: Committed
18:58:23.752 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 11). 2617 bytes result sent to driver
18:58:23.752 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 11) in 4931 ms on localhost (executor driver) (3/4)
18:58:23.962 INFO  [Executor task launch worker for task 10] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190910185819_0003_m_000001_0' to hdfs://mycluster/data/release/dw/release_customer/.hive-staging_hive_2019-09-10_18-58-15_057_8790561570673403795-1/-ext-10000/_temporary/0/task_20190910185819_0003_m_000001
18:58:23.962 INFO  [Executor task launch worker for task 10] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190910185819_0003_m_000001_0: Committed
18:58:23.963 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 10). 2617 bytes result sent to driver
18:58:23.966 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 10) in 5145 ms on localhost (executor driver) (4/4)
18:58:23.966 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
18:58:23.967 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (insertInto at SparkHelper.scala:36) finished in 5.147 s
18:58:23.967 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: insertInto at SparkHelper.scala:36, took 8.004922 s
18:58:24.072 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
18:58:24.073 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
18:58:24.073 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
18:58:24.098 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
18:58:24.098 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
18:58:24.123 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:24.123 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:24.123 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:24.124 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:24.124 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:24.124 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:24.124 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:24.124 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:24.124 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
18:58:24.125 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:24.125 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:24.125 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:24.125 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:24.125 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:24.125 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:24.126 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:24.126 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:24.126 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
18:58:24.128 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
18:58:24.128 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
18:58:24.422 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
18:58:24.423 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
18:58:24.425 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
18:58:24.425 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
18:58:24.426 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
18:58:24.426 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
18:58:24.426 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
18:58:24.427 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
18:58:24.468 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
18:58:24.468 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
18:58:24.493 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_customer[2019-09-06]
18:58:24.494 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_customer[2019-09-06]	
18:58:24.533 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/000000_0
18:58:24.537 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
18:58:24.590 ERROR [main] org.apache.hadoop.hdfs.KeyProviderCache - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
18:58:24.609 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://mycluster/data/release/dw/release_customer/.hive-staging_hive_2019-09-10_18-58-15_057_8790561570673403795-1/-ext-10000/bdp_day=2019-09-06/part-00000-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, dest: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00000-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, Status:true
18:58:24.644 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://mycluster/data/release/dw/release_customer/.hive-staging_hive_2019-09-10_18-58-15_057_8790561570673403795-1/-ext-10000/bdp_day=2019-09-06/part-00001-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, dest: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00001-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, Status:true
18:58:24.657 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://mycluster/data/release/dw/release_customer/.hive-staging_hive_2019-09-10_18-58-15_057_8790561570673403795-1/-ext-10000/bdp_day=2019-09-06/part-00002-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, dest: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00002-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, Status:true
18:58:24.671 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://mycluster/data/release/dw/release_customer/.hive-staging_hive_2019-09-10_18-58-15_057_8790561570673403795-1/-ext-10000/bdp_day=2019-09-06/part-00003-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, dest: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00003-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, Status:true
18:58:24.675 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_customer[2019-09-06]
18:58:24.676 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_customer[2019-09-06]	
18:58:24.707 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_partition : db=dw_release tbl=dw_release_customer
18:58:24.707 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=alter_partition : db=dw_release tbl=dw_release_customer	
18:58:24.707 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - New partition values:[2019-09-06]
18:58:24.791 WARN  [main] hive.log - Updating partition stats fast for: dw_release_customer
18:58:24.793 WARN  [main] hive.log - Updated size to 5782541
18:58:25.039 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://mycluster/data/release/dw/release_customer/.hive-staging_hive_2019-09-10_18-58-15_057_8790561570673403795-1/-ext-10000/bdp_day=2019-09-06 with partSpec {bdp_day=2019-09-06}
18:58:25.118 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_customer`
18:58:25.122 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
18:58:25.122 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: dw_release	
18:58:25.130 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
18:58:25.130 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
18:58:25.156 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
18:58:25.156 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
18:58:25.180 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.180 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.180 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.180 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.181 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.181 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.181 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.181 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.181 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
18:58:25.181 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.182 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.182 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.182 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.183 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.183 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.183 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.183 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.183 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
18:58:25.221 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Inferring case-sensitive schema for table dw_release.dw_release_customer (inference mode: INFER_AND_SAVE)
18:58:25.222 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
18:58:25.222 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: dw_release	
18:58:25.226 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
18:58:25.227 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
18:58:25.267 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
18:58:25.267 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
18:58:25.302 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.303 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.303 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.303 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.303 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.304 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.304 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.304 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.304 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
18:58:25.304 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.305 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.305 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.305 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.305 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.305 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.305 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.306 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.306 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
18:58:25.309 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=dw_release tbl=dw_release_customer
18:58:25.309 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_partitions : db=dw_release tbl=dw_release_customer	
18:58:25.520 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:36
18:58:25.520 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 2 (insertInto at SparkHelper.scala:36) with 1 output partitions
18:58:25.521 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 4 (insertInto at SparkHelper.scala:36)
18:58:25.521 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
18:58:25.521 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
18:58:25.521 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 4 (MapPartitionsRDD[16] at insertInto at SparkHelper.scala:36), which has no missing parents
18:58:25.551 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 75.1 KB, free 1445.0 MB)
18:58:25.554 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 27.4 KB, free 1445.0 MB)
18:58:25.555 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 192.168.145.1:7488 (size: 27.4 KB, free: 1445.6 MB)
18:58:25.556 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1004
18:58:25.557 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[16] at insertInto at SparkHelper.scala:36) (first 15 tasks are for partitions Vector(0))
18:58:25.557 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
18:58:25.561 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 13, localhost, executor driver, partition 0, PROCESS_LOCAL, 5047 bytes)
18:58:25.562 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 13)
18:58:25.688 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 13). 2021 bytes result sent to driver
18:58:25.690 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 13) in 132 ms on localhost (executor driver) (1/1)
18:58:25.690 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
18:58:25.690 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 4 (insertInto at SparkHelper.scala:36) finished in 0.132 s
18:58:25.691 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 2 finished: insertInto at SparkHelper.scala:36, took 0.170949 s
18:58:25.706 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Saving case-sensitive schema for table dw_release.dw_release_customer
18:58:25.707 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
18:58:25.707 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: dw_release	
18:58:25.712 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
18:58:25.712 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
18:58:25.835 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
18:58:25.835 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
18:58:25.870 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.871 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.872 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.873 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.875 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.875 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.875 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.875 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.876 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
18:58:25.876 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.876 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.876 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.876 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.877 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.877 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.877 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.877 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:25.877 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
18:58:25.983 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
18:58:25.983 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
18:58:26.014 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
18:58:26.014 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
18:58:26.036 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:26.036 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:26.037 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:26.037 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:26.037 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:26.037 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:26.037 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:26.037 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:26.037 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
18:58:26.037 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:26.038 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:26.038 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:26.038 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:26.038 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:26.038 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:26.038 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:26.038 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:58:26.038 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
18:58:26.048 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
18:58:26.048 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
18:58:26.142 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_table: db=dw_release tbl=dw_release_customer newtbl=dw_release_customer
18:58:26.143 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=dw_release tbl=dw_release_customer newtbl=dw_release_customer	
18:58:26.282 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@3fc08eec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
18:58:26.283 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.145.1:4040
18:58:26.330 INFO  [dispatcher-event-loop-1] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
18:58:26.356 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
18:58:26.357 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
18:58:26.357 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
18:58:26.359 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
18:58:26.362 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
18:58:26.372 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
18:58:26.373 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\Yongning Y\AppData\Local\Temp\spark-f67c9fe3-7ae4-4532-86d6-f20385c7b1cf
19:41:28.389 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
19:41:28.934 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_exposure_job
19:41:28.961 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: YongningY,root
19:41:28.962 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: YongningY,root
19:41:28.962 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
19:41:28.963 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
19:41:28.964 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(YongningY, root); groups with view permissions: Set(); users  with modify permissions: Set(YongningY, root); groups with modify permissions: Set()
19:41:29.671 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 7723.
19:41:29.694 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
19:41:29.714 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
19:41:29.717 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19:41:29.718 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
19:41:29.729 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\Yongning Y\AppData\Local\Temp\blockmgr-d8a2deb9-250c-4f47-8a74-75c2984893f0
19:41:29.754 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1445.7 MB
19:41:29.819 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
19:41:29.966 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @6150ms
19:41:30.063 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
19:41:30.084 INFO  [main] org.spark_project.jetty.server.Server - Started @6270ms
19:41:30.120 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@6b81f75b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19:41:30.120 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
19:41:30.152 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60b85ba1{/jobs,null,AVAILABLE,@Spark}
19:41:30.153 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b718392{/jobs/json,null,AVAILABLE,@Spark}
19:41:30.153 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f2d2181{/jobs/job,null,AVAILABLE,@Spark}
19:41:30.155 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fcdcf{/jobs/job/json,null,AVAILABLE,@Spark}
19:41:30.157 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46292372{/stages,null,AVAILABLE,@Spark}
19:41:30.165 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c44052e{/stages/json,null,AVAILABLE,@Spark}
19:41:30.165 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@530a8454{/stages/stage,null,AVAILABLE,@Spark}
19:41:30.167 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31198ceb{/stages/stage/json,null,AVAILABLE,@Spark}
19:41:30.168 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75201592{/stages/pool,null,AVAILABLE,@Spark}
19:41:30.172 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@aa5455e{/stages/pool/json,null,AVAILABLE,@Spark}
19:41:30.175 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5dda14d0{/storage,null,AVAILABLE,@Spark}
19:41:30.178 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d9fc57a{/storage/json,null,AVAILABLE,@Spark}
19:41:30.185 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b4ef7{/storage/rdd,null,AVAILABLE,@Spark}
19:41:30.186 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5987e932{/storage/rdd/json,null,AVAILABLE,@Spark}
19:41:30.188 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bbbdd4b{/environment,null,AVAILABLE,@Spark}
19:41:30.190 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25230246{/environment/json,null,AVAILABLE,@Spark}
19:41:30.191 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a8b5227{/executors,null,AVAILABLE,@Spark}
19:41:30.195 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6979efad{/executors/json,null,AVAILABLE,@Spark}
19:41:30.198 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67318f{/executors/threadDump,null,AVAILABLE,@Spark}
19:41:30.199 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17f9344b{/executors/threadDump/json,null,AVAILABLE,@Spark}
19:41:30.210 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54e81b21{/static,null,AVAILABLE,@Spark}
19:41:30.211 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2237bada{/,null,AVAILABLE,@Spark}
19:41:30.212 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5710768a{/api,null,AVAILABLE,@Spark}
19:41:30.214 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bf61e67{/jobs/job/kill,null,AVAILABLE,@Spark}
19:41:30.215 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b273a59{/stages/stage/kill,null,AVAILABLE,@Spark}
19:41:30.219 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.145.1:4040
19:41:30.357 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
19:41:30.400 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7744.
19:41:30.401 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.145.1:7744
19:41:30.403 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19:41:30.445 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.145.1, 7744, None)
19:41:30.452 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.145.1:7744 with 1445.7 MB RAM, BlockManagerId(driver, 192.168.145.1, 7744, None)
19:41:30.461 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.145.1, 7744, None)
19:41:30.462 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.145.1, 7744, None)
19:41:30.789 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f02251{/metrics/json,null,AVAILABLE,@Spark}
19:41:30.853 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/H:/IdeaProject/release_test/target/classes/hive-site.xml
19:41:30.912 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
19:41:30.914 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/user/hive/warehouse'.
19:41:30.924 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@bdc8014{/SQL,null,AVAILABLE,@Spark}
19:41:30.925 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73ba6fe6{/SQL/json,null,AVAILABLE,@Spark}
19:41:30.927 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28d79cba{/SQL/execution,null,AVAILABLE,@Spark}
19:41:30.927 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29f0c4f2{/SQL/execution/json,null,AVAILABLE,@Spark}
19:41:30.929 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bfe3203{/static/sql,null,AVAILABLE,@Spark}
19:41:32.042 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
19:41:32.994 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19:41:33.028 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
19:41:34.407 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19:41:36.432 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
19:41:36.435 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
19:41:36.731 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
19:41:36.735 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
19:41:36.802 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
19:41:36.950 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
19:41:36.952 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
19:41:36.969 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=azkaban pat=*
19:41:36.969 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=azkaban pat=*	
19:41:37.026 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dim pat=*
19:41:37.027 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dim pat=*	
19:41:37.030 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dm pat=*
19:41:37.031 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dm pat=*	
19:41:37.034 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dwd pat=*
19:41:37.034 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dwd pat=*	
19:41:37.039 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dws pat=*
19:41:37.039 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dws pat=*	
19:41:37.043 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_ods pat=*
19:41:37.044 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_ods pat=*	
19:41:37.048 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
19:41:37.048 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19:41:37.053 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_release pat=*
19:41:37.053 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_release pat=*	
19:41:37.056 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_shop pat=*
19:41:37.057 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_shop pat=*	
19:41:37.060 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
19:41:37.060 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
19:41:37.064 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_shop pat=*
19:41:37.064 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_shop pat=*	
19:41:37.068 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
19:41:37.068 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
19:41:37.073 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_shop pat=*
19:41:37.073 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_shop pat=*	
19:41:37.077 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_release pat=*
19:41:37.077 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_release pat=*	
19:41:37.080 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_shop pat=*
19:41:37.080 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_shop pat=*	
19:41:37.085 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
19:41:37.085 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
19:41:37.089 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_shop pat=*
19:41:37.089 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_shop pat=*	
19:41:37.094 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dim pat=*
19:41:37.094 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dim pat=*	
19:41:37.097 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dm pat=*
19:41:37.098 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dm pat=*	
19:41:37.102 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dw pat=*
19:41:37.102 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dw pat=*	
19:41:37.105 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_rds pat=*
19:41:37.106 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_rds pat=*	
19:41:37.109 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=te pat=*
19:41:37.109 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=te pat=*	
19:41:37.112 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test pat=*
19:41:37.112 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test pat=*	
19:41:37.116 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test4 pat=*
19:41:37.117 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test4 pat=*	
19:41:38.930 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/a6352b88-87ef-4afb-99be-16022e742256_resources
19:41:38.948 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/a6352b88-87ef-4afb-99be-16022e742256
19:41:38.950 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/a6352b88-87ef-4afb-99be-16022e742256
19:41:38.956 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/a6352b88-87ef-4afb-99be-16022e742256/_tmp_space.db
19:41:38.960 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
19:41:38.976 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for root: Unknown error.
19:41:38.991 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:41:38.992 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
19:41:38.999 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
19:41:38.999 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
19:41:39.002 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
19:41:39.266 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/44f17a4a-2ffb-4cbb-b569-6ba5dc123612_resources
19:41:39.272 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/44f17a4a-2ffb-4cbb-b569-6ba5dc123612
19:41:39.275 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/44f17a4a-2ffb-4cbb-b569-6ba5dc123612
19:41:39.283 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/44f17a4a-2ffb-4cbb-b569-6ba5dc123612/_tmp_space.db
19:41:39.285 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
19:41:39.320 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
19:41:41.145 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
19:41:41.360 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
19:41:41.361 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
19:41:41.369 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:41:41.369 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:41:41.477 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:41:41.477 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:41:41.510 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:41:41.525 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:41:41.525 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:41:41.525 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:41:41.526 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:41:41.526 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:41:41.526 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:41:41.527 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:41:41.527 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:41:41.528 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:41:41.543 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
19:41:41.930 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
19:41:41.948 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
19:41:41.949 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
19:41:41.951 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
19:41:41.952 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
19:41:41.953 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
19:41:41.955 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
19:41:41.955 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
19:41:42.335 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
19:41:42.336 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
19:41:42.341 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:41:42.341 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:41:42.370 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:41:42.371 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:41:42.401 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:41:42.402 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:41:42.402 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:41:42.402 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:41:42.402 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:41:42.403 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:41:42.403 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:41:42.403 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:41:42.403 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:41:42.404 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:41:42.427 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
19:41:42.427 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
19:41:42.432 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
19:41:42.432 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
19:41:42.731 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 2019-09-06)
19:41:42.733 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 03)
19:41:42.735 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
19:41:42.750 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
19:41:42.758 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
19:41:43.293 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 336.7573 ms
19:41:43.354 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
19:41:43.662 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 87.5943 ms
19:41:43.849 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 310.4 KB, free 1445.4 MB)
19:41:43.964 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.7 KB, free 1445.4 MB)
19:41:43.967 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.145.1:7744 (size: 26.7 KB, free: 1445.7 MB)
19:41:43.971 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DWReleaseExposure.scala:49
19:41:43.983 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 11968796 bytes, open cost is considered as scanning 4194304 bytes.
19:41:44.141 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseExposure.scala:49
19:41:44.163 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 3 (show at DWReleaseExposure.scala:49)
19:41:44.167 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DWReleaseExposure.scala:49) with 1 output partitions
19:41:44.168 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DWReleaseExposure.scala:49)
19:41:44.169 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
19:41:44.172 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
19:41:44.182 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at show at DWReleaseExposure.scala:49), which has no missing parents
19:41:44.317 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 17.6 KB, free 1445.4 MB)
19:41:44.320 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1445.3 MB)
19:41:44.321 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.145.1:7744 (size: 7.5 KB, free: 1445.7 MB)
19:41:44.322 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
19:41:44.340 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at show at DWReleaseExposure.scala:49) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
19:41:44.341 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 4 tasks
19:41:44.397 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5390 bytes)
19:41:44.400 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5390 bytes)
19:41:44.410 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5390 bytes)
19:41:44.412 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5390 bytes)
19:41:44.445 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
19:41:44.445 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
19:41:44.445 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
19:41:44.445 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
19:41:44.595 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-06]
19:41:44.602 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-06]
19:41:44.603 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-06]
19:41:44.618 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-06]
19:41:45.774 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
19:41:45.774 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
19:41:45.776 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
19:41:45.878 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
19:41:45.944 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1458 bytes result sent to driver
19:41:45.951 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1415 bytes result sent to driver
19:41:45.959 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1372 bytes result sent to driver
19:41:45.973 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1591 ms on localhost (executor driver) (1/4)
19:41:45.980 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 1569 ms on localhost (executor driver) (2/4)
19:41:45.981 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 1580 ms on localhost (executor driver) (3/4)
19:41:50.261 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1630 bytes result sent to driver
19:41:50.263 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 5864 ms on localhost (executor driver) (4/4)
19:41:50.265 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
19:41:50.266 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (show at DWReleaseExposure.scala:49) finished in 5.899 s
19:41:50.267 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
19:41:50.268 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
19:41:50.268 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
19:41:50.269 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
19:41:50.273 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[5] at show at DWReleaseExposure.scala:49), which has no missing parents
19:41:50.281 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1445.3 MB)
19:41:50.282 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1445.3 MB)
19:41:50.283 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.145.1:7744 (size: 2.2 KB, free: 1445.7 MB)
19:41:50.284 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
19:41:50.286 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at DWReleaseExposure.scala:49) (first 15 tasks are for partitions Vector(0))
19:41:50.286 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
19:41:50.288 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 4, localhost, executor driver, partition 0, ANY, 4726 bytes)
19:41:50.288 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 4)
19:41:50.305 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
19:41:50.307 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
19:41:50.325 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 4). 1532 bytes result sent to driver
19:41:50.326 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 4) in 40 ms on localhost (executor driver) (1/1)
19:41:50.326 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
19:41:50.327 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DWReleaseExposure.scala:49) finished in 0.040 s
19:41:50.331 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DWReleaseExposure.scala:49, took 6.190642 s
19:41:50.352 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@6b81f75b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19:41:50.354 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.145.1:4040
19:41:50.364 INFO  [dispatcher-event-loop-1] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
19:41:50.380 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
19:41:50.381 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
19:41:50.382 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
19:41:50.384 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
19:41:50.386 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
19:41:50.389 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
19:41:50.390 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\Yongning Y\AppData\Local\Temp\spark-57007f13-ebba-4ede-b320-9a6b0736c663
20:18:24.990 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
20:18:25.524 INFO  [main] org.apache.spark.SparkContext - Submitted application: mid_release_exposure_job
20:18:25.546 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: YongningY,root
20:18:25.546 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: YongningY,root
20:18:25.547 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
20:18:25.547 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
20:18:25.548 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(YongningY, root); groups with view permissions: Set(); users  with modify permissions: Set(YongningY, root); groups with modify permissions: Set()
20:18:26.221 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 7880.
20:18:26.244 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
20:18:26.267 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
20:18:26.271 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20:18:26.272 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
20:18:26.283 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\Yongning Y\AppData\Local\Temp\blockmgr-79972739-56ac-428b-9802-3adf32d59e16
20:18:26.308 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1445.7 MB
20:18:26.370 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
20:18:26.465 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @4073ms
20:18:26.549 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
20:18:26.565 INFO  [main] org.spark_project.jetty.server.Server - Started @4175ms
20:18:26.593 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@3c719ddb{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20:18:26.593 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
20:18:26.618 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@492fc69e{/jobs,null,AVAILABLE,@Spark}
20:18:26.618 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d2260db{/jobs/json,null,AVAILABLE,@Spark}
20:18:26.619 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49bf29c6{/jobs/job,null,AVAILABLE,@Spark}
20:18:26.620 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7668d560{/jobs/job/json,null,AVAILABLE,@Spark}
20:18:26.621 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@126be319{/stages,null,AVAILABLE,@Spark}
20:18:26.621 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c371e13{/stages/json,null,AVAILABLE,@Spark}
20:18:26.622 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e34c607{/stages/stage,null,AVAILABLE,@Spark}
20:18:26.623 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9257031{/stages/stage/json,null,AVAILABLE,@Spark}
20:18:26.624 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7726e185{/stages/pool,null,AVAILABLE,@Spark}
20:18:26.625 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@282308c3{/stages/pool/json,null,AVAILABLE,@Spark}
20:18:26.626 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1db0ec27{/storage,null,AVAILABLE,@Spark}
20:18:26.627 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d4ab71a{/storage/json,null,AVAILABLE,@Spark}
20:18:26.628 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1af05b03{/storage/rdd,null,AVAILABLE,@Spark}
20:18:26.628 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ad777f{/storage/rdd/json,null,AVAILABLE,@Spark}
20:18:26.629 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@438bad7c{/environment,null,AVAILABLE,@Spark}
20:18:26.630 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4fdf8f12{/environment/json,null,AVAILABLE,@Spark}
20:18:26.631 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54f5f647{/executors,null,AVAILABLE,@Spark}
20:18:26.632 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a6d5a8f{/executors/json,null,AVAILABLE,@Spark}
20:18:26.633 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@315ba14a{/executors/threadDump,null,AVAILABLE,@Spark}
20:18:26.634 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27f0ad19{/executors/threadDump/json,null,AVAILABLE,@Spark}
20:18:26.653 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38d5b107{/static,null,AVAILABLE,@Spark}
20:18:26.654 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77e2a6e2{/,null,AVAILABLE,@Spark}
20:18:26.656 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@199e4c2b{/api,null,AVAILABLE,@Spark}
20:18:26.657 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/job/kill,null,AVAILABLE,@Spark}
20:18:26.658 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e7095ac{/stages/stage/kill,null,AVAILABLE,@Spark}
20:18:26.661 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.145.1:4040
20:18:26.792 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
20:18:26.821 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7901.
20:18:26.822 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.145.1:7901
20:18:26.824 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20:18:26.851 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.145.1, 7901, None)
20:18:26.855 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.145.1:7901 with 1445.7 MB RAM, BlockManagerId(driver, 192.168.145.1, 7901, None)
20:18:26.857 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.145.1, 7901, None)
20:18:26.858 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.145.1, 7901, None)
20:18:27.089 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@dffa30b{/metrics/json,null,AVAILABLE,@Spark}
20:18:27.137 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/H:/IdeaProject/release_test/target/classes/hive-site.xml
20:18:27.164 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
20:18:27.165 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/user/hive/warehouse'.
20:18:27.171 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f443fae{/SQL,null,AVAILABLE,@Spark}
20:18:27.172 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79ab34c1{/SQL/json,null,AVAILABLE,@Spark}
20:18:27.173 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@782168b7{/SQL/execution,null,AVAILABLE,@Spark}
20:18:27.174 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7435a578{/SQL/execution/json,null,AVAILABLE,@Spark}
20:18:27.176 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b214b94{/static/sql,null,AVAILABLE,@Spark}
20:18:27.947 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20:18:28.710 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20:18:28.739 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
20:18:29.836 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20:18:31.187 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
20:18:31.190 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
20:18:31.444 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
20:18:31.448 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
20:18:31.497 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
20:18:31.606 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
20:18:31.607 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
20:18:31.621 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=azkaban pat=*
20:18:31.622 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=azkaban pat=*	
20:18:31.680 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dim pat=*
20:18:31.680 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dim pat=*	
20:18:31.684 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dm pat=*
20:18:31.685 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dm pat=*	
20:18:31.689 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dwd pat=*
20:18:31.689 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dwd pat=*	
20:18:31.694 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dws pat=*
20:18:31.694 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dws pat=*	
20:18:31.698 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_ods pat=*
20:18:31.698 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_ods pat=*	
20:18:31.703 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
20:18:31.703 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
20:18:31.706 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_release pat=*
20:18:31.707 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_release pat=*	
20:18:31.710 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_shop pat=*
20:18:31.710 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_shop pat=*	
20:18:31.713 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
20:18:31.713 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
20:18:31.718 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_shop pat=*
20:18:31.718 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_shop pat=*	
20:18:31.722 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
20:18:31.722 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
20:18:31.725 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_shop pat=*
20:18:31.725 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_shop pat=*	
20:18:31.729 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_release pat=*
20:18:31.729 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_release pat=*	
20:18:31.732 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_shop pat=*
20:18:31.732 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_shop pat=*	
20:18:31.737 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
20:18:31.737 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
20:18:31.741 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_shop pat=*
20:18:31.741 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_shop pat=*	
20:18:31.744 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dim pat=*
20:18:31.744 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dim pat=*	
20:18:31.746 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dm pat=*
20:18:31.747 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dm pat=*	
20:18:31.751 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dw pat=*
20:18:31.751 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dw pat=*	
20:18:31.753 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_rds pat=*
20:18:31.754 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_rds pat=*	
20:18:31.757 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=te pat=*
20:18:31.757 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=te pat=*	
20:18:31.759 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test pat=*
20:18:31.760 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test pat=*	
20:18:31.763 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test4 pat=*
20:18:31.764 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test4 pat=*	
20:18:33.467 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/89c3fb28-00fe-4002-bf36-71fc44d9605e_resources
20:18:33.483 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/89c3fb28-00fe-4002-bf36-71fc44d9605e
20:18:33.486 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/89c3fb28-00fe-4002-bf36-71fc44d9605e
20:18:33.491 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/89c3fb28-00fe-4002-bf36-71fc44d9605e/_tmp_space.db
20:18:33.495 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
20:18:33.508 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for root: Unknown error.
20:18:33.523 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:18:33.523 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:18:33.530 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
20:18:33.530 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
20:18:33.534 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
20:18:33.799 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/11cf9919-110f-4b7e-b5aa-f07faaf3efc5_resources
20:18:33.809 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/11cf9919-110f-4b7e-b5aa-f07faaf3efc5
20:18:33.812 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/11cf9919-110f-4b7e-b5aa-f07faaf3efc5
20:18:33.819 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/11cf9919-110f-4b7e-b5aa-f07faaf3efc5/_tmp_space.db
20:18:33.822 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
20:18:33.863 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
20:18:35.265 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
20:18:35.476 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:18:35.476 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:18:35.483 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:18:35.484 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:18:35.594 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:18:35.594 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:18:35.681 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:35.700 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:35.700 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:35.700 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:35.700 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:35.701 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:35.701 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:35.701 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:35.701 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
20:18:35.702 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:35.702 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:35.702 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:35.702 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:35.703 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:35.703 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:35.703 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:35.703 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:35.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:18:35.720 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
20:18:36.053 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
20:18:36.079 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
20:18:36.124 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
20:18:36.126 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:18:36.126 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:18:36.132 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:18:36.133 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:18:36.165 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:18:36.166 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:18:36.198 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:36.199 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:36.199 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:36.200 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:36.200 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:36.200 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:36.200 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:36.200 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:36.201 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:36.201 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:18:36.221 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
20:18:36.222 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as goods_id
20:18:36.240 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: floor(datediff(from_unixtime(unix_timestamp(),'yyyy-MM-dd'),from_unixtime(unix_timestamp(substr(get_json_object(exts,'$.idcard'),7,8),'yyyyMMdd')))/365) as age
20:18:36.251 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: case subStr(get_json_object(exts,'$.idcard'),17,1)%2 when 1 then '男'  when 0 then '女' end as gender 
20:18:36.274 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
20:18:36.275 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
20:18:36.276 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
20:18:36.277 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
20:18:36.277 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
20:18:36.284 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_version
20:18:36.284 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') aid
20:18:36.286 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:18:36.287 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:18:36.297 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:18:36.297 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:18:36.304 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:18:36.304 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:18:36.312 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:18:36.312 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:18:36.322 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:18:36.322 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:18:36.329 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:18:36.329 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:18:36.336 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:18:36.336 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:18:36.342 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:18:36.342 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:18:36.353 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:18:36.354 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:18:36.396 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:18:36.396 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:18:36.401 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:18:36.401 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:18:36.406 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:18:36.406 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:18:36.411 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:18:36.411 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:18:36.416 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:18:36.416 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:18:36.421 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:18:36.421 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:18:36.426 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:18:36.426 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:18:36.431 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:18:36.431 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:18:36.435 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:18:36.436 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:18:36.440 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:18:36.440 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:18:36.445 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:18:36.445 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:18:36.937 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: 
20:18:36.941 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
20:18:36.947 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<ct: bigint>
20:18:36.970 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: 
20:18:36.976 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:18:36.977 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:18:36.987 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:18:36.987 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:18:37.023 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:18:37.024 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:18:37.050 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:37.050 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:37.051 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:37.051 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:37.051 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:37.051 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:37.051 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:37.052 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:37.052 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
20:18:37.052 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:37.052 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:37.053 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:37.053 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:37.053 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:37.053 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:37.053 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:37.054 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:37.054 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:18:37.089 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=dw_release tbl=dw_release_customer
20:18:37.089 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_partitions : db=dw_release tbl=dw_release_customer	
20:18:37.268 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: 
20:18:37.268 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
20:18:37.269 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, exts: string>
20:18:37.269 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: 
20:18:37.269 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:18:37.270 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:18:37.274 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:18:37.274 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:18:37.299 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:18:37.300 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:18:37.332 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:37.332 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:37.333 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:37.333 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:37.333 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:37.333 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:37.334 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:37.334 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:37.334 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:18:37.334 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:18:37.337 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=ods_release tbl=ods_01_release_session
20:18:37.337 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_partitions : db=ods_release tbl=ods_01_release_session	
20:18:37.809 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 267.0145 ms
20:18:38.016 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
20:18:38.079 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 44.5625 ms
20:18:38.174 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 307.7 KB, free 1445.4 MB)
20:18:38.245 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1445.4 MB)
20:18:38.248 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.145.1:7901 (size: 26.3 KB, free: 1445.7 MB)
20:18:38.251 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at MIDReleaseExposure.scala:36
20:18:38.258 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5639939 bytes, open cost is considered as scanning 4194304 bytes.
20:18:38.368 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 19.4338 ms
20:18:38.376 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 308.2 KB, free 1445.1 MB)
20:18:38.403 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 26.4 KB, free 1445.0 MB)
20:18:38.404 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.145.1:7901 (size: 26.4 KB, free: 1445.6 MB)
20:18:38.406 INFO  [main] org.apache.spark.SparkContext - Created broadcast 1 from show at MIDReleaseExposure.scala:36
20:18:38.406 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 11968796 bytes, open cost is considered as scanning 4194304 bytes.
20:18:38.491 INFO  [main] org.apache.spark.SparkContext - Starting job: show at MIDReleaseExposure.scala:36
20:18:38.516 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 8 (show at MIDReleaseExposure.scala:36)
20:18:38.519 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at MIDReleaseExposure.scala:36) with 1 output partitions
20:18:38.519 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at MIDReleaseExposure.scala:36)
20:18:38.519 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
20:18:38.522 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
20:18:38.528 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[8] at show at MIDReleaseExposure.scala:36), which has no missing parents
20:18:38.675 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 25.1 KB, free 1445.0 MB)
20:18:38.678 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.4 KB, free 1445.0 MB)
20:18:38.679 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.145.1:7901 (size: 10.4 KB, free: 1445.6 MB)
20:18:38.680 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
20:18:38.691 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 16 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[8] at show at MIDReleaseExposure.scala:36) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20:18:38.692 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 16 tasks
20:18:38.737 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5802 bytes)
20:18:38.740 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5802 bytes)
20:18:38.746 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5802 bytes)
20:18:38.748 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5802 bytes)
20:18:38.765 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
20:18:38.765 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
20:18:38.776 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
20:18:38.776 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
20:18:39.110 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 130.5773 ms
20:18:39.119 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-06]
20:18:39.119 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-06]
20:18:39.119 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-06]
20:18:39.119 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-06]
20:18:39.803 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 61.42 ms
20:18:39.860 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00003-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, range: 0-1445720, partition values: [2019-09-06]
20:18:39.860 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00003-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, range: 0-1445720, partition values: [2019-09-06]
20:18:39.870 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00003-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, range: 0-1445720, partition values: [2019-09-06]
20:18:39.883 WARN  [Executor task launch worker for task 3] org.apache.parquet.CorruptStatistics - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
20:18:40.486 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 2064 bytes result sent to driver
20:18:40.486 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2021 bytes result sent to driver
20:18:40.486 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 2064 bytes result sent to driver
20:18:40.490 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 5802 bytes)
20:18:40.491 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 5802 bytes)
20:18:40.492 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 5802 bytes)
20:18:40.494 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 4.0 in stage 0.0 (TID 4)
20:18:40.499 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1776 ms on localhost (executor driver) (1/16)
20:18:40.503 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 5.0 in stage 0.0 (TID 5)
20:18:40.503 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 6.0 in stage 0.0 (TID 6)
20:18:40.521 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-06]
20:18:40.536 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-06]
20:18:40.569 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-06]
20:18:40.585 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 1845 ms on localhost (executor driver) (2/16)
20:18:40.607 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00000-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, range: 0-1445655, partition values: [2019-09-06]
20:18:40.640 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00000-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, range: 0-1445655, partition values: [2019-09-06]
20:18:40.707 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 1960 ms on localhost (executor driver) (3/16)
20:18:40.781 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 4.0 in stage 0.0 (TID 4). 1935 bytes result sent to driver
20:18:40.784 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 5802 bytes)
20:18:40.805 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 6.0 in stage 0.0 (TID 6). 1978 bytes result sent to driver
20:18:40.806 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, ANY, 5802 bytes)
20:18:40.807 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 0.0 (TID 6) in 315 ms on localhost (executor driver) (4/16)
20:18:40.807 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 8.0 in stage 0.0 (TID 8)
20:18:40.829 INFO  [Executor task launch worker for task 8] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-06]
20:18:40.833 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 0.0 (TID 4) in 344 ms on localhost (executor driver) (5/16)
20:18:40.849 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 7.0 in stage 0.0 (TID 7)
20:18:40.870 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-06]
20:18:40.878 INFO  [Executor task launch worker for task 8] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00002-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, range: 0-1445586, partition values: [2019-09-06]
20:18:40.957 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 8.0 in stage 0.0 (TID 8). 1935 bytes result sent to driver
20:18:40.958 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, ANY, 5802 bytes)
20:18:40.958 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 9.0 in stage 0.0 (TID 9)
20:18:40.963 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 0.0 (TID 8) in 158 ms on localhost (executor driver) (6/16)
20:18:40.971 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-06]
20:18:41.067 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00000-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, range: 0-1445655, partition values: [2019-09-06]
20:18:41.112 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 7.0 in stage 0.0 (TID 7). 1978 bytes result sent to driver
20:18:41.117 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 10.0 in stage 0.0 (TID 10, localhost, executor driver, partition 10, ANY, 5802 bytes)
20:18:41.118 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 10.0 in stage 0.0 (TID 10)
20:18:41.127 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-06]
20:18:41.129 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 0.0 (TID 7) in 346 ms on localhost (executor driver) (7/16)
20:18:41.149 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00002-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, range: 0-1445586, partition values: [2019-09-06]
20:18:41.191 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 10.0 in stage 0.0 (TID 10). 1935 bytes result sent to driver
20:18:41.191 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 11.0 in stage 0.0 (TID 11, localhost, executor driver, partition 11, ANY, 5802 bytes)
20:18:41.194 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 11.0 in stage 0.0 (TID 11)
20:18:41.203 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-06]
20:18:41.209 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 10.0 in stage 0.0 (TID 10) in 93 ms on localhost (executor driver) (8/16)
20:18:41.243 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00002-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, range: 0-1445586, partition values: [2019-09-06]
20:18:41.291 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 11.0 in stage 0.0 (TID 11). 1978 bytes result sent to driver
20:18:41.293 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 12.0 in stage 0.0 (TID 12, localhost, executor driver, partition 12, ANY, 5802 bytes)
20:18:41.293 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 12.0 in stage 0.0 (TID 12)
20:18:41.294 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 11.0 in stage 0.0 (TID 11) in 103 ms on localhost (executor driver) (9/16)
20:18:41.304 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-06]
20:18:41.319 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00001-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, range: 0-1445580, partition values: [2019-09-06]
20:18:41.355 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 12.0 in stage 0.0 (TID 12). 1935 bytes result sent to driver
20:18:41.357 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 13.0 in stage 0.0 (TID 13, localhost, executor driver, partition 13, ANY, 5802 bytes)
20:18:41.357 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Running task 13.0 in stage 0.0 (TID 13)
20:18:41.362 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 12.0 in stage 0.0 (TID 12) in 66 ms on localhost (executor driver) (10/16)
20:18:41.375 INFO  [Executor task launch worker for task 13] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-06]
20:18:44.377 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray - Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
20:18:45.038 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray - Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
20:18:45.332 INFO  [Executor task launch worker for task 13] org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray - Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
20:18:45.888 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray - Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
20:18:55.166 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00003-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, range: 0-1445720, partition values: [2019-09-06]
20:18:55.539 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00000-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, range: 0-1445655, partition values: [2019-09-06]
20:18:55.660 INFO  [Executor task launch worker for task 13] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00001-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, range: 0-1445580, partition values: [2019-09-06]
20:18:55.952 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00002-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, range: 0-1445586, partition values: [2019-09-06]
20:19:03.630 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (0  time so far)
20:19:03.628 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (0  time so far)
20:19:03.844 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (0  time so far)
20:19:04.527 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (0  time so far)
20:19:13.963 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (1  time so far)
20:19:13.999 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (1  time so far)
20:19:14.152 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (1  time so far)
20:19:14.211 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (1  time so far)
20:19:15.728 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (2  times so far)
20:19:15.773 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (2  times so far)
20:19:15.830 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (2  times so far)
20:19:15.868 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (2  times so far)
20:19:18.068 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (3  times so far)
20:19:18.091 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (3  times so far)
20:19:18.164 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (3  times so far)
20:19:18.268 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (3  times so far)
20:19:19.575 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (4  times so far)
20:19:19.632 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (4  times so far)
20:19:19.820 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (4  times so far)
20:19:19.865 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (4  times so far)
20:19:21.313 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (5  times so far)
20:19:21.370 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (5  times so far)
20:19:21.616 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (5  times so far)
20:19:21.625 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (5  times so far)
20:19:23.031 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (6  times so far)
20:19:23.095 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (6  times so far)
20:19:23.328 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (6  times so far)
20:19:23.356 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (6  times so far)
20:19:24.711 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (7  times so far)
20:19:27.421 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (7  times so far)
20:19:27.569 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (7  times so far)
20:19:27.647 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (7  times so far)
20:19:29.412 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (8  times so far)
20:19:29.990 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (8  times so far)
20:19:30.122 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (8  times so far)
20:19:30.264 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (8  times so far)
20:19:32.005 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (9  times so far)
20:19:37.705 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (9  times so far)
20:19:38.220 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (9  times so far)
20:19:38.363 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (9  times so far)
20:19:39.034 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (10  times so far)
20:19:44.822 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (10  times so far)
20:19:44.901 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (10  times so far)
20:19:45.185 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (10  times so far)
20:19:45.873 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (11  times so far)
20:19:51.235 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (11  times so far)
20:19:51.263 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (11  times so far)
20:19:51.504 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (11  times so far)
20:19:51.716 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (12  times so far)
20:19:57.053 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (12  times so far)
20:19:57.429 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (12  times so far)
20:19:57.598 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (12  times so far)
20:19:57.645 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (13  times so far)
20:20:00.990 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (13  times so far)
20:20:02.221 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (13  times so far)
20:20:03.315 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (14  times so far)
20:20:03.359 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (13  times so far)
20:20:05.994 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (14  times so far)
20:20:07.031 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (14  times so far)
20:20:08.012 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (15  times so far)
20:20:08.027 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (14  times so far)
20:20:11.325 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (15  times so far)
20:20:11.774 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (15  times so far)
20:20:13.062 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (16  times so far)
20:20:13.682 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (15  times so far)
20:20:16.536 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (16  times so far)
20:20:16.675 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (16  times so far)
20:20:17.037 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (17  times so far)
20:20:21.839 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (16  times so far)
20:20:25.461 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (17  times so far)
20:20:25.631 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (18  times so far)
20:20:25.650 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (17  times so far)
20:20:25.991 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (17  times so far)
20:20:29.275 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (18  times so far)
20:20:30.596 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (19  times so far)
20:20:30.690 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (18  times so far)
20:20:31.288 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (18  times so far)
20:20:35.076 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (19  times so far)
20:20:35.819 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (20  times so far)
20:20:36.121 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (19  times so far)
20:20:36.761 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (19  times so far)
20:20:39.904 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (20  times so far)
20:20:40.733 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (20  times so far)
20:20:41.230 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (21  times so far)
20:20:41.360 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (20  times so far)
20:20:43.656 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (21  times so far)
20:20:46.360 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (22  times so far)
20:20:46.378 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (21  times so far)
20:20:46.378 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (21  times so far)
20:20:47.337 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (22  times so far)
20:20:50.702 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (23  times so far)
20:20:51.163 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (22  times so far)
20:20:51.653 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (22  times so far)
20:20:51.701 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (23  times so far)
20:20:54.990 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (24  times so far)
20:20:55.722 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (23  times so far)
20:20:56.036 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (23  times so far)
20:20:56.226 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (24  times so far)
20:21:00.097 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (25  times so far)
20:21:00.217 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (24  times so far)
20:21:00.431 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (25  times so far)
20:21:00.558 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (24  times so far)
20:21:07.118 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (26  times so far)
20:21:07.916 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (25  times so far)
20:21:08.267 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (26  times so far)
20:21:08.457 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (25  times so far)
20:21:11.154 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (27  times so far)
20:21:13.166 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (26  times so far)
20:21:13.649 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (27  times so far)
20:21:13.841 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (26  times so far)
20:21:15.896 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (28  times so far)
20:21:18.423 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (27  times so far)
20:21:19.012 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (27  times so far)
20:21:19.077 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (28  times so far)
20:21:20.149 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (29  times so far)
20:21:22.955 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (28  times so far)
20:21:24.084 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (28  times so far)
20:21:24.852 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (30  times so far)
20:21:24.983 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (29  times so far)
20:21:28.195 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (29  times so far)
20:21:28.429 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (29  times so far)
20:21:29.433 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (31  times so far)
20:21:29.725 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (30  times so far)
20:21:32.968 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (30  times so far)
20:21:33.013 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (30  times so far)
20:21:33.674 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (32  times so far)
20:21:34.281 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (31  times so far)
20:21:37.408 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (31  times so far)
20:21:37.978 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (31  times so far)
20:21:38.195 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (33  times so far)
20:21:38.785 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (32  times so far)
20:21:41.784 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (32  times so far)
20:21:43.061 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (32  times so far)
20:21:43.075 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (34  times so far)
20:21:43.180 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (33  times so far)
20:21:51.054 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (33  times so far)
20:21:52.038 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (35  times so far)
20:21:52.232 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (33  times so far)
20:21:52.458 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (34  times so far)
20:21:54.502 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (34  times so far)
20:21:57.115 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (36  times so far)
20:21:57.455 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (34  times so far)
20:21:57.513 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 77 spilling sort data of 288.0 MB to disk (35  times so far)
20:21:58.734 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 71 spilling sort data of 288.0 MB to disk (35  times so far)
20:25:12.956 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
20:25:13.345 INFO  [main] org.apache.spark.SparkContext - Submitted application: mid_release_exposure_job
20:25:13.372 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: YongningY,root
20:25:13.373 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: YongningY,root
20:25:13.374 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
20:25:13.375 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
20:25:13.376 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(YongningY, root); groups with view permissions: Set(); users  with modify permissions: Set(YongningY, root); groups with modify permissions: Set()
20:25:14.166 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 7976.
20:25:14.190 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
20:25:14.209 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
20:25:14.212 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20:25:14.212 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
20:25:14.221 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\Yongning Y\AppData\Local\Temp\blockmgr-1ea0ae06-d9af-4b57-a402-305341e1ada8
20:25:14.244 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1445.7 MB
20:25:14.327 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
20:25:14.484 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @3780ms
20:25:14.603 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
20:25:14.627 INFO  [main] org.spark_project.jetty.server.Server - Started @3926ms
20:25:14.666 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@3fc08eec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20:25:14.667 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
20:25:14.713 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@492fc69e{/jobs,null,AVAILABLE,@Spark}
20:25:14.714 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d2260db{/jobs/json,null,AVAILABLE,@Spark}
20:25:14.715 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49bf29c6{/jobs/job,null,AVAILABLE,@Spark}
20:25:14.729 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7668d560{/jobs/job/json,null,AVAILABLE,@Spark}
20:25:14.730 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@126be319{/stages,null,AVAILABLE,@Spark}
20:25:14.732 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c371e13{/stages/json,null,AVAILABLE,@Spark}
20:25:14.734 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e34c607{/stages/stage,null,AVAILABLE,@Spark}
20:25:14.739 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9257031{/stages/stage/json,null,AVAILABLE,@Spark}
20:25:14.741 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7726e185{/stages/pool,null,AVAILABLE,@Spark}
20:25:14.742 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@282308c3{/stages/pool/json,null,AVAILABLE,@Spark}
20:25:14.745 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1db0ec27{/storage,null,AVAILABLE,@Spark}
20:25:14.746 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d4ab71a{/storage/json,null,AVAILABLE,@Spark}
20:25:14.747 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1af05b03{/storage/rdd,null,AVAILABLE,@Spark}
20:25:14.748 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ad777f{/storage/rdd/json,null,AVAILABLE,@Spark}
20:25:14.749 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@438bad7c{/environment,null,AVAILABLE,@Spark}
20:25:14.750 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4fdf8f12{/environment/json,null,AVAILABLE,@Spark}
20:25:14.751 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54f5f647{/executors,null,AVAILABLE,@Spark}
20:25:14.755 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a6d5a8f{/executors/json,null,AVAILABLE,@Spark}
20:25:14.757 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@315ba14a{/executors/threadDump,null,AVAILABLE,@Spark}
20:25:14.759 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27f0ad19{/executors/threadDump/json,null,AVAILABLE,@Spark}
20:25:14.782 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38d5b107{/static,null,AVAILABLE,@Spark}
20:25:14.784 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77e2a6e2{/,null,AVAILABLE,@Spark}
20:25:14.789 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@199e4c2b{/api,null,AVAILABLE,@Spark}
20:25:14.792 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/job/kill,null,AVAILABLE,@Spark}
20:25:14.793 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e7095ac{/stages/stage/kill,null,AVAILABLE,@Spark}
20:25:14.797 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.145.1:4040
20:25:14.937 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
20:25:15.059 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7997.
20:25:15.060 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.145.1:7997
20:25:15.063 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20:25:15.111 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.145.1, 7997, None)
20:25:15.115 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.145.1:7997 with 1445.7 MB RAM, BlockManagerId(driver, 192.168.145.1, 7997, None)
20:25:15.119 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.145.1, 7997, None)
20:25:15.120 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.145.1, 7997, None)
20:25:15.462 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@dffa30b{/metrics/json,null,AVAILABLE,@Spark}
20:25:15.514 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/H:/IdeaProject/release_test/target/classes/hive-site.xml
20:25:15.552 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
20:25:15.553 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/user/hive/warehouse'.
20:25:15.562 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f443fae{/SQL,null,AVAILABLE,@Spark}
20:25:15.563 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79ab34c1{/SQL/json,null,AVAILABLE,@Spark}
20:25:15.564 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@782168b7{/SQL/execution,null,AVAILABLE,@Spark}
20:25:15.565 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7435a578{/SQL/execution/json,null,AVAILABLE,@Spark}
20:25:15.567 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b214b94{/static/sql,null,AVAILABLE,@Spark}
20:25:17.573 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20:25:18.717 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20:25:18.764 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
20:25:19.869 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20:25:21.337 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
20:25:21.340 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
20:25:21.759 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
20:25:21.762 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
20:25:21.832 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
20:25:21.982 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
20:25:21.983 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
20:25:22.002 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=azkaban pat=*
20:25:22.002 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=azkaban pat=*	
20:25:22.095 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dim pat=*
20:25:22.095 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dim pat=*	
20:25:22.114 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dm pat=*
20:25:22.114 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dm pat=*	
20:25:22.117 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dwd pat=*
20:25:22.118 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dwd pat=*	
20:25:22.122 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dws pat=*
20:25:22.122 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dws pat=*	
20:25:22.125 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_ods pat=*
20:25:22.126 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_ods pat=*	
20:25:22.129 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
20:25:22.129 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
20:25:22.133 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_release pat=*
20:25:22.133 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_release pat=*	
20:25:22.137 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_shop pat=*
20:25:22.137 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_shop pat=*	
20:25:22.141 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
20:25:22.141 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
20:25:22.145 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_shop pat=*
20:25:22.145 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_shop pat=*	
20:25:22.149 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
20:25:22.149 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
20:25:22.152 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_shop pat=*
20:25:22.153 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_shop pat=*	
20:25:22.156 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_release pat=*
20:25:22.156 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_release pat=*	
20:25:22.159 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_shop pat=*
20:25:22.159 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_shop pat=*	
20:25:22.164 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
20:25:22.164 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
20:25:22.168 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_shop pat=*
20:25:22.168 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_shop pat=*	
20:25:22.171 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dim pat=*
20:25:22.171 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dim pat=*	
20:25:22.174 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dm pat=*
20:25:22.174 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dm pat=*	
20:25:22.178 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dw pat=*
20:25:22.178 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dw pat=*	
20:25:22.181 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_rds pat=*
20:25:22.181 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_rds pat=*	
20:25:22.184 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=te pat=*
20:25:22.184 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=te pat=*	
20:25:22.187 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test pat=*
20:25:22.187 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test pat=*	
20:25:22.192 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test4 pat=*
20:25:22.193 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test4 pat=*	
20:25:25.799 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/42803dce-23c8-4400-9b19-69912e2d18ac_resources
20:25:25.838 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/42803dce-23c8-4400-9b19-69912e2d18ac
20:25:25.841 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/42803dce-23c8-4400-9b19-69912e2d18ac
20:25:25.846 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/42803dce-23c8-4400-9b19-69912e2d18ac/_tmp_space.db
20:25:25.849 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
20:25:25.873 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for root: Unknown error.
20:25:25.924 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:25:25.924 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:25:25.937 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
20:25:25.937 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
20:25:25.965 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
20:25:26.301 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/43afbb07-6dbc-4b32-a366-748c344b744b_resources
20:25:26.306 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/43afbb07-6dbc-4b32-a366-748c344b744b
20:25:26.309 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/43afbb07-6dbc-4b32-a366-748c344b744b
20:25:26.315 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/43afbb07-6dbc-4b32-a366-748c344b744b/_tmp_space.db
20:25:26.317 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
20:25:26.461 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
20:25:28.284 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
20:25:28.775 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:25:28.775 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:25:28.786 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:25:28.786 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:25:29.027 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:25:29.028 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:25:29.214 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:29.240 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:29.240 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:29.241 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:29.241 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:29.242 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:29.242 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:29.243 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:29.244 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
20:25:29.245 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:29.245 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:29.245 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:29.246 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:29.246 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:29.247 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:29.247 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:29.247 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:29.248 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:25:29.294 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
20:25:30.575 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
20:25:30.639 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
20:25:30.754 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
20:25:30.755 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:25:30.755 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:25:30.762 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:25:30.762 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:25:30.812 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:25:30.812 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:25:30.874 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:30.875 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:30.875 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:30.875 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:30.876 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:30.876 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:30.876 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:30.876 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:30.876 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:30.877 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:25:30.902 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
20:25:30.903 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as goods_id
20:25:30.949 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: floor(datediff(from_unixtime(unix_timestamp(),'yyyy-MM-dd'),from_unixtime(unix_timestamp(substr(get_json_object(exts,'$.idcard'),7,8),'yyyyMMdd')))/365) as age
20:25:30.960 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: case subStr(get_json_object(exts,'$.idcard'),17,1)%2 when 1 then '男'  when 0 then '女' end as gender 
20:25:30.985 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
20:25:30.986 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
20:25:30.987 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
20:25:30.987 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
20:25:30.988 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
20:25:30.989 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_version
20:25:30.989 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') aid
20:25:30.992 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:25:30.992 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:25:30.998 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:25:30.998 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:25:31.002 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:25:31.002 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:25:31.006 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:25:31.006 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:25:31.011 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:25:31.011 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:25:31.016 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:25:31.016 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:25:31.020 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:25:31.020 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:25:31.024 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:25:31.025 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:25:31.028 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:25:31.029 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:25:31.033 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:25:31.033 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:25:31.037 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:25:31.038 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:25:31.043 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:25:31.044 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:25:31.049 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:25:31.049 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:25:31.052 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:25:31.053 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:25:31.057 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:25:31.057 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:25:31.062 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:25:31.062 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:25:31.066 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:25:31.066 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:25:31.072 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:25:31.072 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:25:31.077 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:25:31.077 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:25:31.080 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:25:31.081 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:25:32.309 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: 
20:25:32.314 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
20:25:32.318 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<ct: bigint>
20:25:32.361 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: 
20:25:32.380 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:25:32.380 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:25:32.386 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:25:32.386 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:25:32.421 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:25:32.422 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:25:32.454 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:32.454 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:32.454 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:32.454 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:32.455 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:32.455 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:32.455 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:32.455 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:32.456 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
20:25:32.456 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:32.456 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:32.456 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:32.457 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:32.457 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:32.457 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:32.457 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:32.457 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:32.458 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:25:32.508 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=dw_release tbl=dw_release_customer
20:25:32.508 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_partitions : db=dw_release tbl=dw_release_customer	
20:25:32.982 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: 
20:25:32.982 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
20:25:32.983 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, exts: string>
20:25:32.984 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: 
20:25:32.984 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:25:32.985 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:25:32.991 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:25:32.992 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:25:33.030 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:25:33.030 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:25:33.067 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:33.067 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:33.068 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:33.068 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:33.068 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:33.069 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:33.069 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:33.069 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:33.070 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:25:33.070 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:25:33.073 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=ods_release tbl=ods_01_release_session
20:25:33.073 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_partitions : db=ods_release tbl=ods_01_release_session	
20:25:33.853 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 321.9126 ms
20:25:34.219 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 61.7314 ms
20:25:34.647 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 307.7 KB, free 1445.4 MB)
20:25:34.792 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1445.4 MB)
20:25:34.803 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.145.1:7997 (size: 26.3 KB, free: 1445.7 MB)
20:25:34.826 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at MIDReleaseExposure.scala:36
20:25:34.887 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5639939 bytes, open cost is considered as scanning 4194304 bytes.
20:25:35.185 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 20.6702 ms
20:25:35.192 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 308.2 KB, free 1445.1 MB)
20:25:35.210 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 26.4 KB, free 1445.0 MB)
20:25:35.212 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.145.1:7997 (size: 26.4 KB, free: 1445.6 MB)
20:25:35.213 INFO  [main] org.apache.spark.SparkContext - Created broadcast 1 from show at MIDReleaseExposure.scala:36
20:25:35.213 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 11968796 bytes, open cost is considered as scanning 4194304 bytes.
20:25:35.346 INFO  [main] org.apache.spark.SparkContext - Starting job: show at MIDReleaseExposure.scala:36
20:25:35.423 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 8 (show at MIDReleaseExposure.scala:36)
20:25:35.443 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at MIDReleaseExposure.scala:36) with 1 output partitions
20:25:35.444 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at MIDReleaseExposure.scala:36)
20:25:35.444 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
20:25:35.459 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
20:25:35.520 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[8] at show at MIDReleaseExposure.scala:36), which has no missing parents
20:25:35.890 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 25.1 KB, free 1445.0 MB)
20:25:35.893 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.4 KB, free 1445.0 MB)
20:25:35.895 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.145.1:7997 (size: 10.4 KB, free: 1445.6 MB)
20:25:35.895 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
20:25:35.917 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 16 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[8] at show at MIDReleaseExposure.scala:36) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20:25:35.918 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 16 tasks
20:25:36.033 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5802 bytes)
20:25:36.039 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5802 bytes)
20:25:36.040 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5802 bytes)
20:25:36.044 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5802 bytes)
20:25:36.080 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
20:25:36.080 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
20:25:36.080 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
20:25:36.080 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
20:25:36.424 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 62.6229 ms
20:25:36.432 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-06]
20:25:36.432 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-06]
20:25:36.432 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-06]
20:25:36.432 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-06]
20:25:37.346 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 20.895 ms
20:25:37.477 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00003-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, range: 0-1445720, partition values: [2019-09-06]
20:25:37.477 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00003-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, range: 0-1445720, partition values: [2019-09-06]
20:25:37.477 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00003-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, range: 0-1445720, partition values: [2019-09-06]
20:25:37.490 WARN  [Executor task launch worker for task 0] org.apache.parquet.CorruptStatistics - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
20:25:37.585 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
20:25:38.249 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 2064 bytes result sent to driver
20:25:38.249 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2064 bytes result sent to driver
20:25:38.249 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 2021 bytes result sent to driver
20:25:38.337 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 5802 bytes)
20:25:38.342 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 5802 bytes)
20:25:38.343 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 4.0 in stage 0.0 (TID 4)
20:25:38.346 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 5802 bytes)
20:25:38.376 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 5.0 in stage 0.0 (TID 5)
20:25:38.380 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 6.0 in stage 0.0 (TID 6)
20:25:38.404 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-06]
20:25:38.404 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 2332 ms on localhost (executor driver) (1/16)
20:25:38.418 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-06]
20:25:38.419 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-06]
20:25:38.440 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 2443 ms on localhost (executor driver) (2/16)
20:25:38.441 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 2401 ms on localhost (executor driver) (3/16)
20:25:38.479 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00000-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, range: 0-1445655, partition values: [2019-09-06]
20:25:38.495 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00000-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, range: 0-1445655, partition values: [2019-09-06]
20:25:38.632 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 4.0 in stage 0.0 (TID 4). 1978 bytes result sent to driver
20:25:38.635 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 5802 bytes)
20:25:38.635 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 7.0 in stage 0.0 (TID 7)
20:25:38.659 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-06]
20:25:38.661 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 0.0 (TID 4) in 327 ms on localhost (executor driver) (4/16)
20:25:38.691 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00000-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, range: 0-1445655, partition values: [2019-09-06]
20:25:38.738 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 7.0 in stage 0.0 (TID 7). 1935 bytes result sent to driver
20:25:38.743 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, ANY, 5802 bytes)
20:25:38.743 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 8.0 in stage 0.0 (TID 8)
20:25:38.746 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 0.0 (TID 7) in 111 ms on localhost (executor driver) (5/16)
20:25:38.760 INFO  [Executor task launch worker for task 8] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-06]
20:25:38.791 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 6.0 in stage 0.0 (TID 6). 1935 bytes result sent to driver
20:25:38.793 INFO  [Executor task launch worker for task 8] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00002-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, range: 0-1445586, partition values: [2019-09-06]
20:25:38.795 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 0.0 (TID 6) in 451 ms on localhost (executor driver) (6/16)
20:25:38.796 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, ANY, 5802 bytes)
20:25:38.814 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 9.0 in stage 0.0 (TID 9)
20:25:38.852 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-06]
20:25:38.859 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 8.0 in stage 0.0 (TID 8). 1935 bytes result sent to driver
20:25:38.860 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 10.0 in stage 0.0 (TID 10, localhost, executor driver, partition 10, ANY, 5802 bytes)
20:25:38.860 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 10.0 in stage 0.0 (TID 10)
20:25:38.860 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 0.0 (TID 8) in 121 ms on localhost (executor driver) (7/16)
20:25:38.876 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-06]
20:25:38.998 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00002-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, range: 0-1445586, partition values: [2019-09-06]
20:25:39.035 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 10.0 in stage 0.0 (TID 10). 1935 bytes result sent to driver
20:25:39.036 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 11.0 in stage 0.0 (TID 11, localhost, executor driver, partition 11, ANY, 5802 bytes)
20:25:39.037 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 11.0 in stage 0.0 (TID 11)
20:25:39.037 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 10.0 in stage 0.0 (TID 10) in 178 ms on localhost (executor driver) (8/16)
20:25:39.048 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-06]
20:25:39.085 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00002-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, range: 0-1445586, partition values: [2019-09-06]
20:25:39.138 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 11.0 in stage 0.0 (TID 11). 1978 bytes result sent to driver
20:25:39.139 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 12.0 in stage 0.0 (TID 12, localhost, executor driver, partition 12, ANY, 5802 bytes)
20:25:39.139 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 12.0 in stage 0.0 (TID 12)
20:25:39.148 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-06]
20:25:39.149 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 11.0 in stage 0.0 (TID 11) in 113 ms on localhost (executor driver) (9/16)
20:25:39.201 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00001-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, range: 0-1445580, partition values: [2019-09-06]
20:25:39.358 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 12.0 in stage 0.0 (TID 12). 1935 bytes result sent to driver
20:25:39.360 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 13.0 in stage 0.0 (TID 13, localhost, executor driver, partition 13, ANY, 5802 bytes)
20:25:39.361 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Running task 13.0 in stage 0.0 (TID 13)
20:25:39.361 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 12.0 in stage 0.0 (TID 12) in 223 ms on localhost (executor driver) (10/16)
20:25:39.369 INFO  [Executor task launch worker for task 13] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-06]
20:25:43.010 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray - Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
20:25:43.022 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray - Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
20:25:48.929 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00000-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, range: 0-1445655, partition values: [2019-09-06]
20:25:48.935 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_customer/bdp_day=2019-09-06/part-00002-bd57af87-8f74-41ac-97a9-3b1619aabc68.c000, range: 0-1445586, partition values: [2019-09-06]
20:25:50.586 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 640.0 MB to disk (0  time so far)
20:25:50.593 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 640.0 MB to disk (0  time so far)
20:25:55.139 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 640.0 MB to disk (1  time so far)
20:25:55.142 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 640.0 MB to disk (1  time so far)
20:26:02.787 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 640.0 MB to disk (2  times so far)
20:26:03.004 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 640.0 MB to disk (2  times so far)
20:26:09.601 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 640.0 MB to disk (3  times so far)
20:26:09.720 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 640.0 MB to disk (3  times so far)
20:26:15.816 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 640.0 MB to disk (4  times so far)
20:26:15.966 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 640.0 MB to disk (4  times so far)
20:26:22.800 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 640.0 MB to disk (5  times so far)
20:27:06.201 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
20:27:07.811 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_exposure_job
20:27:08.141 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: YongningY,root
20:27:08.148 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: YongningY,root
20:27:08.158 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
20:27:08.159 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
20:27:08.199 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(YongningY, root); groups with view permissions: Set(); users  with modify permissions: Set(YongningY, root); groups with modify permissions: Set()
20:27:10.077 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 8061.
20:27:10.173 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
20:27:10.261 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
20:27:10.305 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20:27:10.306 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
20:27:10.328 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\Yongning Y\AppData\Local\Temp\blockmgr-a40d0278-c2f3-42ec-92e5-451bea868e29
20:27:10.443 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1445.7 MB
20:27:10.594 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
20:27:10.870 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @10786ms
20:27:11.085 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
20:27:11.114 INFO  [main] org.spark_project.jetty.server.Server - Started @11031ms
20:27:11.154 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@3fc08eec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20:27:11.155 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
20:27:11.194 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@492fc69e{/jobs,null,AVAILABLE,@Spark}
20:27:11.195 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d2260db{/jobs/json,null,AVAILABLE,@Spark}
20:27:11.196 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49bf29c6{/jobs/job,null,AVAILABLE,@Spark}
20:27:11.197 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7668d560{/jobs/job/json,null,AVAILABLE,@Spark}
20:27:11.200 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@126be319{/stages,null,AVAILABLE,@Spark}
20:27:11.201 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c371e13{/stages/json,null,AVAILABLE,@Spark}
20:27:11.202 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e34c607{/stages/stage,null,AVAILABLE,@Spark}
20:27:11.203 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9257031{/stages/stage/json,null,AVAILABLE,@Spark}
20:27:11.204 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7726e185{/stages/pool,null,AVAILABLE,@Spark}
20:27:11.205 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@282308c3{/stages/pool/json,null,AVAILABLE,@Spark}
20:27:11.206 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1db0ec27{/storage,null,AVAILABLE,@Spark}
20:27:11.209 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d4ab71a{/storage/json,null,AVAILABLE,@Spark}
20:27:11.209 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1af05b03{/storage/rdd,null,AVAILABLE,@Spark}
20:27:11.210 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ad777f{/storage/rdd/json,null,AVAILABLE,@Spark}
20:27:11.211 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@438bad7c{/environment,null,AVAILABLE,@Spark}
20:27:11.212 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4fdf8f12{/environment/json,null,AVAILABLE,@Spark}
20:27:11.213 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54f5f647{/executors,null,AVAILABLE,@Spark}
20:27:11.217 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a6d5a8f{/executors/json,null,AVAILABLE,@Spark}
20:27:11.217 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@315ba14a{/executors/threadDump,null,AVAILABLE,@Spark}
20:27:11.229 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27f0ad19{/executors/threadDump/json,null,AVAILABLE,@Spark}
20:27:11.240 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38d5b107{/static,null,AVAILABLE,@Spark}
20:27:11.241 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77e2a6e2{/,null,AVAILABLE,@Spark}
20:27:11.242 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@199e4c2b{/api,null,AVAILABLE,@Spark}
20:27:11.243 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/job/kill,null,AVAILABLE,@Spark}
20:27:11.244 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e7095ac{/stages/stage/kill,null,AVAILABLE,@Spark}
20:27:11.249 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.145.1:4040
20:27:11.540 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
20:27:11.611 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 8082.
20:27:11.612 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.145.1:8082
20:27:11.628 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20:27:11.677 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.145.1, 8082, None)
20:27:11.689 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.145.1:8082 with 1445.7 MB RAM, BlockManagerId(driver, 192.168.145.1, 8082, None)
20:27:11.697 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.145.1, 8082, None)
20:27:11.698 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.145.1, 8082, None)
20:27:11.951 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@dffa30b{/metrics/json,null,AVAILABLE,@Spark}
20:27:12.192 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/H:/IdeaProject/release_test/target/classes/hive-site.xml
20:27:12.257 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
20:27:12.258 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/user/hive/warehouse'.
20:27:12.295 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f443fae{/SQL,null,AVAILABLE,@Spark}
20:27:12.295 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79ab34c1{/SQL/json,null,AVAILABLE,@Spark}
20:27:12.296 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@782168b7{/SQL/execution,null,AVAILABLE,@Spark}
20:27:12.297 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7435a578{/SQL/execution/json,null,AVAILABLE,@Spark}
20:27:12.302 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b214b94{/static/sql,null,AVAILABLE,@Spark}
20:27:14.451 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20:27:15.449 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20:27:15.482 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
20:27:16.597 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20:27:17.904 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
20:27:17.906 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
20:27:18.284 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
20:27:18.289 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
20:27:18.370 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
20:27:18.539 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
20:27:18.541 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
20:27:18.559 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=azkaban pat=*
20:27:18.559 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=azkaban pat=*	
20:27:18.616 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dim pat=*
20:27:18.616 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dim pat=*	
20:27:18.625 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dm pat=*
20:27:18.625 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dm pat=*	
20:27:18.629 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dwd pat=*
20:27:18.629 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dwd pat=*	
20:27:18.633 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dws pat=*
20:27:18.633 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dws pat=*	
20:27:18.636 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_ods pat=*
20:27:18.637 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_ods pat=*	
20:27:18.640 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
20:27:18.640 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
20:27:18.643 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_release pat=*
20:27:18.644 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_release pat=*	
20:27:18.647 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_shop pat=*
20:27:18.647 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_shop pat=*	
20:27:18.650 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
20:27:18.650 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
20:27:18.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_shop pat=*
20:27:18.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_shop pat=*	
20:27:18.657 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
20:27:18.658 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
20:27:18.661 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_shop pat=*
20:27:18.661 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_shop pat=*	
20:27:18.664 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_release pat=*
20:27:18.665 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_release pat=*	
20:27:18.667 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_shop pat=*
20:27:18.668 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_shop pat=*	
20:27:18.673 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
20:27:18.673 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
20:27:18.676 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_shop pat=*
20:27:18.677 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_shop pat=*	
20:27:18.680 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dim pat=*
20:27:18.680 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dim pat=*	
20:27:18.683 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dm pat=*
20:27:18.683 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dm pat=*	
20:27:18.687 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dw pat=*
20:27:18.687 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dw pat=*	
20:27:18.691 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_rds pat=*
20:27:18.691 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_rds pat=*	
20:27:18.694 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=te pat=*
20:27:18.694 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=te pat=*	
20:27:18.696 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test pat=*
20:27:18.697 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test pat=*	
20:27:18.700 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test4 pat=*
20:27:18.700 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test4 pat=*	
20:27:21.725 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/308ccb8e-1ef0-4dfb-a05b-d08ccbca7773_resources
20:27:21.769 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/308ccb8e-1ef0-4dfb-a05b-d08ccbca7773
20:27:21.774 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/308ccb8e-1ef0-4dfb-a05b-d08ccbca7773
20:27:21.780 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/308ccb8e-1ef0-4dfb-a05b-d08ccbca7773/_tmp_space.db
20:27:21.784 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
20:27:21.824 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for root: Unknown error.
20:27:21.873 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:27:21.873 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:27:21.888 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
20:27:21.889 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
20:27:21.893 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
20:27:22.234 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/473123c6-7e15-40d8-88d1-3bfeee3cbdf3_resources
20:27:22.239 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/473123c6-7e15-40d8-88d1-3bfeee3cbdf3
20:27:22.242 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/473123c6-7e15-40d8-88d1-3bfeee3cbdf3
20:27:22.248 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/473123c6-7e15-40d8-88d1-3bfeee3cbdf3/_tmp_space.db
20:27:22.250 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
20:27:22.388 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
20:27:24.296 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
20:27:24.706 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:27:24.706 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:27:24.713 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:27:24.713 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:27:24.864 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:27:24.865 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:27:24.906 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:24.923 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:24.923 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:24.924 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:24.924 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:24.924 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:24.924 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:24.925 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:24.925 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:24.925 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:27:24.989 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
20:27:25.926 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
20:27:25.944 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
20:27:25.944 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
20:27:25.945 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
20:27:25.946 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
20:27:25.946 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
20:27:25.947 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
20:27:25.948 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
20:27:26.685 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:27:26.685 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:27:26.690 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:27:26.690 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:27:26.715 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:27:26.715 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:27:26.743 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:26.744 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:26.744 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:26.744 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:26.744 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:26.745 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:26.745 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:26.745 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:26.745 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:26.745 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:27:26.803 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:27:26.803 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:27:26.818 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:27:26.818 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:27:27.530 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 2019-09-06)
20:27:27.534 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 03)
20:27:27.539 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
20:27:27.626 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
20:27:27.662 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
20:27:28.555 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 254.1549 ms
20:27:29.128 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 63.8206 ms
20:27:29.406 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 310.4 KB, free 1445.4 MB)
20:27:29.562 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.7 KB, free 1445.4 MB)
20:27:29.564 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.145.1:8082 (size: 26.7 KB, free: 1445.7 MB)
20:27:29.641 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DWReleaseExposure.scala:48
20:27:29.798 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 11968796 bytes, open cost is considered as scanning 4194304 bytes.
20:27:30.384 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseExposure.scala:48
20:27:30.460 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 3 (show at DWReleaseExposure.scala:48)
20:27:30.463 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DWReleaseExposure.scala:48) with 1 output partitions
20:27:30.464 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DWReleaseExposure.scala:48)
20:27:30.464 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
20:27:30.466 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
20:27:30.524 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at show at DWReleaseExposure.scala:48), which has no missing parents
20:27:30.838 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 17.6 KB, free 1445.4 MB)
20:27:30.841 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1445.3 MB)
20:27:30.841 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.145.1:8082 (size: 7.5 KB, free: 1445.7 MB)
20:27:30.842 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
20:27:30.860 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at show at DWReleaseExposure.scala:48) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20:27:30.861 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 4 tasks
20:27:30.936 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5390 bytes)
20:27:30.939 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5390 bytes)
20:27:30.940 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5390 bytes)
20:27:30.940 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5390 bytes)
20:27:30.984 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
20:27:30.984 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
20:27:30.984 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
20:27:30.984 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
20:27:31.247 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-06]
20:27:31.247 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-06]
20:27:31.247 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-06]
20:27:31.247 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-06]
20:27:31.317 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
20:27:31.989 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:27:31.989 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:27:32.042 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:27:32.144 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:27:32.361 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1458 bytes result sent to driver
20:27:32.361 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1501 bytes result sent to driver
20:27:32.361 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1458 bytes result sent to driver
20:27:32.392 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1468 ms on localhost (executor driver) (1/4)
20:27:32.396 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 1457 ms on localhost (executor driver) (2/4)
20:27:32.397 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 1457 ms on localhost (executor driver) (3/4)
20:27:33.526 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1587 bytes result sent to driver
20:27:33.530 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 2592 ms on localhost (executor driver) (4/4)
20:27:33.535 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
20:27:33.535 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (show at DWReleaseExposure.scala:48) finished in 2.643 s
20:27:33.537 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
20:27:33.538 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
20:27:33.549 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
20:27:33.551 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
20:27:33.565 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[5] at show at DWReleaseExposure.scala:48), which has no missing parents
20:27:33.575 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1445.3 MB)
20:27:33.577 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1445.3 MB)
20:27:33.578 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.145.1:8082 (size: 2.2 KB, free: 1445.7 MB)
20:27:33.579 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
20:27:33.581 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at DWReleaseExposure.scala:48) (first 15 tasks are for partitions Vector(0))
20:27:33.581 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
20:27:33.584 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 4, localhost, executor driver, partition 0, ANY, 4726 bytes)
20:27:33.584 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 4)
20:27:33.628 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
20:27:33.630 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 16 ms
20:27:33.689 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 4). 1575 bytes result sent to driver
20:27:33.690 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 4) in 108 ms on localhost (executor driver) (1/1)
20:27:33.690 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
20:27:33.690 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DWReleaseExposure.scala:48) finished in 0.108 s
20:27:33.715 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DWReleaseExposure.scala:48, took 3.330944 s
20:27:33.812 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_exposure
20:27:33.825 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:27:33.825 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:27:33.852 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:33.853 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:33.854 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:33.854 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:33.854 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:33.855 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:33.855 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:33.855 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:27:33.986 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://mycluster/data/release/dw/release_exposure/.hive-staging_hive_2019-09-10_20-27-33_984_5155434981485819816-1
20:27:34.390 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:27:34.390 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:27:34.396 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:27:34.396 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:27:34.423 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:27:34.424 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:27:34.452 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:34.453 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:34.453 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:34.453 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:34.453 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:34.453 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:34.454 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:34.454 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:34.454 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:34.454 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:27:34.457 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:27:34.457 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:27:34.457 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:27:34.457 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:27:34.497 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 2019-09-06)
20:27:34.498 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 03)
20:27:34.499 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
20:27:34.499 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
20:27:34.499 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
20:27:34.514 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:27:34.517 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:27:34.547 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 310.4 KB, free 1445.0 MB)
20:27:34.560 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 26.7 KB, free 1445.0 MB)
20:27:34.561 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.145.1:8082 (size: 26.7 KB, free: 1445.6 MB)
20:27:34.562 INFO  [main] org.apache.spark.SparkContext - Created broadcast 3 from insertInto at SparkHelper.scala:36
20:27:34.562 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 11968796 bytes, open cost is considered as scanning 4194304 bytes.
20:27:34.712 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:36
20:27:34.713 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 9 (insertInto at SparkHelper.scala:36)
20:27:34.714 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (insertInto at SparkHelper.scala:36) with 4 output partitions
20:27:34.714 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (insertInto at SparkHelper.scala:36)
20:27:34.714 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
20:27:34.714 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
20:27:34.716 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[9] at insertInto at SparkHelper.scala:36), which has no missing parents
20:27:34.719 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 17.6 KB, free 1445.0 MB)
20:27:34.721 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1445.0 MB)
20:27:34.722 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.145.1:8082 (size: 7.5 KB, free: 1445.6 MB)
20:27:34.722 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1004
20:27:34.723 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[9] at insertInto at SparkHelper.scala:36) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20:27:34.723 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 4 tasks
20:27:34.724 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 5, localhost, executor driver, partition 0, ANY, 5390 bytes)
20:27:34.724 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 6, localhost, executor driver, partition 1, ANY, 5390 bytes)
20:27:34.725 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 2.0 (TID 7, localhost, executor driver, partition 2, ANY, 5390 bytes)
20:27:34.725 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 2.0 (TID 8, localhost, executor driver, partition 3, ANY, 5390 bytes)
20:27:34.726 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 5)
20:27:34.726 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 6)
20:27:34.729 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 2.0 in stage 2.0 (TID 7)
20:27:34.729 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 3.0 in stage 2.0 (TID 8)
20:27:34.731 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-06]
20:27:34.737 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-06]
20:27:34.740 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-06]
20:27:34.743 INFO  [Executor task launch worker for task 8] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-06]
20:27:34.776 INFO  [Executor task launch worker for task 6] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:27:34.782 INFO  [Executor task launch worker for task 7] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:27:34.790 INFO  [Executor task launch worker for task 8] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:27:34.818 INFO  [Executor task launch worker for task 5] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:27:34.825 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 2.0 in stage 2.0 (TID 7). 1329 bytes result sent to driver
20:27:34.837 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 3.0 in stage 2.0 (TID 8). 1372 bytes result sent to driver
20:27:34.840 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 2.0 (TID 7) in 115 ms on localhost (executor driver) (1/4)
20:27:34.841 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 2.0 (TID 8) in 116 ms on localhost (executor driver) (2/4)
20:27:34.942 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 5). 1415 bytes result sent to driver
20:27:34.943 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 5) in 219 ms on localhost (executor driver) (3/4)
20:27:35.596 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.145.1:8082 in memory (size: 2.2 KB, free: 1445.6 MB)
20:27:35.603 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 81
20:27:35.662 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 1.0 in stage 2.0 (TID 6). 1587 bytes result sent to driver
20:27:35.663 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 6) in 939 ms on localhost (executor driver) (4/4)
20:27:35.663 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
20:27:35.665 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (insertInto at SparkHelper.scala:36) finished in 0.942 s
20:27:35.665 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
20:27:35.665 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
20:27:35.666 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 3)
20:27:35.666 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
20:27:35.666 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:36), which has no missing parents
20:27:35.715 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 168.9 KB, free 1444.8 MB)
20:27:35.717 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 63.0 KB, free 1444.8 MB)
20:27:35.717 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.145.1:8082 (size: 63.0 KB, free: 1445.6 MB)
20:27:35.718 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1004
20:27:35.718 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:36) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20:27:35.718 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 4 tasks
20:27:35.719 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 9, localhost, executor driver, partition 0, ANY, 4726 bytes)
20:27:35.719 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 10, localhost, executor driver, partition 1, ANY, 4726 bytes)
20:27:35.720 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 11, localhost, executor driver, partition 2, ANY, 4726 bytes)
20:27:35.720 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 3.0 (TID 12, localhost, executor driver, partition 3, ANY, 4726 bytes)
20:27:35.721 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 9)
20:27:35.721 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 11)
20:27:35.721 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 3.0 in stage 3.0 (TID 12)
20:27:35.721 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 10)
20:27:35.775 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
20:27:35.775 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:27:35.780 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
20:27:35.780 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:27:35.782 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
20:27:35.782 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:27:35.783 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
20:27:35.783 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:27:35.896 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 37.1069 ms
20:27:35.947 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 20.1241 ms
20:27:36.048 INFO  [Executor task launch worker for task 12] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:27:36.049 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:27:36.051 INFO  [Executor task launch worker for task 11] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:27:36.051 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:27:36.053 INFO  [Executor task launch worker for task 10] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:27:36.053 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:27:36.054 INFO  [Executor task launch worker for task 9] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:27:36.055 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:27:36.074 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.8965 ms
20:27:36.138 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 33.2114 ms
20:27:36.190 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 48.3716 ms
20:27:36.455 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@573c2c97
20:27:36.456 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3ef2552e
20:27:36.456 INFO  [Executor task launch worker for task 12] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@25387f93
20:27:36.456 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@78a96747
20:27:36.496 INFO  [Executor task launch worker for task 10] org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
20:27:36.517 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:27:36.517 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:27:36.517 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:27:36.517 INFO  [Executor task launch worker for task 12] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:27:36.517 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://mycluster/data/release/dw/release_exposure/.hive-staging_hive_2019-09-10_20-27-33_984_5155434981485819816-1/-ext-10000/_temporary/0/_temporary/attempt_20190910202736_0003_m_000002_0/bdp_day=2019-09-06/part-00002-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000
20:27:36.517 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://mycluster/data/release/dw/release_exposure/.hive-staging_hive_2019-09-10_20-27-33_984_5155434981485819816-1/-ext-10000/_temporary/0/_temporary/attempt_20190910202736_0003_m_000000_0/bdp_day=2019-09-06/part-00000-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000
20:27:36.517 INFO  [Executor task launch worker for task 12] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://mycluster/data/release/dw/release_exposure/.hive-staging_hive_2019-09-10_20-27-33_984_5155434981485819816-1/-ext-10000/_temporary/0/_temporary/attempt_20190910202736_0003_m_000003_0/bdp_day=2019-09-06/part-00003-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000
20:27:36.517 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://mycluster/data/release/dw/release_exposure/.hive-staging_hive_2019-09-10_20-27-33_984_5155434981485819816-1/-ext-10000/_temporary/0/_temporary/attempt_20190910202736_0003_m_000001_0/bdp_day=2019-09-06/part-00001-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000
20:27:36.526 INFO  [Executor task launch worker for task 10] parquet.hadoop.codec.CodecConfig - Compression set to false
20:27:36.526 INFO  [Executor task launch worker for task 9] parquet.hadoop.codec.CodecConfig - Compression set to false
20:27:36.526 INFO  [Executor task launch worker for task 11] parquet.hadoop.codec.CodecConfig - Compression set to false
20:27:36.526 INFO  [Executor task launch worker for task 12] parquet.hadoop.codec.CodecConfig - Compression set to false
20:27:36.526 INFO  [Executor task launch worker for task 10] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:27:36.526 INFO  [Executor task launch worker for task 9] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:27:36.526 INFO  [Executor task launch worker for task 11] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:27:36.526 INFO  [Executor task launch worker for task 12] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:27:36.527 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:27:36.527 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:27:36.527 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:27:36.527 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:27:36.527 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:27:36.527 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:27:36.527 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:27:36.527 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:27:36.527 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Validation is off
20:27:36.528 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Validation is off
20:27:36.528 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:27:36.528 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:27:36.528 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:27:36.528 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:27:36.528 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:27:36.528 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:27:36.528 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:27:36.528 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:27:36.528 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:27:36.528 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:27:36.528 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Validation is off
20:27:36.528 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Validation is off
20:27:36.528 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:27:36.528 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:27:36.836 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@40249174
20:27:36.836 INFO  [Executor task launch worker for task 12] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@1261bce2
20:27:36.837 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@58ffda6c
20:27:36.837 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@19cda6dc
20:27:37.170 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 4
20:27:37.170 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 7
20:27:37.170 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 5
20:27:37.240 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 0
20:27:37.240 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 1
20:27:37.243 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 192.168.145.1:8082 in memory (size: 26.7 KB, free: 1445.6 MB)
20:27:37.375 INFO  [Executor task launch worker for task 10] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,126
20:27:37.385 INFO  [Executor task launch worker for task 11] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,501
20:27:37.391 INFO  [Executor task launch worker for task 12] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,146
20:27:37.393 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 6
20:27:37.398 INFO  [Executor task launch worker for task 9] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,016
20:27:37.399 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.145.1:8082 in memory (size: 7.5 KB, free: 1445.6 MB)
20:27:37.400 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 8
20:27:37.400 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 3
20:27:37.403 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 192.168.145.1:8082 in memory (size: 7.5 KB, free: 1445.6 MB)
20:27:37.403 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 2
20:27:37.556 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 387,427B for [release_session] BINARY: 14,346 values, 387,350B raw, 387,350B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:27:37.556 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 387,427B for [release_session] BINARY: 14,346 values, 387,350B raw, 387,350B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:27:37.556 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 387,427B for [release_session] BINARY: 14,346 values, 387,350B raw, 387,350B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:27:37.557 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 387,400B for [release_session] BINARY: 14,345 values, 387,323B raw, 387,323B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:27:37.559 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:27:37.559 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 143,511B for [device_num] BINARY: 14,346 values, 143,468B raw, 143,468B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:27:37.560 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 3,658B for [device_type] BINARY: 14,346 values, 3,627B raw, 3,627B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:27:37.561 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:27:37.561 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,346 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:27:37.561 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,345 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:27:37.561 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:27:37.562 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 143,511B for [device_num] BINARY: 14,346 values, 143,468B raw, 143,468B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:27:37.562 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:27:37.564 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 143,501B for [device_num] BINARY: 14,345 values, 143,458B raw, 143,458B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:27:37.565 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 3,657B for [device_type] BINARY: 14,346 values, 3,626B raw, 3,626B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:27:37.565 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 143,511B for [device_num] BINARY: 14,346 values, 143,468B raw, 143,468B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:27:37.566 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,346 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
20:27:37.566 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 3,656B for [device_type] BINARY: 14,345 values, 3,625B raw, 3,625B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:27:37.567 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 3,658B for [device_type] BINARY: 14,346 values, 3,627B raw, 3,627B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:27:37.567 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,346 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:27:37.567 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,345 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:27:37.567 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,346 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:27:37.568 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,345 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:27:37.569 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:27:37.569 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:27:37.569 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,345 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
20:27:37.569 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,346 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
20:27:37.570 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,346 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
20:27:40.298 INFO  [Executor task launch worker for task 11] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190910202736_0003_m_000002_0' to hdfs://mycluster/data/release/dw/release_exposure/.hive-staging_hive_2019-09-10_20-27-33_984_5155434981485819816-1/-ext-10000/_temporary/0/task_20190910202736_0003_m_000002
20:27:40.298 INFO  [Executor task launch worker for task 10] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190910202736_0003_m_000001_0' to hdfs://mycluster/data/release/dw/release_exposure/.hive-staging_hive_2019-09-10_20-27-33_984_5155434981485819816-1/-ext-10000/_temporary/0/task_20190910202736_0003_m_000001
20:27:40.298 INFO  [Executor task launch worker for task 9] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190910202736_0003_m_000000_0' to hdfs://mycluster/data/release/dw/release_exposure/.hive-staging_hive_2019-09-10_20-27-33_984_5155434981485819816-1/-ext-10000/_temporary/0/task_20190910202736_0003_m_000000
20:27:40.303 INFO  [Executor task launch worker for task 11] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190910202736_0003_m_000002_0: Committed
20:27:40.303 INFO  [Executor task launch worker for task 10] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190910202736_0003_m_000001_0: Committed
20:27:40.304 INFO  [Executor task launch worker for task 9] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190910202736_0003_m_000000_0: Committed
20:27:40.323 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 9). 2570 bytes result sent to driver
20:27:40.323 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 10). 2570 bytes result sent to driver
20:27:40.323 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 11). 2570 bytes result sent to driver
20:27:40.326 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 10) in 4607 ms on localhost (executor driver) (1/4)
20:27:40.327 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 11) in 4607 ms on localhost (executor driver) (2/4)
20:27:40.327 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 9) in 4608 ms on localhost (executor driver) (3/4)
20:27:40.706 INFO  [Executor task launch worker for task 12] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190910202736_0003_m_000003_0' to hdfs://mycluster/data/release/dw/release_exposure/.hive-staging_hive_2019-09-10_20-27-33_984_5155434981485819816-1/-ext-10000/_temporary/0/task_20190910202736_0003_m_000003
20:27:40.706 INFO  [Executor task launch worker for task 12] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190910202736_0003_m_000003_0: Committed
20:27:40.708 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 3.0 in stage 3.0 (TID 12). 2527 bytes result sent to driver
20:27:40.709 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 3.0 (TID 12) in 4989 ms on localhost (executor driver) (4/4)
20:27:40.709 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
20:27:40.711 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (insertInto at SparkHelper.scala:36) finished in 4.991 s
20:27:40.712 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: insertInto at SparkHelper.scala:36, took 5.998985 s
20:27:40.789 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
20:27:40.790 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:27:40.790 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:27:40.814 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:27:40.815 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:27:40.837 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:40.838 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:40.838 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:40.839 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:40.839 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:40.839 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:40.839 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:40.840 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:27:40.848 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:27:40.848 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:27:40.914 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:27:40.914 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:27:40.915 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:27:40.915 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:27:40.916 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:27:40.917 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:27:40.917 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:27:40.917 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:27:40.945 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:27:40.945 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:27:40.975 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_exposure[2019-09-06]
20:27:40.975 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_exposure[2019-09-06]	
20:27:41.038 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/000000_0
20:27:41.044 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
20:27:41.096 ERROR [main] org.apache.hadoop.hdfs.KeyProviderCache - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20:27:41.114 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://mycluster/data/release/dw/release_exposure/.hive-staging_hive_2019-09-10_20-27-33_984_5155434981485819816-1/-ext-10000/bdp_day=2019-09-06/part-00000-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, dest: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00000-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, Status:true
20:27:41.149 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://mycluster/data/release/dw/release_exposure/.hive-staging_hive_2019-09-10_20-27-33_984_5155434981485819816-1/-ext-10000/bdp_day=2019-09-06/part-00001-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, dest: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00001-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, Status:true
20:27:41.163 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://mycluster/data/release/dw/release_exposure/.hive-staging_hive_2019-09-10_20-27-33_984_5155434981485819816-1/-ext-10000/bdp_day=2019-09-06/part-00002-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, dest: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00002-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, Status:true
20:27:41.198 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://mycluster/data/release/dw/release_exposure/.hive-staging_hive_2019-09-10_20-27-33_984_5155434981485819816-1/-ext-10000/bdp_day=2019-09-06/part-00003-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, dest: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00003-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, Status:true
20:27:41.204 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_exposure[2019-09-06]
20:27:41.204 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_exposure[2019-09-06]	
20:27:41.243 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_partition : db=dw_release tbl=dw_release_exposure
20:27:41.243 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=alter_partition : db=dw_release tbl=dw_release_exposure	
20:27:41.243 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - New partition values:[2019-09-06]
20:27:41.318 WARN  [main] hive.log - Updating partition stats fast for: dw_release_exposure
20:27:41.321 WARN  [main] hive.log - Updated size to 2286732
20:27:41.535 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://mycluster/data/release/dw/release_exposure/.hive-staging_hive_2019-09-10_20-27-33_984_5155434981485819816-1/-ext-10000/bdp_day=2019-09-06 with partSpec {bdp_day=2019-09-06}
20:27:41.554 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_exposure`
20:27:41.560 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:27:41.560 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:27:41.566 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:27:41.566 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:27:41.595 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:27:41.595 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:27:41.621 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:41.622 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:41.622 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:41.622 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:41.623 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:41.623 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:41.623 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:41.623 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:27:41.629 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Inferring case-sensitive schema for table dw_release.dw_release_exposure (inference mode: INFER_AND_SAVE)
20:27:41.631 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:27:41.631 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:27:41.636 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:27:41.636 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:27:41.664 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:27:41.664 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:27:41.687 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:41.687 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:41.688 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:41.688 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:41.688 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:41.688 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:41.688 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:41.689 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:27:41.690 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=dw_release tbl=dw_release_exposure
20:27:41.690 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_partitions : db=dw_release tbl=dw_release_exposure	
20:27:41.867 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:36
20:27:41.868 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 2 (insertInto at SparkHelper.scala:36) with 1 output partitions
20:27:41.868 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 4 (insertInto at SparkHelper.scala:36)
20:27:41.868 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
20:27:41.869 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
20:27:41.869 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 4 (MapPartitionsRDD[13] at insertInto at SparkHelper.scala:36), which has no missing parents
20:27:41.885 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 75.1 KB, free 1445.1 MB)
20:27:41.886 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 27.4 KB, free 1445.0 MB)
20:27:41.887 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 192.168.145.1:8082 (size: 27.4 KB, free: 1445.6 MB)
20:27:41.887 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1004
20:27:41.888 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[13] at insertInto at SparkHelper.scala:36) (first 15 tasks are for partitions Vector(0))
20:27:41.888 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
20:27:41.891 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 13, localhost, executor driver, partition 0, PROCESS_LOCAL, 5047 bytes)
20:27:41.892 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 13)
20:27:42.017 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 13). 1648 bytes result sent to driver
20:27:42.018 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 13) in 128 ms on localhost (executor driver) (1/1)
20:27:42.018 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
20:27:42.018 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 4 (insertInto at SparkHelper.scala:36) finished in 0.130 s
20:27:42.019 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 2 finished: insertInto at SparkHelper.scala:36, took 0.151434 s
20:27:42.044 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Saving case-sensitive schema for table dw_release.dw_release_exposure
20:27:42.045 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:27:42.045 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:27:42.049 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:27:42.049 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:27:42.066 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:27:42.066 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:27:42.090 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:42.090 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:42.090 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:42.090 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:42.090 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:42.091 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:42.091 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:42.091 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:27:42.131 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:27:42.131 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:27:42.160 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:27:42.160 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:27:42.188 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:42.188 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:42.189 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:42.189 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:42.189 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:42.189 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:42.189 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:27:42.190 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:27:42.197 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:27:42.197 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:27:42.272 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_table: db=dw_release tbl=dw_release_exposure newtbl=dw_release_exposure
20:27:42.272 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=alter_table: db=dw_release tbl=dw_release_exposure newtbl=dw_release_exposure	
20:27:42.431 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@3fc08eec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20:27:42.433 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.145.1:4040
20:27:42.478 INFO  [dispatcher-event-loop-0] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
20:27:42.502 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
20:27:42.503 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
20:27:42.503 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
20:27:42.506 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
20:27:42.509 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
20:27:42.522 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
20:27:42.523 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\Yongning Y\AppData\Local\Temp\spark-58ba89e8-98e3-49ba-a178-eb8120638015
20:28:42.969 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
20:28:43.400 INFO  [main] org.apache.spark.SparkContext - Submitted application: mid_release_exposure_job
20:28:43.426 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: YongningY,root
20:28:43.426 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: YongningY,root
20:28:43.427 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
20:28:43.427 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
20:28:43.428 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(YongningY, root); groups with view permissions: Set(); users  with modify permissions: Set(YongningY, root); groups with modify permissions: Set()
20:28:44.078 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 8146.
20:28:44.095 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
20:28:44.112 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
20:28:44.115 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20:28:44.115 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
20:28:44.123 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\Yongning Y\AppData\Local\Temp\blockmgr-42b18f8a-640b-48ca-83e0-10d0ac534d72
20:28:44.141 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1445.7 MB
20:28:44.184 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
20:28:44.273 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @3993ms
20:28:44.351 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
20:28:44.367 INFO  [main] org.spark_project.jetty.server.Server - Started @4089ms
20:28:44.392 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@7d2632d5{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20:28:44.393 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
20:28:44.414 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60b85ba1{/jobs,null,AVAILABLE,@Spark}
20:28:44.415 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b718392{/jobs/json,null,AVAILABLE,@Spark}
20:28:44.416 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f2d2181{/jobs/job,null,AVAILABLE,@Spark}
20:28:44.417 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fcdcf{/jobs/job/json,null,AVAILABLE,@Spark}
20:28:44.417 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46292372{/stages,null,AVAILABLE,@Spark}
20:28:44.418 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c44052e{/stages/json,null,AVAILABLE,@Spark}
20:28:44.419 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@530a8454{/stages/stage,null,AVAILABLE,@Spark}
20:28:44.421 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31198ceb{/stages/stage/json,null,AVAILABLE,@Spark}
20:28:44.421 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75201592{/stages/pool,null,AVAILABLE,@Spark}
20:28:44.422 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@aa5455e{/stages/pool/json,null,AVAILABLE,@Spark}
20:28:44.423 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5dda14d0{/storage,null,AVAILABLE,@Spark}
20:28:44.424 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d9fc57a{/storage/json,null,AVAILABLE,@Spark}
20:28:44.425 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b4ef7{/storage/rdd,null,AVAILABLE,@Spark}
20:28:44.426 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5987e932{/storage/rdd/json,null,AVAILABLE,@Spark}
20:28:44.427 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bbbdd4b{/environment,null,AVAILABLE,@Spark}
20:28:44.427 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25230246{/environment/json,null,AVAILABLE,@Spark}
20:28:44.428 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a8b5227{/executors,null,AVAILABLE,@Spark}
20:28:44.428 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6979efad{/executors/json,null,AVAILABLE,@Spark}
20:28:44.429 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67318f{/executors/threadDump,null,AVAILABLE,@Spark}
20:28:44.430 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17f9344b{/executors/threadDump/json,null,AVAILABLE,@Spark}
20:28:44.446 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54e81b21{/static,null,AVAILABLE,@Spark}
20:28:44.448 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2237bada{/,null,AVAILABLE,@Spark}
20:28:44.449 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5710768a{/api,null,AVAILABLE,@Spark}
20:28:44.450 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bf61e67{/jobs/job/kill,null,AVAILABLE,@Spark}
20:28:44.451 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b273a59{/stages/stage/kill,null,AVAILABLE,@Spark}
20:28:44.453 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.145.1:4040
20:28:44.531 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
20:28:44.555 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 8167.
20:28:44.555 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.145.1:8167
20:28:44.557 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20:28:44.588 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.145.1, 8167, None)
20:28:44.591 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.145.1:8167 with 1445.7 MB RAM, BlockManagerId(driver, 192.168.145.1, 8167, None)
20:28:44.594 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.145.1, 8167, None)
20:28:44.595 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.145.1, 8167, None)
20:28:44.840 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f02251{/metrics/json,null,AVAILABLE,@Spark}
20:28:44.903 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/H:/IdeaProject/release_test/target/classes/hive-site.xml
20:28:44.934 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
20:28:44.935 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/user/hive/warehouse'.
20:28:44.943 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64138b0c{/SQL,null,AVAILABLE,@Spark}
20:28:44.944 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22d9c961{/SQL/json,null,AVAILABLE,@Spark}
20:28:44.945 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79ab34c1{/SQL/execution,null,AVAILABLE,@Spark}
20:28:44.946 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@281f23f2{/SQL/execution/json,null,AVAILABLE,@Spark}
20:28:44.948 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6093d508{/static/sql,null,AVAILABLE,@Spark}
20:28:45.874 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20:28:46.709 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20:28:46.739 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
20:28:47.765 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20:28:49.076 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
20:28:49.079 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
20:28:49.328 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
20:28:49.332 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
20:28:49.378 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
20:28:49.513 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
20:28:49.516 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
20:28:49.533 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=azkaban pat=*
20:28:49.533 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=azkaban pat=*	
20:28:49.590 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dim pat=*
20:28:49.590 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dim pat=*	
20:28:49.593 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dm pat=*
20:28:49.593 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dm pat=*	
20:28:49.597 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dwd pat=*
20:28:49.597 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dwd pat=*	
20:28:49.601 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dws pat=*
20:28:49.601 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dws pat=*	
20:28:49.605 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_ods pat=*
20:28:49.606 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_ods pat=*	
20:28:49.609 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
20:28:49.610 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
20:28:49.613 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_release pat=*
20:28:49.613 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_release pat=*	
20:28:49.616 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_shop pat=*
20:28:49.616 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_shop pat=*	
20:28:49.619 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
20:28:49.620 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
20:28:49.623 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_shop pat=*
20:28:49.623 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_shop pat=*	
20:28:49.626 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
20:28:49.626 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
20:28:49.629 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_shop pat=*
20:28:49.629 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_shop pat=*	
20:28:49.632 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_release pat=*
20:28:49.633 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_release pat=*	
20:28:49.636 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_shop pat=*
20:28:49.636 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_shop pat=*	
20:28:49.640 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
20:28:49.640 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
20:28:49.643 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_shop pat=*
20:28:49.643 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_shop pat=*	
20:28:49.646 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dim pat=*
20:28:49.647 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dim pat=*	
20:28:49.649 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dm pat=*
20:28:49.649 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dm pat=*	
20:28:49.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dw pat=*
20:28:49.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dw pat=*	
20:28:49.655 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_rds pat=*
20:28:49.656 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_rds pat=*	
20:28:49.659 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=te pat=*
20:28:49.659 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=te pat=*	
20:28:49.661 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test pat=*
20:28:49.661 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test pat=*	
20:28:49.665 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test4 pat=*
20:28:49.665 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test4 pat=*	
20:28:51.343 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/62e62ff9-e4d0-4529-8625-544fe0c50be8_resources
20:28:51.470 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/62e62ff9-e4d0-4529-8625-544fe0c50be8
20:28:51.482 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/62e62ff9-e4d0-4529-8625-544fe0c50be8
20:28:51.535 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/62e62ff9-e4d0-4529-8625-544fe0c50be8/_tmp_space.db
20:28:51.560 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
20:28:51.594 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for root: Unknown error.
20:28:51.609 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:28:51.609 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:28:51.615 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
20:28:51.615 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
20:28:51.619 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
20:28:51.810 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/2e2d49e5-4678-40f6-b8bd-484a04830cc7_resources
20:28:51.819 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/2e2d49e5-4678-40f6-b8bd-484a04830cc7
20:28:51.824 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/2e2d49e5-4678-40f6-b8bd-484a04830cc7
20:28:51.829 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/2e2d49e5-4678-40f6-b8bd-484a04830cc7/_tmp_space.db
20:28:51.832 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
20:28:51.867 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
20:28:53.261 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_exposure
20:28:53.440 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:28:53.440 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:28:53.446 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:28:53.446 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:28:53.545 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:28:53.545 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:28:53.575 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:53.587 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:53.588 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:53.588 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:53.588 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:53.588 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:53.588 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:53.588 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:28:53.602 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
20:28:53.920 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
20:28:53.947 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
20:28:53.990 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
20:28:53.991 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:28:53.992 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:28:53.998 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:28:53.998 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:28:54.029 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:28:54.029 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:28:54.060 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:54.061 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:54.061 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:54.061 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:54.061 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:54.061 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:54.062 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:54.062 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:54.062 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:54.062 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:28:54.084 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
20:28:54.085 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as goods_id
20:28:54.156 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: floor(datediff(from_unixtime(unix_timestamp(),'yyyy-MM-dd'),from_unixtime(unix_timestamp(substr(get_json_object(exts,'$.idcard'),7,8),'yyyyMMdd')))/365) as age
20:28:54.168 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: case subStr(get_json_object(exts,'$.idcard'),17,1)%2 when 1 then '男'  when 0 then '女' end as gender 
20:28:54.190 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
20:28:54.192 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
20:28:54.195 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
20:28:54.197 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
20:28:54.198 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
20:28:54.198 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_version
20:28:54.200 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') aid
20:28:54.209 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:28:54.209 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:28:54.216 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:28:54.216 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:28:54.222 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:28:54.222 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:28:54.226 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:28:54.227 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:28:54.232 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:28:54.232 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:28:54.238 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:28:54.238 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:28:54.242 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:28:54.242 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:28:54.246 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:28:54.247 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:28:54.250 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:28:54.251 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:28:54.256 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:28:54.256 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:28:54.260 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:28:54.260 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:28:54.265 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:28:54.265 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:28:54.269 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:28:54.270 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:28:54.273 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:28:54.273 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:28:54.278 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:28:54.278 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:28:54.282 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:28:54.282 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:28:54.286 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:28:54.286 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:28:54.289 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:28:54.290 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:28:54.294 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:28:54.294 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:28:54.298 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:28:54.298 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:28:54.832 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: 
20:28:54.836 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
20:28:54.840 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<ct: bigint>
20:28:54.855 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: 
20:28:54.859 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:28:54.860 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:28:54.867 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:28:54.868 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:28:54.896 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:28:54.897 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:28:54.920 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:54.920 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:54.921 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:54.921 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:54.921 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:54.921 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:54.921 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:54.921 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:28:54.945 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=dw_release tbl=dw_release_exposure
20:28:54.946 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_partitions : db=dw_release tbl=dw_release_exposure	
20:28:55.075 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: 
20:28:55.076 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
20:28:55.077 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, exts: string>
20:28:55.077 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: 
20:28:55.077 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:28:55.078 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:28:55.082 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:28:55.083 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:28:55.113 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:28:55.113 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:28:55.139 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:55.140 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:55.140 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:55.140 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:55.141 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:55.141 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:55.141 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:55.141 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:55.142 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:28:55.142 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:28:55.144 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=ods_release tbl=ods_01_release_session
20:28:55.144 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_partitions : db=ods_release tbl=ods_01_release_session	
20:28:55.627 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 295.955 ms
20:28:55.871 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 52.5368 ms
20:28:55.957 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 307.7 KB, free 1445.4 MB)
20:28:56.034 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1445.4 MB)
20:28:56.037 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.145.1:8167 (size: 26.3 KB, free: 1445.7 MB)
20:28:56.041 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at MIDReleaseExposure.scala:36
20:28:56.049 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4765987 bytes, open cost is considered as scanning 4194304 bytes.
20:28:56.146 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
20:28:56.210 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 22.1545 ms
20:28:56.219 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 308.2 KB, free 1445.1 MB)
20:28:56.241 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 26.4 KB, free 1445.0 MB)
20:28:56.242 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.145.1:8167 (size: 26.4 KB, free: 1445.6 MB)
20:28:56.243 INFO  [main] org.apache.spark.SparkContext - Created broadcast 1 from show at MIDReleaseExposure.scala:36
20:28:56.244 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 11968796 bytes, open cost is considered as scanning 4194304 bytes.
20:28:56.308 INFO  [main] org.apache.spark.SparkContext - Starting job: show at MIDReleaseExposure.scala:36
20:28:56.328 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 8 (show at MIDReleaseExposure.scala:36)
20:28:56.331 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at MIDReleaseExposure.scala:36) with 1 output partitions
20:28:56.332 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at MIDReleaseExposure.scala:36)
20:28:56.332 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
20:28:56.335 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
20:28:56.340 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[8] at show at MIDReleaseExposure.scala:36), which has no missing parents
20:28:56.498 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 25.1 KB, free 1445.0 MB)
20:28:56.501 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.4 KB, free 1445.0 MB)
20:28:56.503 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.145.1:8167 (size: 10.4 KB, free: 1445.6 MB)
20:28:56.503 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
20:28:56.517 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 16 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[8] at show at MIDReleaseExposure.scala:36) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20:28:56.519 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 16 tasks
20:28:56.561 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5802 bytes)
20:28:56.563 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5802 bytes)
20:28:56.564 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5802 bytes)
20:28:56.565 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5802 bytes)
20:28:56.576 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
20:28:56.576 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
20:28:56.576 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
20:28:56.576 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
20:28:56.856 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 97.4612 ms
20:28:56.871 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-06]
20:28:56.871 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-06]
20:28:56.871 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-06]
20:28:56.873 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-06]
20:28:57.563 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 64.3974 ms
20:28:57.593 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00000-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, range: 0-571693, partition values: [2019-09-06]
20:28:57.593 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00000-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, range: 0-571693, partition values: [2019-09-06]
20:28:57.593 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00000-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, range: 0-571693, partition values: [2019-09-06]
20:28:57.618 WARN  [Executor task launch worker for task 3] org.apache.parquet.CorruptStatistics - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
20:28:58.220 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 2021 bytes result sent to driver
20:28:58.220 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2064 bytes result sent to driver
20:28:58.220 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 2064 bytes result sent to driver
20:28:58.225 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 5802 bytes)
20:28:58.225 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 4.0 in stage 0.0 (TID 4)
20:28:58.226 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 5802 bytes)
20:28:58.227 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 5.0 in stage 0.0 (TID 5)
20:28:58.227 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 5802 bytes)
20:28:58.231 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 6.0 in stage 0.0 (TID 6)
20:28:58.247 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-06]
20:28:58.250 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-06]
20:28:58.256 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-06]
20:28:58.259 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 1693 ms on localhost (executor driver) (1/16)
20:28:58.305 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 1741 ms on localhost (executor driver) (2/16)
20:28:58.309 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1760 ms on localhost (executor driver) (3/16)
20:28:58.330 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00003-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, range: 0-571693, partition values: [2019-09-06]
20:28:58.346 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00003-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, range: 0-571693, partition values: [2019-09-06]
20:28:58.608 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 6.0 in stage 0.0 (TID 6). 1978 bytes result sent to driver
20:28:58.610 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 5802 bytes)
20:28:58.610 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 7.0 in stage 0.0 (TID 7)
20:28:58.624 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-06]
20:28:58.642 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00003-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, range: 0-571693, partition values: [2019-09-06]
20:28:58.658 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 4.0 in stage 0.0 (TID 4). 1935 bytes result sent to driver
20:28:58.659 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, ANY, 5802 bytes)
20:28:58.660 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 0.0 (TID 4) in 438 ms on localhost (executor driver) (4/16)
20:28:58.669 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 0.0 (TID 6) in 442 ms on localhost (executor driver) (5/16)
20:28:58.753 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 8.0 in stage 0.0 (TID 8)
20:28:58.764 INFO  [Executor task launch worker for task 8] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-06]
20:28:58.772 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 7.0 in stage 0.0 (TID 7). 1935 bytes result sent to driver
20:28:58.773 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, ANY, 5802 bytes)
20:28:58.774 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 0.0 (TID 7) in 165 ms on localhost (executor driver) (6/16)
20:28:58.775 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 9.0 in stage 0.0 (TID 9)
20:28:58.787 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-06]
20:28:58.854 INFO  [Executor task launch worker for task 8] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00001-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, range: 0-571692, partition values: [2019-09-06]
20:28:58.937 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 8.0 in stage 0.0 (TID 8). 1935 bytes result sent to driver
20:28:58.938 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 10.0 in stage 0.0 (TID 10, localhost, executor driver, partition 10, ANY, 5802 bytes)
20:28:58.938 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 10.0 in stage 0.0 (TID 10)
20:28:58.938 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 0.0 (TID 8) in 279 ms on localhost (executor driver) (7/16)
20:28:58.948 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-06]
20:28:59.003 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00001-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, range: 0-571692, partition values: [2019-09-06]
20:28:59.045 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 10.0 in stage 0.0 (TID 10). 1935 bytes result sent to driver
20:28:59.046 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 11.0 in stage 0.0 (TID 11, localhost, executor driver, partition 11, ANY, 5802 bytes)
20:28:59.046 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 11.0 in stage 0.0 (TID 11)
20:28:59.046 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 10.0 in stage 0.0 (TID 10) in 109 ms on localhost (executor driver) (8/16)
20:28:59.056 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-06]
20:28:59.146 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00001-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, range: 0-571692, partition values: [2019-09-06]
20:28:59.186 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 11.0 in stage 0.0 (TID 11). 1935 bytes result sent to driver
20:28:59.187 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 12.0 in stage 0.0 (TID 12, localhost, executor driver, partition 12, ANY, 5802 bytes)
20:28:59.187 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 12.0 in stage 0.0 (TID 12)
20:28:59.187 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 11.0 in stage 0.0 (TID 11) in 141 ms on localhost (executor driver) (9/16)
20:28:59.197 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-06]
20:28:59.218 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00002-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, range: 0-571654, partition values: [2019-09-06]
20:28:59.245 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 12.0 in stage 0.0 (TID 12). 1935 bytes result sent to driver
20:28:59.247 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 13.0 in stage 0.0 (TID 13, localhost, executor driver, partition 13, ANY, 5802 bytes)
20:28:59.247 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Running task 13.0 in stage 0.0 (TID 13)
20:28:59.247 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 12.0 in stage 0.0 (TID 12) in 61 ms on localhost (executor driver) (10/16)
20:28:59.257 INFO  [Executor task launch worker for task 13] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-06]
20:29:01.180 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray - Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
20:29:01.759 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray - Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
20:29:04.107 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray - Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
20:29:04.151 INFO  [Executor task launch worker for task 13] org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray - Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
20:29:10.074 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00000-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, range: 0-571693, partition values: [2019-09-06]
20:29:10.217 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00003-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, range: 0-571693, partition values: [2019-09-06]
20:29:11.408 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (0  time so far)
20:29:11.420 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (0  time so far)
20:29:12.765 INFO  [Executor task launch worker for task 13] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00002-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, range: 0-571654, partition values: [2019-09-06]
20:29:12.849 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00001-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, range: 0-571692, partition values: [2019-09-06]
20:29:13.338 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (0  time so far)
20:29:13.428 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (0  time so far)
20:29:13.521 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (1  time so far)
20:29:13.556 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (1  time so far)
20:29:15.160 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (1  time so far)
20:29:15.209 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (1  time so far)
20:29:15.351 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (2  times so far)
20:29:15.373 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (2  times so far)
20:29:21.111 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (3  times so far)
20:29:21.146 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (2  times so far)
20:29:21.440 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (2  times so far)
20:29:21.485 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (3  times so far)
20:29:25.411 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (4  times so far)
20:29:25.552 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (3  times so far)
20:29:26.023 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (3  times so far)
20:29:26.108 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (4  times so far)
20:29:34.203 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (4  times so far)
20:29:34.453 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (5  times so far)
20:29:34.525 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (4  times so far)
20:29:34.640 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (5  times so far)
20:29:38.345 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (5  times so far)
20:29:39.134 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (6  times so far)
20:29:39.532 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (5  times so far)
20:29:39.646 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (6  times so far)
20:29:42.699 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (6  times so far)
20:29:43.941 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (7  times so far)
20:29:44.712 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (6  times so far)
20:29:44.886 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (7  times so far)
20:29:47.450 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (7  times so far)
20:29:48.278 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (8  times so far)
20:29:49.220 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (7  times so far)
20:29:49.670 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (8  times so far)
20:29:52.796 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (8  times so far)
20:29:53.595 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (9  times so far)
20:29:53.732 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (8  times so far)
20:29:54.212 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (9  times so far)
20:29:56.673 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (9  times so far)
20:29:58.592 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (9  times so far)
20:29:58.855 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (10  times so far)
20:29:58.994 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (10  times so far)
20:30:00.353 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (10  times so far)
20:30:03.446 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (10  times so far)
20:30:04.067 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (11  times so far)
20:30:04.108 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (11  times so far)
20:30:04.447 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (11  times so far)
20:30:07.027 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (11  times so far)
20:30:12.568 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (12  times so far)
20:30:12.999 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (12  times so far)
20:30:13.640 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (12  times so far)
20:30:14.704 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (12  times so far)
20:30:17.876 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (13  times so far)
20:30:18.674 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (13  times so far)
20:30:19.148 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (13  times so far)
20:30:21.030 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (13  times so far)
20:30:23.071 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (14  times so far)
20:30:23.917 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (14  times so far)
20:30:24.116 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (14  times so far)
20:30:25.267 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (14  times so far)
20:30:27.555 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (15  times so far)
20:30:28.785 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (15  times so far)
20:30:29.167 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (15  times so far)
20:30:30.287 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (15  times so far)
20:30:31.741 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (16  times so far)
20:30:33.641 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (16  times so far)
20:30:34.013 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (16  times so far)
20:30:35.482 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (16  times so far)
20:30:36.171 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (17  times so far)
20:30:38.027 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (17  times so far)
20:30:38.805 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (17  times so far)
20:30:40.523 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (17  times so far)
20:30:40.972 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (18  times so far)
20:30:42.705 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (18  times so far)
20:30:42.805 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (18  times so far)
20:30:45.459 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (19  times so far)
20:30:45.833 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (18  times so far)
20:30:47.037 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (19  times so far)
20:30:47.521 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (19  times so far)
20:30:49.861 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (20  times so far)
20:30:50.021 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (19  times so far)
20:30:55.879 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (20  times so far)
20:30:56.179 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (20  times so far)
20:30:59.212 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (20  times so far)
20:30:59.224 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (21  times so far)
20:31:00.106 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (21  times so far)
20:31:00.678 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (21  times so far)
20:31:04.066 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (22  times so far)
20:31:04.114 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (21  times so far)
20:31:04.489 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (22  times so far)
20:31:04.570 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (22  times so far)
20:31:08.411 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (23  times so far)
20:31:08.969 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (22  times so far)
20:31:09.188 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (23  times so far)
20:31:09.607 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (23  times so far)
20:31:12.011 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (24  times so far)
20:31:13.980 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (24  times so far)
20:31:14.068 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (23  times so far)
20:31:14.330 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (24  times so far)
20:31:15.585 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (25  times so far)
20:31:18.730 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (25  times so far)
20:31:19.030 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (24  times so far)
20:31:19.264 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (25  times so far)
20:31:19.610 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (26  times so far)
20:31:23.035 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (26  times so far)
20:31:23.930 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (25  times so far)
20:31:24.236 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (27  times so far)
20:31:24.258 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (26  times so far)
20:31:25.745 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (27  times so far)
20:31:28.435 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (26  times so far)
20:31:28.805 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (28  times so far)
20:31:29.017 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (27  times so far)
20:31:29.530 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (28  times so far)
20:31:36.192 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (27  times so far)
20:31:37.558 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (28  times so far)
20:31:37.796 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (29  times so far)
20:31:38.893 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (29  times so far)
20:31:39.714 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (28  times so far)
20:31:42.004 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (29  times so far)
20:31:42.583 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (30  times so far)
20:31:43.694 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (30  times so far)
20:31:44.360 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (29  times so far)
20:31:45.833 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (30  times so far)
20:31:47.408 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (31  times so far)
20:31:48.588 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (31  times so far)
20:31:49.063 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (30  times so far)
20:31:50.308 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (31  times so far)
20:31:52.841 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (32  times so far)
20:31:54.356 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (32  times so far)
20:31:54.784 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (31  times so far)
20:31:57.552 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (32  times so far)
20:31:59.923 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (33  times so far)
20:32:00.829 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (33  times so far)
20:32:00.980 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (32  times so far)
20:32:03.231 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (33  times so far)
20:32:06.120 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (34  times so far)
20:32:07.059 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (34  times so far)
20:32:08.224 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (33  times so far)
20:32:09.883 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (34  times so far)
20:32:11.724 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (35  times so far)
20:32:11.761 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (35  times so far)
20:32:12.577 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (34  times so far)
20:32:16.253 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (35  times so far)
20:32:20.754 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (36  times so far)
20:32:20.959 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (36  times so far)
20:32:21.470 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (35  times so far)
20:32:22.599 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (36  times so far)
20:32:25.521 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (37  times so far)
20:32:25.891 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (37  times so far)
20:32:26.781 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (36  times so far)
20:32:27.561 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (37  times so far)
20:32:30.229 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (38  times so far)
20:32:31.359 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (38  times so far)
20:32:31.463 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (37  times so far)
20:32:32.762 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (38  times so far)
20:32:34.541 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (39  times so far)
20:32:36.703 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (39  times so far)
20:32:37.089 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (38  times so far)
20:32:37.689 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (39  times so far)
20:32:39.174 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (40  times so far)
20:32:40.987 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (40  times so far)
20:32:41.942 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (39  times so far)
20:32:42.679 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (40  times so far)
20:32:43.983 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (41  times so far)
20:32:45.475 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (41  times so far)
20:32:46.994 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (40  times so far)
20:32:47.369 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (41  times so far)
20:32:48.996 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (42  times so far)
20:32:50.820 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (42  times so far)
20:32:51.515 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (41  times so far)
20:32:51.996 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (42  times so far)
20:32:53.956 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (43  times so far)
20:32:56.237 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (43  times so far)
20:32:56.485 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (42  times so far)
20:32:56.725 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (43  times so far)
20:33:01.829 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (44  times so far)
20:33:06.705 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (44  times so far)
20:33:07.027 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (44  times so far)
20:33:07.233 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (43  times so far)
20:33:07.754 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (45  times so far)
20:33:11.461 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (45  times so far)
20:33:12.018 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (45  times so far)
20:33:12.213 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (44  times so far)
20:33:12.575 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (46  times so far)
20:33:15.795 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (46  times so far)
20:33:17.291 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (45  times so far)
20:33:17.293 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (46  times so far)
20:33:18.270 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (47  times so far)
20:33:19.714 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (47  times so far)
20:33:22.137 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (46  times so far)
20:33:23.479 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (47  times so far)
20:33:23.641 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (48  times so far)
20:33:24.521 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (48  times so far)
20:33:26.770 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (47  times so far)
20:33:28.480 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (48  times so far)
20:33:29.142 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (49  times so far)
20:33:29.646 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (49  times so far)
20:33:31.258 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (48  times so far)
20:33:32.923 INFO  [Executor task launch worker for task 13] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 78 spilling sort data of 288.0 MB to disk (49  times so far)
20:33:33.976 INFO  [Executor task launch worker for task 1] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 69 spilling sort data of 288.0 MB to disk (50  times so far)
20:33:34.692 INFO  [Executor task launch worker for task 5] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 70 spilling sort data of 288.0 MB to disk (50  times so far)
20:33:35.443 INFO  [Executor task launch worker for task 9] org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter - Thread 68 spilling sort data of 288.0 MB to disk (49  times so far)
20:45:18.271 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
20:45:18.872 INFO  [main] org.apache.spark.SparkContext - Submitted application: mid_release_exposure_job
20:45:18.898 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: YongningY,root
20:45:18.898 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: YongningY,root
20:45:18.899 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
20:45:18.900 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
20:45:18.900 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(YongningY, root); groups with view permissions: Set(); users  with modify permissions: Set(YongningY, root); groups with modify permissions: Set()
20:45:19.789 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 8251.
20:45:19.862 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
20:45:19.995 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
20:45:20.017 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20:45:20.018 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
20:45:20.060 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\Yongning Y\AppData\Local\Temp\blockmgr-f94fc0ae-e5c4-4c3a-b6c2-44b0b3b66b20
20:45:20.158 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1445.7 MB
20:45:20.305 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
20:45:20.568 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @4763ms
20:45:20.747 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
20:45:20.765 INFO  [main] org.spark_project.jetty.server.Server - Started @4961ms
20:45:20.799 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@3fc08eec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20:45:20.800 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
20:45:20.837 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@492fc69e{/jobs,null,AVAILABLE,@Spark}
20:45:20.838 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d2260db{/jobs/json,null,AVAILABLE,@Spark}
20:45:20.838 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49bf29c6{/jobs/job,null,AVAILABLE,@Spark}
20:45:20.839 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7668d560{/jobs/job/json,null,AVAILABLE,@Spark}
20:45:20.840 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@126be319{/stages,null,AVAILABLE,@Spark}
20:45:20.841 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c371e13{/stages/json,null,AVAILABLE,@Spark}
20:45:20.841 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e34c607{/stages/stage,null,AVAILABLE,@Spark}
20:45:20.850 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9257031{/stages/stage/json,null,AVAILABLE,@Spark}
20:45:20.851 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7726e185{/stages/pool,null,AVAILABLE,@Spark}
20:45:20.852 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@282308c3{/stages/pool/json,null,AVAILABLE,@Spark}
20:45:20.853 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1db0ec27{/storage,null,AVAILABLE,@Spark}
20:45:20.859 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d4ab71a{/storage/json,null,AVAILABLE,@Spark}
20:45:20.863 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1af05b03{/storage/rdd,null,AVAILABLE,@Spark}
20:45:20.864 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ad777f{/storage/rdd/json,null,AVAILABLE,@Spark}
20:45:20.867 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@438bad7c{/environment,null,AVAILABLE,@Spark}
20:45:20.870 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4fdf8f12{/environment/json,null,AVAILABLE,@Spark}
20:45:20.871 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54f5f647{/executors,null,AVAILABLE,@Spark}
20:45:20.873 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a6d5a8f{/executors/json,null,AVAILABLE,@Spark}
20:45:20.874 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@315ba14a{/executors/threadDump,null,AVAILABLE,@Spark}
20:45:20.875 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27f0ad19{/executors/threadDump/json,null,AVAILABLE,@Spark}
20:45:20.886 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38d5b107{/static,null,AVAILABLE,@Spark}
20:45:20.887 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77e2a6e2{/,null,AVAILABLE,@Spark}
20:45:20.889 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@199e4c2b{/api,null,AVAILABLE,@Spark}
20:45:20.890 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/job/kill,null,AVAILABLE,@Spark}
20:45:20.890 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e7095ac{/stages/stage/kill,null,AVAILABLE,@Spark}
20:45:20.895 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.145.1:4040
20:45:21.174 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
20:45:21.244 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 8272.
20:45:21.245 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.145.1:8272
20:45:21.261 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20:45:21.302 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.145.1, 8272, None)
20:45:21.313 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.145.1:8272 with 1445.7 MB RAM, BlockManagerId(driver, 192.168.145.1, 8272, None)
20:45:21.319 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.145.1, 8272, None)
20:45:21.319 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.145.1, 8272, None)
20:45:21.596 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@dffa30b{/metrics/json,null,AVAILABLE,@Spark}
20:45:21.793 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/H:/IdeaProject/release_test/target/classes/hive-site.xml
20:45:21.891 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
20:45:21.892 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/user/hive/warehouse'.
20:45:21.936 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f443fae{/SQL,null,AVAILABLE,@Spark}
20:45:21.937 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79ab34c1{/SQL/json,null,AVAILABLE,@Spark}
20:45:21.938 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@782168b7{/SQL/execution,null,AVAILABLE,@Spark}
20:45:21.938 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7435a578{/SQL/execution/json,null,AVAILABLE,@Spark}
20:45:21.947 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b214b94{/static/sql,null,AVAILABLE,@Spark}
20:45:23.962 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20:45:25.005 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20:45:25.048 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
20:45:26.376 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20:45:28.532 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
20:45:28.535 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
20:45:29.301 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
20:45:29.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
20:45:29.407 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
20:45:29.609 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
20:45:29.611 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
20:45:29.635 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=azkaban pat=*
20:45:29.636 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=azkaban pat=*	
20:45:29.751 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dim pat=*
20:45:29.752 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dim pat=*	
20:45:29.775 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dm pat=*
20:45:29.775 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dm pat=*	
20:45:29.780 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dwd pat=*
20:45:29.781 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dwd pat=*	
20:45:29.786 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dws pat=*
20:45:29.787 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dws pat=*	
20:45:29.793 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_ods pat=*
20:45:29.794 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_ods pat=*	
20:45:29.800 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
20:45:29.800 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
20:45:29.805 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_release pat=*
20:45:29.805 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_release pat=*	
20:45:29.809 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_shop pat=*
20:45:29.810 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_shop pat=*	
20:45:29.814 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
20:45:29.815 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
20:45:29.819 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_shop pat=*
20:45:29.819 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_shop pat=*	
20:45:29.825 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
20:45:29.825 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
20:45:29.831 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_shop pat=*
20:45:29.831 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_shop pat=*	
20:45:29.836 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_release pat=*
20:45:29.837 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_release pat=*	
20:45:29.841 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_shop pat=*
20:45:29.841 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_shop pat=*	
20:45:29.850 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
20:45:29.850 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
20:45:29.855 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_shop pat=*
20:45:29.855 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_shop pat=*	
20:45:29.860 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dim pat=*
20:45:29.860 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dim pat=*	
20:45:29.866 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dm pat=*
20:45:29.866 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dm pat=*	
20:45:29.872 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dw pat=*
20:45:29.872 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dw pat=*	
20:45:29.876 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_rds pat=*
20:45:29.876 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_rds pat=*	
20:45:29.882 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=te pat=*
20:45:29.882 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=te pat=*	
20:45:29.886 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test pat=*
20:45:29.887 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test pat=*	
20:45:29.892 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test4 pat=*
20:45:29.893 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test4 pat=*	
20:45:32.758 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/c2fcd5ff-b1bb-4e8b-9d51-b8e49aa6c51f_resources
20:45:32.800 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/c2fcd5ff-b1bb-4e8b-9d51-b8e49aa6c51f
20:45:32.802 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/c2fcd5ff-b1bb-4e8b-9d51-b8e49aa6c51f
20:45:32.808 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/c2fcd5ff-b1bb-4e8b-9d51-b8e49aa6c51f/_tmp_space.db
20:45:32.812 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
20:45:32.845 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for root: Unknown error.
20:45:32.932 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:45:32.932 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:45:32.952 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
20:45:32.952 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
20:45:32.955 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
20:45:33.541 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/4c3aae0a-edbc-47c9-a3c7-86455738d4a1_resources
20:45:33.578 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/4c3aae0a-edbc-47c9-a3c7-86455738d4a1
20:45:33.581 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/4c3aae0a-edbc-47c9-a3c7-86455738d4a1
20:45:33.588 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/4c3aae0a-edbc-47c9-a3c7-86455738d4a1/_tmp_space.db
20:45:33.592 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
20:45:33.922 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
20:46:02.634 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
20:46:03.332 INFO  [main] org.apache.spark.SparkContext - Submitted application: mid_release_exposure_job
20:46:03.358 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: YongningY,root
20:46:03.359 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: YongningY,root
20:46:03.359 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
20:46:03.360 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
20:46:03.360 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(YongningY, root); groups with view permissions: Set(); users  with modify permissions: Set(YongningY, root); groups with modify permissions: Set()
20:46:04.089 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 8361.
20:46:04.107 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
20:46:04.126 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
20:46:04.132 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20:46:04.132 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
20:46:04.144 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\Yongning Y\AppData\Local\Temp\blockmgr-59abb468-4db7-4776-b392-806cb4fe65a8
20:46:04.167 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1445.7 MB
20:46:04.265 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
20:46:04.435 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @4811ms
20:46:04.546 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
20:46:04.566 INFO  [main] org.spark_project.jetty.server.Server - Started @4945ms
20:46:04.596 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@583ebbb4{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20:46:04.596 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
20:46:04.627 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60b85ba1{/jobs,null,AVAILABLE,@Spark}
20:46:04.628 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b718392{/jobs/json,null,AVAILABLE,@Spark}
20:46:04.629 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f2d2181{/jobs/job,null,AVAILABLE,@Spark}
20:46:04.630 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fcdcf{/jobs/job/json,null,AVAILABLE,@Spark}
20:46:04.631 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46292372{/stages,null,AVAILABLE,@Spark}
20:46:04.631 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c44052e{/stages/json,null,AVAILABLE,@Spark}
20:46:04.632 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@530a8454{/stages/stage,null,AVAILABLE,@Spark}
20:46:04.636 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31198ceb{/stages/stage/json,null,AVAILABLE,@Spark}
20:46:04.641 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75201592{/stages/pool,null,AVAILABLE,@Spark}
20:46:04.654 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@aa5455e{/stages/pool/json,null,AVAILABLE,@Spark}
20:46:04.656 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5dda14d0{/storage,null,AVAILABLE,@Spark}
20:46:04.657 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d9fc57a{/storage/json,null,AVAILABLE,@Spark}
20:46:04.659 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b4ef7{/storage/rdd,null,AVAILABLE,@Spark}
20:46:04.660 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5987e932{/storage/rdd/json,null,AVAILABLE,@Spark}
20:46:04.668 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bbbdd4b{/environment,null,AVAILABLE,@Spark}
20:46:04.670 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25230246{/environment/json,null,AVAILABLE,@Spark}
20:46:04.679 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a8b5227{/executors,null,AVAILABLE,@Spark}
20:46:04.683 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6979efad{/executors/json,null,AVAILABLE,@Spark}
20:46:04.684 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67318f{/executors/threadDump,null,AVAILABLE,@Spark}
20:46:04.685 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17f9344b{/executors/threadDump/json,null,AVAILABLE,@Spark}
20:46:04.723 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54e81b21{/static,null,AVAILABLE,@Spark}
20:46:04.724 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2237bada{/,null,AVAILABLE,@Spark}
20:46:04.725 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5710768a{/api,null,AVAILABLE,@Spark}
20:46:04.726 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bf61e67{/jobs/job/kill,null,AVAILABLE,@Spark}
20:46:04.727 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b273a59{/stages/stage/kill,null,AVAILABLE,@Spark}
20:46:04.730 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.145.1:4040
20:46:04.835 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
20:46:04.870 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 8382.
20:46:04.871 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.145.1:8382
20:46:04.873 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20:46:04.902 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.145.1, 8382, None)
20:46:04.907 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.145.1:8382 with 1445.7 MB RAM, BlockManagerId(driver, 192.168.145.1, 8382, None)
20:46:04.910 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.145.1, 8382, None)
20:46:04.911 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.145.1, 8382, None)
20:46:05.311 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f02251{/metrics/json,null,AVAILABLE,@Spark}
20:46:05.397 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/H:/IdeaProject/release_test/target/classes/hive-site.xml
20:46:05.446 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
20:46:05.448 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/user/hive/warehouse'.
20:46:05.462 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64138b0c{/SQL,null,AVAILABLE,@Spark}
20:46:05.462 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22d9c961{/SQL/json,null,AVAILABLE,@Spark}
20:46:05.463 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79ab34c1{/SQL/execution,null,AVAILABLE,@Spark}
20:46:05.463 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@281f23f2{/SQL/execution/json,null,AVAILABLE,@Spark}
20:46:05.465 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6093d508{/static/sql,null,AVAILABLE,@Spark}
20:46:06.468 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20:46:07.323 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20:46:07.351 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
20:46:08.600 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20:46:10.017 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
20:46:10.021 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
20:46:10.373 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
20:46:10.377 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
20:46:10.451 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
20:46:10.650 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
20:46:10.652 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
20:46:10.669 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=azkaban pat=*
20:46:10.670 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=azkaban pat=*	
20:46:10.737 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dim pat=*
20:46:10.737 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dim pat=*	
20:46:10.741 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dm pat=*
20:46:10.741 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dm pat=*	
20:46:10.745 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dwd pat=*
20:46:10.745 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dwd pat=*	
20:46:10.750 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dws pat=*
20:46:10.750 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dws pat=*	
20:46:10.754 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_ods pat=*
20:46:10.754 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_ods pat=*	
20:46:10.758 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
20:46:10.759 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
20:46:10.762 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_release pat=*
20:46:10.763 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_release pat=*	
20:46:10.767 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_shop pat=*
20:46:10.768 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_shop pat=*	
20:46:10.771 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
20:46:10.772 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
20:46:10.776 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_shop pat=*
20:46:10.776 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_shop pat=*	
20:46:10.780 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
20:46:10.780 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
20:46:10.784 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_shop pat=*
20:46:10.784 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_shop pat=*	
20:46:10.788 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_release pat=*
20:46:10.788 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_release pat=*	
20:46:10.791 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_shop pat=*
20:46:10.791 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_shop pat=*	
20:46:10.797 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
20:46:10.797 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
20:46:10.801 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_shop pat=*
20:46:10.801 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_shop pat=*	
20:46:10.805 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dim pat=*
20:46:10.806 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dim pat=*	
20:46:10.810 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dm pat=*
20:46:10.810 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dm pat=*	
20:46:10.814 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dw pat=*
20:46:10.815 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dw pat=*	
20:46:10.820 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_rds pat=*
20:46:10.820 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_rds pat=*	
20:46:10.824 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=te pat=*
20:46:10.824 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=te pat=*	
20:46:10.828 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test pat=*
20:46:10.828 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test pat=*	
20:46:10.832 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test4 pat=*
20:46:10.832 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test4 pat=*	
20:46:12.654 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/f16a4e1f-0b4b-410d-85c7-113cca689979_resources
20:46:12.711 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/f16a4e1f-0b4b-410d-85c7-113cca689979
20:46:12.716 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/f16a4e1f-0b4b-410d-85c7-113cca689979
20:46:12.727 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/f16a4e1f-0b4b-410d-85c7-113cca689979/_tmp_space.db
20:46:12.776 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
20:46:12.815 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for root: Unknown error.
20:46:12.832 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:46:12.832 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:46:12.845 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
20:46:12.845 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
20:46:12.849 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
20:46:13.251 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/20f9ceeb-6192-4dd5-a539-81a38289a4b4_resources
20:46:13.258 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/20f9ceeb-6192-4dd5-a539-81a38289a4b4
20:46:13.261 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/20f9ceeb-6192-4dd5-a539-81a38289a4b4
20:46:13.268 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/20f9ceeb-6192-4dd5-a539-81a38289a4b4/_tmp_space.db
20:46:13.271 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
20:46:13.305 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
20:46:15.674 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_exposure
20:46:16.387 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:46:16.388 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:46:16.411 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:46:16.412 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:46:16.765 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:46:16.765 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:46:16.797 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:46:16.810 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:46:16.810 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:46:16.811 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:46:16.811 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:46:16.812 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:46:16.813 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:46:16.813 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:46:16.877 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
20:46:17.716 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
20:46:17.730 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
20:46:17.838 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
20:46:17.839 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:46:17.839 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:46:17.845 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:46:17.845 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:46:17.876 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:46:17.876 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:46:17.910 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:46:17.911 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:46:17.911 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:46:17.911 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:46:17.911 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:46:17.911 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:46:17.912 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:46:17.912 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:46:17.912 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:46:17.912 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:46:17.969 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
20:46:17.970 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as goods_id
20:46:18.043 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: floor(datediff(from_unixtime(unix_timestamp(),'yyyy-MM-dd'),from_unixtime(unix_timestamp(substr(get_json_object(exts,'$.idcard'),7,8),'yyyyMMdd')))/365) as age
20:46:18.054 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: case subStr(get_json_object(exts,'$.idcard'),17,1)%2 when 1 then '男'  when 0 then '女' end as gender 
20:46:18.079 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
20:46:18.080 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
20:46:18.081 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
20:46:18.082 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
20:46:18.083 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
20:46:18.085 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_version
20:46:18.085 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') aid
20:46:18.097 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:46:18.097 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:46:18.103 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:46:18.103 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:46:18.109 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:46:18.109 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:46:18.114 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:46:18.115 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:46:18.121 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:46:18.121 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:46:18.127 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:46:18.127 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:46:18.131 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:46:18.131 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:46:18.138 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:46:18.139 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:46:18.143 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:46:18.144 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:46:18.151 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:46:18.151 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:46:18.155 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:46:18.155 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:46:18.160 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:46:18.160 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:46:18.165 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:46:18.165 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:46:18.171 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:46:18.171 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:46:18.188 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:46:18.188 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:46:18.199 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:46:18.199 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:46:18.209 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:46:18.210 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:46:18.216 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:46:18.217 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:46:18.223 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:46:18.223 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:46:18.230 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:46:18.230 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:46:18.760 ERROR [main] com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$ - USING column `release_session` cannot be resolved on the left side of the join. The left-side columns: [ct, bdp_day];
org.apache.spark.sql.AnalysisException: USING column `release_session` cannot be resolved on the left side of the join. The left-side columns: [ct, bdp_day];
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$87$$anonfun$apply$65.apply(Analyzer.scala:2246)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$87$$anonfun$apply$65.apply(Analyzer.scala:2246)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$87.apply(Analyzer.scala:2245)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$87.apply(Analyzer.scala:2244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$commonNaturalJoinProcessing(Analyzer.scala:2244)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin$$anonfun$apply$32.applyOrElse(Analyzer.scala:2230)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin$$anonfun$apply$32.applyOrElse(Analyzer.scala:2227)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:62)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:62)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:61)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin$.apply(Analyzer.scala:2227)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin$.apply(Analyzer.scala:2226)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)
	at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)
	at scala.collection.immutable.List.foldLeft(List.scala:84)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:69)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:67)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:50)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2888)
	at org.apache.spark.sql.Dataset.join(Dataset.scala:784)
	at org.apache.spark.sql.Dataset.join(Dataset.scala:753)
	at org.apache.spark.sql.Dataset.join(Dataset.scala:728)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$.handleReleaseJob(MIDReleaseExposure.scala:47)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$$anonfun$handleJobs$1.apply(MIDReleaseExposure.scala:86)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$$anonfun$handleJobs$1.apply(MIDReleaseExposure.scala:84)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$.handleJobs(MIDReleaseExposure.scala:84)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$.main(MIDReleaseExposure.scala:106)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure.main(MIDReleaseExposure.scala)
20:46:18.823 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@583ebbb4{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20:46:18.828 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.145.1:4040
20:46:18.914 INFO  [dispatcher-event-loop-1] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
20:46:18.953 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
20:46:18.953 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
20:46:18.965 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
20:46:18.990 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
20:46:18.999 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
20:46:19.036 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
20:46:19.038 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\Yongning Y\AppData\Local\Temp\spark-30f00902-4218-494e-a8ff-674fb477e65d
20:47:29.125 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
20:47:29.637 INFO  [main] org.apache.spark.SparkContext - Submitted application: mid_release_exposure_job
20:47:29.663 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: YongningY,root
20:47:29.664 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: YongningY,root
20:47:29.665 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
20:47:29.665 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
20:47:29.666 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(YongningY, root); groups with view permissions: Set(); users  with modify permissions: Set(YongningY, root); groups with modify permissions: Set()
20:47:30.377 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 8452.
20:47:30.396 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
20:47:30.413 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
20:47:30.417 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20:47:30.417 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
20:47:30.425 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\Yongning Y\AppData\Local\Temp\blockmgr-7bd39e1a-1c99-4b19-8fc0-9dc2a38a17f8
20:47:30.443 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1445.7 MB
20:47:30.490 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
20:47:30.572 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @4051ms
20:47:30.653 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
20:47:30.672 INFO  [main] org.spark_project.jetty.server.Server - Started @4153ms
20:47:30.702 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@6d1112cc{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20:47:30.703 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
20:47:30.730 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60b85ba1{/jobs,null,AVAILABLE,@Spark}
20:47:30.731 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b718392{/jobs/json,null,AVAILABLE,@Spark}
20:47:30.731 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f2d2181{/jobs/job,null,AVAILABLE,@Spark}
20:47:30.732 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fcdcf{/jobs/job/json,null,AVAILABLE,@Spark}
20:47:30.733 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46292372{/stages,null,AVAILABLE,@Spark}
20:47:30.734 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c44052e{/stages/json,null,AVAILABLE,@Spark}
20:47:30.735 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@530a8454{/stages/stage,null,AVAILABLE,@Spark}
20:47:30.737 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31198ceb{/stages/stage/json,null,AVAILABLE,@Spark}
20:47:30.737 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75201592{/stages/pool,null,AVAILABLE,@Spark}
20:47:30.738 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@aa5455e{/stages/pool/json,null,AVAILABLE,@Spark}
20:47:30.739 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5dda14d0{/storage,null,AVAILABLE,@Spark}
20:47:30.740 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d9fc57a{/storage/json,null,AVAILABLE,@Spark}
20:47:30.741 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b4ef7{/storage/rdd,null,AVAILABLE,@Spark}
20:47:30.742 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5987e932{/storage/rdd/json,null,AVAILABLE,@Spark}
20:47:30.742 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bbbdd4b{/environment,null,AVAILABLE,@Spark}
20:47:30.743 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25230246{/environment/json,null,AVAILABLE,@Spark}
20:47:30.745 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a8b5227{/executors,null,AVAILABLE,@Spark}
20:47:30.745 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6979efad{/executors/json,null,AVAILABLE,@Spark}
20:47:30.746 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67318f{/executors/threadDump,null,AVAILABLE,@Spark}
20:47:30.747 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17f9344b{/executors/threadDump/json,null,AVAILABLE,@Spark}
20:47:30.762 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54e81b21{/static,null,AVAILABLE,@Spark}
20:47:30.764 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2237bada{/,null,AVAILABLE,@Spark}
20:47:30.765 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5710768a{/api,null,AVAILABLE,@Spark}
20:47:30.766 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bf61e67{/jobs/job/kill,null,AVAILABLE,@Spark}
20:47:30.767 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b273a59{/stages/stage/kill,null,AVAILABLE,@Spark}
20:47:30.770 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.145.1:4040
20:47:30.863 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
20:47:30.908 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 8473.
20:47:30.909 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.145.1:8473
20:47:30.911 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20:47:30.953 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.145.1, 8473, None)
20:47:30.957 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.145.1:8473 with 1445.7 MB RAM, BlockManagerId(driver, 192.168.145.1, 8473, None)
20:47:30.959 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.145.1, 8473, None)
20:47:30.960 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.145.1, 8473, None)
20:47:31.195 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f02251{/metrics/json,null,AVAILABLE,@Spark}
20:47:31.243 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/H:/IdeaProject/release_test/target/classes/hive-site.xml
20:47:31.278 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
20:47:31.279 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/user/hive/warehouse'.
20:47:31.286 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@bdc8014{/SQL,null,AVAILABLE,@Spark}
20:47:31.287 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73ba6fe6{/SQL/json,null,AVAILABLE,@Spark}
20:47:31.288 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28d79cba{/SQL/execution,null,AVAILABLE,@Spark}
20:47:31.289 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29f0c4f2{/SQL/execution/json,null,AVAILABLE,@Spark}
20:47:31.291 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bfe3203{/static/sql,null,AVAILABLE,@Spark}
20:47:32.127 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20:47:32.939 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20:47:32.971 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
20:47:34.065 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20:47:35.449 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
20:47:35.451 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
20:47:35.697 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
20:47:35.701 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
20:47:35.750 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
20:47:35.861 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
20:47:35.863 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
20:47:35.877 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=azkaban pat=*
20:47:35.877 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=azkaban pat=*	
20:47:35.933 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dim pat=*
20:47:35.933 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dim pat=*	
20:47:35.937 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dm pat=*
20:47:35.937 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dm pat=*	
20:47:35.940 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dwd pat=*
20:47:35.940 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dwd pat=*	
20:47:35.944 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dws pat=*
20:47:35.944 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dws pat=*	
20:47:35.948 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_ods pat=*
20:47:35.948 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_ods pat=*	
20:47:35.953 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
20:47:35.953 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
20:47:35.958 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_release pat=*
20:47:35.959 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_release pat=*	
20:47:35.963 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_shop pat=*
20:47:35.963 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_shop pat=*	
20:47:35.966 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
20:47:35.967 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
20:47:35.970 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_shop pat=*
20:47:35.971 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_shop pat=*	
20:47:35.974 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
20:47:35.975 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
20:47:35.978 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_shop pat=*
20:47:35.978 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_shop pat=*	
20:47:35.981 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_release pat=*
20:47:35.981 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_release pat=*	
20:47:35.985 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_shop pat=*
20:47:35.985 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_shop pat=*	
20:47:35.989 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
20:47:35.989 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
20:47:35.993 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_shop pat=*
20:47:35.994 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_shop pat=*	
20:47:35.997 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dim pat=*
20:47:35.998 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dim pat=*	
20:47:36.001 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dm pat=*
20:47:36.001 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dm pat=*	
20:47:36.005 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dw pat=*
20:47:36.005 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dw pat=*	
20:47:36.008 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_rds pat=*
20:47:36.008 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_rds pat=*	
20:47:36.011 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=te pat=*
20:47:36.011 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=te pat=*	
20:47:36.014 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test pat=*
20:47:36.014 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test pat=*	
20:47:36.018 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test4 pat=*
20:47:36.018 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test4 pat=*	
20:47:37.742 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/6768cb0f-e927-4c1b-91a4-cf6a7d42207e_resources
20:47:37.755 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/6768cb0f-e927-4c1b-91a4-cf6a7d42207e
20:47:37.757 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/6768cb0f-e927-4c1b-91a4-cf6a7d42207e
20:47:37.791 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/6768cb0f-e927-4c1b-91a4-cf6a7d42207e/_tmp_space.db
20:47:37.796 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
20:47:37.808 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for root: Unknown error.
20:47:37.820 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:47:37.820 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:47:37.827 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
20:47:37.827 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
20:47:37.831 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
20:47:38.034 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/fed030c1-706d-44e1-adfa-8b429ad0d817_resources
20:47:38.044 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/fed030c1-706d-44e1-adfa-8b429ad0d817
20:47:38.046 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/fed030c1-706d-44e1-adfa-8b429ad0d817
20:47:38.057 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/fed030c1-706d-44e1-adfa-8b429ad0d817/_tmp_space.db
20:47:38.060 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
20:47:38.096 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
20:47:39.617 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_exposure
20:47:39.802 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:47:39.803 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:47:39.809 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:47:39.809 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:47:39.911 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:47:39.911 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:47:39.941 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:47:39.954 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:47:39.954 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:47:39.955 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:47:39.955 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:47:39.955 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:47:39.955 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:47:39.955 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:47:39.969 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
20:47:40.293 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
20:47:40.306 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
20:47:40.375 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
20:47:40.376 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:47:40.376 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:47:40.385 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:47:40.385 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:47:40.414 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:47:40.414 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:47:40.446 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:47:40.446 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:47:40.446 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:47:40.447 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:47:40.447 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:47:40.447 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:47:40.447 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:47:40.448 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:47:40.448 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:47:40.448 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:47:40.467 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
20:47:40.468 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as goods_id
20:47:40.487 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: floor(datediff(from_unixtime(unix_timestamp(),'yyyy-MM-dd'),from_unixtime(unix_timestamp(substr(get_json_object(exts,'$.idcard'),7,8),'yyyyMMdd')))/365) as age
20:47:40.504 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: case subStr(get_json_object(exts,'$.idcard'),17,1)%2 when 1 then '男'  when 0 then '女' end as gender 
20:47:40.528 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
20:47:40.529 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
20:47:40.533 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
20:47:40.538 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
20:47:40.539 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
20:47:40.541 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_version
20:47:40.547 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') aid
20:47:40.558 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:47:40.558 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:47:40.567 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:47:40.568 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:47:40.575 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:47:40.576 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:47:40.585 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:47:40.587 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:47:40.598 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:47:40.598 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:47:40.605 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:47:40.605 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:47:40.611 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:47:40.611 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:47:40.617 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:47:40.617 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:47:40.622 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:47:40.622 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:47:40.628 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:47:40.628 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:47:40.633 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:47:40.634 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:47:40.639 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:47:40.640 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:47:40.644 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:47:40.644 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:47:40.649 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:47:40.649 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:47:40.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:47:40.654 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:47:40.658 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:47:40.658 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:47:40.662 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:47:40.662 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:47:40.667 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:47:40.667 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:47:40.671 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:47:40.672 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:47:40.677 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:47:40.677 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:47:40.929 ERROR [main] com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$ - Unsupported join type 'left join'. Supported join types include: 'inner', 'outer', 'full', 'fullouter', 'leftouter', 'left', 'rightouter', 'right', 'leftsemi', 'leftanti', 'cross'.
java.lang.IllegalArgumentException: Unsupported join type 'left join'. Supported join types include: 'inner', 'outer', 'full', 'fullouter', 'leftouter', 'left', 'rightouter', 'right', 'leftsemi', 'leftanti', 'cross'.
	at org.apache.spark.sql.catalyst.plans.JoinType$.apply(joinTypes.scala:43)
	at org.apache.spark.sql.Dataset.join(Dataset.scala:781)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$.handleReleaseJob(MIDReleaseExposure.scala:47)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$$anonfun$handleJobs$1.apply(MIDReleaseExposure.scala:86)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$$anonfun$handleJobs$1.apply(MIDReleaseExposure.scala:84)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$.handleJobs(MIDReleaseExposure.scala:84)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$.main(MIDReleaseExposure.scala:106)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure.main(MIDReleaseExposure.scala)
20:47:40.935 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@6d1112cc{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20:47:40.937 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.145.1:4040
20:47:40.948 INFO  [dispatcher-event-loop-3] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
20:47:40.956 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
20:47:40.956 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
20:47:40.962 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
20:47:40.966 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
20:47:40.968 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
20:47:40.972 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
20:47:40.973 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\Yongning Y\AppData\Local\Temp\spark-cfea48e6-aea5-488f-b357-320a871e9347
20:48:12.427 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
20:48:13.164 INFO  [main] org.apache.spark.SparkContext - Submitted application: mid_release_exposure_job
20:48:13.201 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: YongningY,root
20:48:13.207 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: YongningY,root
20:48:13.208 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
20:48:13.209 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
20:48:13.210 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(YongningY, root); groups with view permissions: Set(); users  with modify permissions: Set(YongningY, root); groups with modify permissions: Set()
20:48:13.830 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 8508.
20:48:13.847 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
20:48:13.863 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
20:48:13.866 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20:48:13.867 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
20:48:13.875 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\Yongning Y\AppData\Local\Temp\blockmgr-d83f19a3-a076-43d8-9c5c-acc6f62201ff
20:48:13.893 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1445.7 MB
20:48:13.944 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
20:48:14.052 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @4116ms
20:48:14.130 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
20:48:14.153 INFO  [main] org.spark_project.jetty.server.Server - Started @4219ms
20:48:14.183 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@3fc08eec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20:48:14.184 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
20:48:14.210 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@492fc69e{/jobs,null,AVAILABLE,@Spark}
20:48:14.210 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d2260db{/jobs/json,null,AVAILABLE,@Spark}
20:48:14.211 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49bf29c6{/jobs/job,null,AVAILABLE,@Spark}
20:48:14.212 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7668d560{/jobs/job/json,null,AVAILABLE,@Spark}
20:48:14.213 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@126be319{/stages,null,AVAILABLE,@Spark}
20:48:14.214 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c371e13{/stages/json,null,AVAILABLE,@Spark}
20:48:14.214 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e34c607{/stages/stage,null,AVAILABLE,@Spark}
20:48:14.216 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9257031{/stages/stage/json,null,AVAILABLE,@Spark}
20:48:14.216 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7726e185{/stages/pool,null,AVAILABLE,@Spark}
20:48:14.217 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@282308c3{/stages/pool/json,null,AVAILABLE,@Spark}
20:48:14.218 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1db0ec27{/storage,null,AVAILABLE,@Spark}
20:48:14.219 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d4ab71a{/storage/json,null,AVAILABLE,@Spark}
20:48:14.221 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1af05b03{/storage/rdd,null,AVAILABLE,@Spark}
20:48:14.221 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ad777f{/storage/rdd/json,null,AVAILABLE,@Spark}
20:48:14.222 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@438bad7c{/environment,null,AVAILABLE,@Spark}
20:48:14.224 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4fdf8f12{/environment/json,null,AVAILABLE,@Spark}
20:48:14.225 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54f5f647{/executors,null,AVAILABLE,@Spark}
20:48:14.226 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a6d5a8f{/executors/json,null,AVAILABLE,@Spark}
20:48:14.227 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@315ba14a{/executors/threadDump,null,AVAILABLE,@Spark}
20:48:14.228 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27f0ad19{/executors/threadDump/json,null,AVAILABLE,@Spark}
20:48:14.243 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38d5b107{/static,null,AVAILABLE,@Spark}
20:48:14.245 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77e2a6e2{/,null,AVAILABLE,@Spark}
20:48:14.246 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@199e4c2b{/api,null,AVAILABLE,@Spark}
20:48:14.247 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/job/kill,null,AVAILABLE,@Spark}
20:48:14.248 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e7095ac{/stages/stage/kill,null,AVAILABLE,@Spark}
20:48:14.252 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.145.1:4040
20:48:14.386 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
20:48:14.415 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 8529.
20:48:14.416 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.145.1:8529
20:48:14.417 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20:48:14.442 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.145.1, 8529, None)
20:48:14.445 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.145.1:8529 with 1445.7 MB RAM, BlockManagerId(driver, 192.168.145.1, 8529, None)
20:48:14.448 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.145.1, 8529, None)
20:48:14.449 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.145.1, 8529, None)
20:48:14.665 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@dffa30b{/metrics/json,null,AVAILABLE,@Spark}
20:48:14.721 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/H:/IdeaProject/release_test/target/classes/hive-site.xml
20:48:14.757 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
20:48:14.758 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/user/hive/warehouse'.
20:48:14.766 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49dbaaf3{/SQL,null,AVAILABLE,@Spark}
20:48:14.767 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@736f3e9e{/SQL/json,null,AVAILABLE,@Spark}
20:48:14.768 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1698d7c0{/SQL/execution,null,AVAILABLE,@Spark}
20:48:14.769 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@87abc48{/SQL/execution/json,null,AVAILABLE,@Spark}
20:48:14.771 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13047d7d{/static/sql,null,AVAILABLE,@Spark}
20:48:15.575 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20:48:16.333 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20:48:16.364 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
20:48:17.388 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20:48:18.709 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
20:48:18.711 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
20:48:18.968 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
20:48:18.972 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
20:48:19.020 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
20:48:19.127 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
20:48:19.130 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
20:48:19.144 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=azkaban pat=*
20:48:19.144 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=azkaban pat=*	
20:48:19.202 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dim pat=*
20:48:19.202 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dim pat=*	
20:48:19.207 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dm pat=*
20:48:19.207 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dm pat=*	
20:48:19.211 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dwd pat=*
20:48:19.212 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dwd pat=*	
20:48:19.218 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dws pat=*
20:48:19.218 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dws pat=*	
20:48:19.224 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_ods pat=*
20:48:19.224 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_ods pat=*	
20:48:19.229 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
20:48:19.229 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
20:48:19.234 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_release pat=*
20:48:19.234 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_release pat=*	
20:48:19.238 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_shop pat=*
20:48:19.238 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_shop pat=*	
20:48:19.243 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
20:48:19.244 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
20:48:19.248 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_shop pat=*
20:48:19.249 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_shop pat=*	
20:48:19.252 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
20:48:19.252 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
20:48:19.256 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_shop pat=*
20:48:19.256 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_shop pat=*	
20:48:19.260 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_release pat=*
20:48:19.260 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_release pat=*	
20:48:19.263 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_shop pat=*
20:48:19.263 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_shop pat=*	
20:48:19.267 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
20:48:19.268 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
20:48:19.271 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_shop pat=*
20:48:19.271 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_shop pat=*	
20:48:19.275 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dim pat=*
20:48:19.275 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dim pat=*	
20:48:19.277 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dm pat=*
20:48:19.278 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dm pat=*	
20:48:19.281 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dw pat=*
20:48:19.281 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dw pat=*	
20:48:19.283 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_rds pat=*
20:48:19.283 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_rds pat=*	
20:48:19.286 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=te pat=*
20:48:19.286 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=te pat=*	
20:48:19.289 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test pat=*
20:48:19.289 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test pat=*	
20:48:19.293 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test4 pat=*
20:48:19.293 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test4 pat=*	
20:48:21.044 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/0b50bf4c-07bf-4c47-889e-0a9a8d6445aa_resources
20:48:21.058 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/0b50bf4c-07bf-4c47-889e-0a9a8d6445aa
20:48:21.060 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/0b50bf4c-07bf-4c47-889e-0a9a8d6445aa
20:48:21.064 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/0b50bf4c-07bf-4c47-889e-0a9a8d6445aa/_tmp_space.db
20:48:21.068 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
20:48:21.079 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for root: Unknown error.
20:48:21.091 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:48:21.091 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:48:21.097 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
20:48:21.097 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
20:48:21.100 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
20:48:21.328 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/8e3360e0-73b4-402d-bcbb-ade977ed9bd8_resources
20:48:21.335 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/8e3360e0-73b4-402d-bcbb-ade977ed9bd8
20:48:21.340 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/8e3360e0-73b4-402d-bcbb-ade977ed9bd8
20:48:21.349 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/8e3360e0-73b4-402d-bcbb-ade977ed9bd8/_tmp_space.db
20:48:21.351 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
20:48:21.386 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
20:48:22.851 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_exposure
20:48:23.062 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:48:23.062 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:48:23.069 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:48:23.069 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:48:23.165 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:48:23.165 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:48:23.196 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:48:23.209 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:48:23.209 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:48:23.209 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:48:23.210 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:48:23.210 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:48:23.210 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:48:23.210 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:48:23.224 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
20:48:23.548 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
20:48:23.564 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
20:48:23.636 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
20:48:23.637 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:48:23.637 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:48:23.644 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:48:23.645 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:48:23.677 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:48:23.677 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:48:23.712 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:48:23.712 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:48:23.712 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:48:23.712 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:48:23.713 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:48:23.713 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:48:23.713 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:48:23.714 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:48:23.714 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:48:23.714 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:48:23.746 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
20:48:23.746 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as goods_id
20:48:23.771 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: floor(datediff(from_unixtime(unix_timestamp(),'yyyy-MM-dd'),from_unixtime(unix_timestamp(substr(get_json_object(exts,'$.idcard'),7,8),'yyyyMMdd')))/365) as age
20:48:23.786 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: case subStr(get_json_object(exts,'$.idcard'),17,1)%2 when 1 then '男'  when 0 then '女' end as gender 
20:48:23.808 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
20:48:23.809 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
20:48:23.814 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
20:48:23.815 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
20:48:23.816 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
20:48:23.817 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_version
20:48:23.818 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') aid
20:48:23.827 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:48:23.828 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:48:23.837 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:48:23.837 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:48:23.847 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:48:23.847 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:48:23.854 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:48:23.854 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:48:23.861 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:48:23.861 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:48:23.865 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:48:23.865 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:48:23.870 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:48:23.870 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:48:23.875 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:48:23.876 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:48:23.880 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:48:23.880 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:48:23.885 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:48:23.885 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:48:23.890 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:48:23.890 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:48:23.895 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:48:23.896 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:48:23.899 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:48:23.899 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:48:23.904 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:48:23.904 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:48:23.908 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:48:23.908 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:48:23.912 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:48:23.913 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:48:23.916 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:48:23.916 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:48:23.921 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:48:23.922 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:48:23.925 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:48:23.926 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:48:23.930 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:48:23.930 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:48:24.239 ERROR [main] com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$ - USING column `release_session` cannot be resolved on the left side of the join. The left-side columns: [ct, bdp_day];
org.apache.spark.sql.AnalysisException: USING column `release_session` cannot be resolved on the left side of the join. The left-side columns: [ct, bdp_day];
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$87$$anonfun$apply$65.apply(Analyzer.scala:2246)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$87$$anonfun$apply$65.apply(Analyzer.scala:2246)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$87.apply(Analyzer.scala:2245)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$87.apply(Analyzer.scala:2244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$commonNaturalJoinProcessing(Analyzer.scala:2244)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin$$anonfun$apply$32.applyOrElse(Analyzer.scala:2230)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin$$anonfun$apply$32.applyOrElse(Analyzer.scala:2227)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:62)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:62)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:61)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin$.apply(Analyzer.scala:2227)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin$.apply(Analyzer.scala:2226)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)
	at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)
	at scala.collection.immutable.List.foldLeft(List.scala:84)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:69)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:67)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:50)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2888)
	at org.apache.spark.sql.Dataset.join(Dataset.scala:784)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$.handleReleaseJob(MIDReleaseExposure.scala:47)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$$anonfun$handleJobs$1.apply(MIDReleaseExposure.scala:86)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$$anonfun$handleJobs$1.apply(MIDReleaseExposure.scala:84)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$.handleJobs(MIDReleaseExposure.scala:84)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$.main(MIDReleaseExposure.scala:106)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure.main(MIDReleaseExposure.scala)
20:48:24.251 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@3fc08eec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20:48:24.253 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.145.1:4040
20:48:24.265 INFO  [dispatcher-event-loop-3] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
20:48:24.273 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
20:48:24.273 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
20:48:24.280 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
20:48:24.283 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
20:48:24.285 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
20:48:24.288 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
20:48:24.289 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\Yongning Y\AppData\Local\Temp\spark-892b4529-77c9-4f9a-be11-4477ed79eeda
20:53:04.207 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
20:53:04.863 INFO  [main] org.apache.spark.SparkContext - Submitted application: mid_release_exposure_job
20:53:04.890 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: YongningY,root
20:53:04.891 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: YongningY,root
20:53:04.892 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
20:53:04.893 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
20:53:04.893 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(YongningY, root); groups with view permissions: Set(); users  with modify permissions: Set(YongningY, root); groups with modify permissions: Set()
20:53:05.580 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 8569.
20:53:05.599 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
20:53:05.616 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
20:53:05.619 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20:53:05.620 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
20:53:05.628 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\Yongning Y\AppData\Local\Temp\blockmgr-cf975eb1-66f0-4057-8b57-9319bc8939ea
20:53:05.647 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1445.7 MB
20:53:05.702 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
20:53:05.810 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @4488ms
20:53:05.881 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
20:53:05.896 INFO  [main] org.spark_project.jetty.server.Server - Started @4577ms
20:53:05.919 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@244caa6e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20:53:05.919 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
20:53:05.940 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60b85ba1{/jobs,null,AVAILABLE,@Spark}
20:53:05.941 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b718392{/jobs/json,null,AVAILABLE,@Spark}
20:53:05.942 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f2d2181{/jobs/job,null,AVAILABLE,@Spark}
20:53:05.942 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fcdcf{/jobs/job/json,null,AVAILABLE,@Spark}
20:53:05.943 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46292372{/stages,null,AVAILABLE,@Spark}
20:53:05.949 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c44052e{/stages/json,null,AVAILABLE,@Spark}
20:53:05.950 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@530a8454{/stages/stage,null,AVAILABLE,@Spark}
20:53:05.952 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31198ceb{/stages/stage/json,null,AVAILABLE,@Spark}
20:53:05.953 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75201592{/stages/pool,null,AVAILABLE,@Spark}
20:53:05.954 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@aa5455e{/stages/pool/json,null,AVAILABLE,@Spark}
20:53:05.955 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5dda14d0{/storage,null,AVAILABLE,@Spark}
20:53:05.956 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d9fc57a{/storage/json,null,AVAILABLE,@Spark}
20:53:05.959 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b4ef7{/storage/rdd,null,AVAILABLE,@Spark}
20:53:05.960 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5987e932{/storage/rdd/json,null,AVAILABLE,@Spark}
20:53:05.961 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bbbdd4b{/environment,null,AVAILABLE,@Spark}
20:53:05.962 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25230246{/environment/json,null,AVAILABLE,@Spark}
20:53:05.963 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a8b5227{/executors,null,AVAILABLE,@Spark}
20:53:05.964 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6979efad{/executors/json,null,AVAILABLE,@Spark}
20:53:05.965 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67318f{/executors/threadDump,null,AVAILABLE,@Spark}
20:53:05.966 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17f9344b{/executors/threadDump/json,null,AVAILABLE,@Spark}
20:53:05.975 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54e81b21{/static,null,AVAILABLE,@Spark}
20:53:05.976 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2237bada{/,null,AVAILABLE,@Spark}
20:53:05.978 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5710768a{/api,null,AVAILABLE,@Spark}
20:53:05.979 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bf61e67{/jobs/job/kill,null,AVAILABLE,@Spark}
20:53:05.979 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b273a59{/stages/stage/kill,null,AVAILABLE,@Spark}
20:53:05.982 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.145.1:4040
20:53:06.075 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
20:53:06.106 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 8590.
20:53:06.107 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.145.1:8590
20:53:06.108 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20:53:06.139 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.145.1, 8590, None)
20:53:06.142 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.145.1:8590 with 1445.7 MB RAM, BlockManagerId(driver, 192.168.145.1, 8590, None)
20:53:06.144 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.145.1, 8590, None)
20:53:06.145 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.145.1, 8590, None)
20:53:06.413 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f02251{/metrics/json,null,AVAILABLE,@Spark}
20:53:06.462 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/H:/IdeaProject/release_test/target/classes/hive-site.xml
20:53:06.496 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
20:53:06.496 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/user/hive/warehouse'.
20:53:06.502 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@bdc8014{/SQL,null,AVAILABLE,@Spark}
20:53:06.503 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73ba6fe6{/SQL/json,null,AVAILABLE,@Spark}
20:53:06.504 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28d79cba{/SQL/execution,null,AVAILABLE,@Spark}
20:53:06.504 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29f0c4f2{/SQL/execution/json,null,AVAILABLE,@Spark}
20:53:06.506 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bfe3203{/static/sql,null,AVAILABLE,@Spark}
20:53:07.340 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20:53:08.127 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20:53:08.159 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
20:53:09.281 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20:53:10.763 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
20:53:10.765 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
20:53:11.106 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
20:53:11.111 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
20:53:11.184 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
20:53:11.335 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
20:53:11.338 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
20:53:11.352 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=azkaban pat=*
20:53:11.352 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=azkaban pat=*	
20:53:11.429 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dim pat=*
20:53:11.429 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dim pat=*	
20:53:11.432 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dm pat=*
20:53:11.433 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dm pat=*	
20:53:11.436 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dwd pat=*
20:53:11.436 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dwd pat=*	
20:53:11.440 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dws pat=*
20:53:11.440 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dws pat=*	
20:53:11.443 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_ods pat=*
20:53:11.444 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_ods pat=*	
20:53:11.446 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
20:53:11.447 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
20:53:11.450 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_release pat=*
20:53:11.450 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_release pat=*	
20:53:11.453 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_shop pat=*
20:53:11.453 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_shop pat=*	
20:53:11.456 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
20:53:11.456 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
20:53:11.459 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_shop pat=*
20:53:11.459 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_shop pat=*	
20:53:11.462 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
20:53:11.462 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
20:53:11.465 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_shop pat=*
20:53:11.466 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_shop pat=*	
20:53:11.469 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_release pat=*
20:53:11.470 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_release pat=*	
20:53:11.473 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_shop pat=*
20:53:11.473 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_shop pat=*	
20:53:11.478 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
20:53:11.478 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
20:53:11.481 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_shop pat=*
20:53:11.481 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_shop pat=*	
20:53:11.485 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dim pat=*
20:53:11.485 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dim pat=*	
20:53:11.488 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dm pat=*
20:53:11.488 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dm pat=*	
20:53:11.492 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dw pat=*
20:53:11.492 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dw pat=*	
20:53:11.495 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_rds pat=*
20:53:11.495 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_rds pat=*	
20:53:11.498 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=te pat=*
20:53:11.498 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=te pat=*	
20:53:11.501 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test pat=*
20:53:11.501 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test pat=*	
20:53:11.505 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test4 pat=*
20:53:11.506 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test4 pat=*	
20:53:13.255 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/0493d91b-cf8e-4f12-a96e-c53d1f229f7f_resources
20:53:13.300 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/0493d91b-cf8e-4f12-a96e-c53d1f229f7f
20:53:13.303 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/0493d91b-cf8e-4f12-a96e-c53d1f229f7f
20:53:13.308 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/0493d91b-cf8e-4f12-a96e-c53d1f229f7f/_tmp_space.db
20:53:13.311 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
20:53:13.321 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for root: Unknown error.
20:53:13.332 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:53:13.333 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:53:13.340 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
20:53:13.340 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
20:53:13.343 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
20:53:13.563 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/0d92db61-58e1-4b87-a981-e1758ceedae4_resources
20:53:13.575 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/0d92db61-58e1-4b87-a981-e1758ceedae4
20:53:13.579 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/0d92db61-58e1-4b87-a981-e1758ceedae4
20:53:13.588 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/0d92db61-58e1-4b87-a981-e1758ceedae4/_tmp_space.db
20:53:13.591 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
20:53:13.632 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
20:53:15.325 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_exposure
20:53:15.533 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:53:15.533 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:53:15.540 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:53:15.540 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:53:15.633 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:53:15.633 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:53:15.661 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:53:15.674 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:53:15.674 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:53:15.675 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:53:15.675 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:53:15.676 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:53:15.676 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:53:15.676 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:53:15.689 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
20:53:16.003 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: idcard
20:53:16.015 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: age
20:53:16.016 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: gender
20:53:16.017 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: area_code
20:53:16.017 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: longitude
20:53:16.018 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: latitude
20:53:16.018 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: matter_id
20:53:16.019 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: model_code
20:53:16.020 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: model_version
20:53:16.021 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: aid
20:53:16.021 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
20:53:16.022 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
20:53:16.117 ERROR [main] com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$ - cannot resolve '`idcard`' given input columns: [ct, bdp_day, device_num, device_type, sources, channels, release_status, release_session]; line 1 pos 0;
'Project ['idcard, 'age, 'gender, 'area_code, 'longitude, 'latitude, 'matter_id, 'model_code, 'model_version, 'aid, ct#6L, bdp_day#7]
+- SubqueryAlias dw_release_exposure
   +- Relation[release_session#0,release_status#1,device_num#2,device_type#3,sources#4,channels#5,ct#6L,bdp_day#7] parquet

org.apache.spark.sql.AnalysisException: cannot resolve '`idcard`' given input columns: [ct, bdp_day, device_num, device_type, sources, channels, release_status, release_session]; line 1 pos 0;
'Project ['idcard, 'age, 'gender, 'area_code, 'longitude, 'latitude, 'matter_id, 'model_code, 'model_version, 'aid, ct#6L, bdp_day#7]
+- SubqueryAlias dw_release_exposure
   +- Relation[release_session#0,release_status#1,device_num#2,device_type#3,sources#4,channels#5,ct#6L,bdp_day#7] parquet

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2888)
	at org.apache.spark.sql.Dataset.select(Dataset.scala:1154)
	at org.apache.spark.sql.Dataset.selectExpr(Dataset.scala:1189)
	at com.qf.bigdata.release.util.SparkHelper$.readTableData(SparkHelper.scala:26)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$.handleReleaseJob(MIDReleaseExposure.scala:40)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$$anonfun$handleJobs$1.apply(MIDReleaseExposure.scala:86)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$$anonfun$handleJobs$1.apply(MIDReleaseExposure.scala:84)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$.handleJobs(MIDReleaseExposure.scala:84)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$.main(MIDReleaseExposure.scala:106)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure.main(MIDReleaseExposure.scala)
20:53:16.124 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@244caa6e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20:53:16.127 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.145.1:4040
20:53:16.138 INFO  [dispatcher-event-loop-3] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
20:53:16.145 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
20:53:16.146 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
20:53:16.152 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
20:53:16.156 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
20:53:16.161 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
20:53:16.165 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
20:53:16.166 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\Yongning Y\AppData\Local\Temp\spark-388410eb-4100-4983-b339-83b1893d6911
20:53:57.623 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
20:53:58.399 INFO  [main] org.apache.spark.SparkContext - Submitted application: mid_release_exposure_job
20:53:58.437 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: YongningY,root
20:53:58.438 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: YongningY,root
20:53:58.439 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
20:53:58.440 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
20:53:58.440 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(YongningY, root); groups with view permissions: Set(); users  with modify permissions: Set(YongningY, root); groups with modify permissions: Set()
20:53:59.135 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 8629.
20:53:59.152 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
20:53:59.169 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
20:53:59.172 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20:53:59.172 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
20:53:59.181 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\Yongning Y\AppData\Local\Temp\blockmgr-68ec40c9-76c9-4e13-9516-347b6cb799e8
20:53:59.200 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1445.7 MB
20:53:59.252 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
20:53:59.359 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @4667ms
20:53:59.473 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
20:53:59.492 INFO  [main] org.spark_project.jetty.server.Server - Started @4802ms
20:53:59.521 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@47a2abb8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20:53:59.521 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
20:53:59.550 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60b85ba1{/jobs,null,AVAILABLE,@Spark}
20:53:59.551 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b718392{/jobs/json,null,AVAILABLE,@Spark}
20:53:59.552 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f2d2181{/jobs/job,null,AVAILABLE,@Spark}
20:53:59.553 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fcdcf{/jobs/job/json,null,AVAILABLE,@Spark}
20:53:59.554 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46292372{/stages,null,AVAILABLE,@Spark}
20:53:59.555 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c44052e{/stages/json,null,AVAILABLE,@Spark}
20:53:59.556 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@530a8454{/stages/stage,null,AVAILABLE,@Spark}
20:53:59.559 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31198ceb{/stages/stage/json,null,AVAILABLE,@Spark}
20:53:59.560 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75201592{/stages/pool,null,AVAILABLE,@Spark}
20:53:59.561 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@aa5455e{/stages/pool/json,null,AVAILABLE,@Spark}
20:53:59.577 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5dda14d0{/storage,null,AVAILABLE,@Spark}
20:53:59.579 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d9fc57a{/storage/json,null,AVAILABLE,@Spark}
20:53:59.580 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b4ef7{/storage/rdd,null,AVAILABLE,@Spark}
20:53:59.581 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5987e932{/storage/rdd/json,null,AVAILABLE,@Spark}
20:53:59.582 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bbbdd4b{/environment,null,AVAILABLE,@Spark}
20:53:59.583 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25230246{/environment/json,null,AVAILABLE,@Spark}
20:53:59.585 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a8b5227{/executors,null,AVAILABLE,@Spark}
20:53:59.589 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6979efad{/executors/json,null,AVAILABLE,@Spark}
20:53:59.589 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67318f{/executors/threadDump,null,AVAILABLE,@Spark}
20:53:59.590 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17f9344b{/executors/threadDump/json,null,AVAILABLE,@Spark}
20:53:59.613 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54e81b21{/static,null,AVAILABLE,@Spark}
20:53:59.615 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2237bada{/,null,AVAILABLE,@Spark}
20:53:59.617 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5710768a{/api,null,AVAILABLE,@Spark}
20:53:59.618 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bf61e67{/jobs/job/kill,null,AVAILABLE,@Spark}
20:53:59.620 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b273a59{/stages/stage/kill,null,AVAILABLE,@Spark}
20:53:59.624 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.145.1:4040
20:53:59.737 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
20:53:59.785 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 8652.
20:53:59.786 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.145.1:8652
20:53:59.789 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20:53:59.856 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.145.1, 8652, None)
20:53:59.862 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.145.1:8652 with 1445.7 MB RAM, BlockManagerId(driver, 192.168.145.1, 8652, None)
20:53:59.869 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.145.1, 8652, None)
20:53:59.870 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.145.1, 8652, None)
20:54:00.200 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f02251{/metrics/json,null,AVAILABLE,@Spark}
20:54:00.268 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/H:/IdeaProject/release_test/target/classes/hive-site.xml
20:54:00.314 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
20:54:00.315 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/user/hive/warehouse'.
20:54:00.323 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@bdc8014{/SQL,null,AVAILABLE,@Spark}
20:54:00.324 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73ba6fe6{/SQL/json,null,AVAILABLE,@Spark}
20:54:00.325 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28d79cba{/SQL/execution,null,AVAILABLE,@Spark}
20:54:00.326 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29f0c4f2{/SQL/execution/json,null,AVAILABLE,@Spark}
20:54:00.328 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1bfe3203{/static/sql,null,AVAILABLE,@Spark}
20:54:01.350 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20:54:02.100 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20:54:02.129 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
20:54:03.241 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20:54:04.794 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
20:54:04.796 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
20:54:05.065 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
20:54:05.069 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
20:54:05.119 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
20:54:05.232 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
20:54:05.234 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
20:54:05.248 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=azkaban pat=*
20:54:05.248 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=azkaban pat=*	
20:54:05.305 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dim pat=*
20:54:05.306 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dim pat=*	
20:54:05.310 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dm pat=*
20:54:05.310 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dm pat=*	
20:54:05.313 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dwd pat=*
20:54:05.314 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dwd pat=*	
20:54:05.317 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dws pat=*
20:54:05.318 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dws pat=*	
20:54:05.322 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_ods pat=*
20:54:05.322 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_ods pat=*	
20:54:05.327 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
20:54:05.327 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
20:54:05.331 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_release pat=*
20:54:05.331 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_release pat=*	
20:54:05.335 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_shop pat=*
20:54:05.335 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_shop pat=*	
20:54:05.338 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
20:54:05.338 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
20:54:05.343 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_shop pat=*
20:54:05.343 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_shop pat=*	
20:54:05.347 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
20:54:05.347 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
20:54:05.351 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_shop pat=*
20:54:05.351 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_shop pat=*	
20:54:05.355 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_release pat=*
20:54:05.355 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_release pat=*	
20:54:05.359 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_shop pat=*
20:54:05.359 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_shop pat=*	
20:54:05.365 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
20:54:05.365 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
20:54:05.369 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_shop pat=*
20:54:05.369 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_shop pat=*	
20:54:05.372 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dim pat=*
20:54:05.373 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dim pat=*	
20:54:05.377 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dm pat=*
20:54:05.377 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dm pat=*	
20:54:05.381 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dw pat=*
20:54:05.381 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dw pat=*	
20:54:05.384 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_rds pat=*
20:54:05.384 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_rds pat=*	
20:54:05.387 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=te pat=*
20:54:05.387 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=te pat=*	
20:54:05.391 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test pat=*
20:54:05.391 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test pat=*	
20:54:05.395 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test4 pat=*
20:54:05.395 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test4 pat=*	
20:54:07.128 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/71fc56a8-872a-4eb2-8141-141da7d82cc7_resources
20:54:07.143 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/71fc56a8-872a-4eb2-8141-141da7d82cc7
20:54:07.145 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/71fc56a8-872a-4eb2-8141-141da7d82cc7
20:54:07.149 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/71fc56a8-872a-4eb2-8141-141da7d82cc7/_tmp_space.db
20:54:07.152 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
20:54:07.164 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for root: Unknown error.
20:54:07.176 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:54:07.176 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:54:07.182 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
20:54:07.182 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
20:54:07.185 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
20:54:07.399 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/77dcbf6a-bb04-4553-85f6-e417e863c1b6_resources
20:54:07.409 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/77dcbf6a-bb04-4553-85f6-e417e863c1b6
20:54:07.412 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/77dcbf6a-bb04-4553-85f6-e417e863c1b6
20:54:07.422 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/77dcbf6a-bb04-4553-85f6-e417e863c1b6/_tmp_space.db
20:54:07.426 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
20:54:07.466 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
20:54:08.988 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_exposure
20:54:09.193 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:54:09.194 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:54:09.203 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:54:09.203 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:54:09.300 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:54:09.301 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:54:09.329 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:54:09.341 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:54:09.342 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:54:09.342 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:54:09.342 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:54:09.342 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:54:09.342 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:54:09.343 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:54:09.358 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
20:54:09.699 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: idcard
20:54:09.712 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: age
20:54:09.712 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: gender
20:54:09.713 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: area_code
20:54:09.713 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: longitude
20:54:09.714 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: latitude
20:54:09.715 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: matter_id
20:54:09.715 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: model_code
20:54:09.716 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: model_version
20:54:09.716 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: aid
20:54:09.716 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
20:54:09.717 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
20:54:09.779 ERROR [main] com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$ - cannot resolve '`idcard`' given input columns: [device_type, channels, bdp_day, release_session, sources, device_num, ct, release_status]; line 1 pos 0;
'Project ['idcard, 'age, 'gender, 'area_code, 'longitude, 'latitude, 'matter_id, 'model_code, 'model_version, 'aid, ct#6L, bdp_day#7]
+- SubqueryAlias dw_release_exposure
   +- Relation[release_session#0,release_status#1,device_num#2,device_type#3,sources#4,channels#5,ct#6L,bdp_day#7] parquet

org.apache.spark.sql.AnalysisException: cannot resolve '`idcard`' given input columns: [device_type, channels, bdp_day, release_session, sources, device_num, ct, release_status]; line 1 pos 0;
'Project ['idcard, 'age, 'gender, 'area_code, 'longitude, 'latitude, 'matter_id, 'model_code, 'model_version, 'aid, ct#6L, bdp_day#7]
+- SubqueryAlias dw_release_exposure
   +- Relation[release_session#0,release_status#1,device_num#2,device_type#3,sources#4,channels#5,ct#6L,bdp_day#7] parquet

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2888)
	at org.apache.spark.sql.Dataset.select(Dataset.scala:1154)
	at org.apache.spark.sql.Dataset.selectExpr(Dataset.scala:1189)
	at com.qf.bigdata.release.util.SparkHelper$.readTableData(SparkHelper.scala:26)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$.handleReleaseJob(MIDReleaseExposure.scala:40)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$$anonfun$handleJobs$1.apply(MIDReleaseExposure.scala:86)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$$anonfun$handleJobs$1.apply(MIDReleaseExposure.scala:84)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$.handleJobs(MIDReleaseExposure.scala:84)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$.main(MIDReleaseExposure.scala:106)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure.main(MIDReleaseExposure.scala)
20:54:09.787 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@47a2abb8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20:54:09.790 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.145.1:4040
20:54:09.803 INFO  [dispatcher-event-loop-3] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
20:54:09.812 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
20:54:09.812 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
20:54:09.820 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
20:54:09.825 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
20:54:09.827 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
20:54:09.831 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
20:54:09.832 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\Yongning Y\AppData\Local\Temp\spark-e4d357b7-c0d2-4633-bbb9-29347799176b
20:55:28.097 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
20:55:28.505 INFO  [main] org.apache.spark.SparkContext - Submitted application: mid_release_exposure_job
20:55:28.531 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: YongningY,root
20:55:28.532 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: YongningY,root
20:55:28.532 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
20:55:28.533 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
20:55:28.533 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(YongningY, root); groups with view permissions: Set(); users  with modify permissions: Set(YongningY, root); groups with modify permissions: Set()
20:55:29.123 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 8689.
20:55:29.142 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
20:55:29.159 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
20:55:29.162 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20:55:29.163 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
20:55:29.171 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\Yongning Y\AppData\Local\Temp\blockmgr-549f963d-8148-41a3-b020-5121a90013ba
20:55:29.189 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1445.7 MB
20:55:29.238 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
20:55:29.333 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @3812ms
20:55:29.409 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
20:55:29.422 INFO  [main] org.spark_project.jetty.server.Server - Started @3903ms
20:55:29.448 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@3fc08eec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20:55:29.448 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
20:55:29.470 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@492fc69e{/jobs,null,AVAILABLE,@Spark}
20:55:29.471 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d2260db{/jobs/json,null,AVAILABLE,@Spark}
20:55:29.472 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49bf29c6{/jobs/job,null,AVAILABLE,@Spark}
20:55:29.473 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7668d560{/jobs/job/json,null,AVAILABLE,@Spark}
20:55:29.473 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@126be319{/stages,null,AVAILABLE,@Spark}
20:55:29.474 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c371e13{/stages/json,null,AVAILABLE,@Spark}
20:55:29.474 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e34c607{/stages/stage,null,AVAILABLE,@Spark}
20:55:29.476 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9257031{/stages/stage/json,null,AVAILABLE,@Spark}
20:55:29.476 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7726e185{/stages/pool,null,AVAILABLE,@Spark}
20:55:29.477 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@282308c3{/stages/pool/json,null,AVAILABLE,@Spark}
20:55:29.478 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1db0ec27{/storage,null,AVAILABLE,@Spark}
20:55:29.479 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d4ab71a{/storage/json,null,AVAILABLE,@Spark}
20:55:29.481 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1af05b03{/storage/rdd,null,AVAILABLE,@Spark}
20:55:29.482 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ad777f{/storage/rdd/json,null,AVAILABLE,@Spark}
20:55:29.482 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@438bad7c{/environment,null,AVAILABLE,@Spark}
20:55:29.483 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4fdf8f12{/environment/json,null,AVAILABLE,@Spark}
20:55:29.484 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54f5f647{/executors,null,AVAILABLE,@Spark}
20:55:29.485 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a6d5a8f{/executors/json,null,AVAILABLE,@Spark}
20:55:29.485 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@315ba14a{/executors/threadDump,null,AVAILABLE,@Spark}
20:55:29.487 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27f0ad19{/executors/threadDump/json,null,AVAILABLE,@Spark}
20:55:29.500 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38d5b107{/static,null,AVAILABLE,@Spark}
20:55:29.501 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77e2a6e2{/,null,AVAILABLE,@Spark}
20:55:29.504 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@199e4c2b{/api,null,AVAILABLE,@Spark}
20:55:29.505 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/job/kill,null,AVAILABLE,@Spark}
20:55:29.506 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e7095ac{/stages/stage/kill,null,AVAILABLE,@Spark}
20:55:29.509 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.145.1:4040
20:55:29.640 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
20:55:29.667 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 8710.
20:55:29.668 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.145.1:8710
20:55:29.670 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20:55:29.693 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.145.1, 8710, None)
20:55:29.696 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.145.1:8710 with 1445.7 MB RAM, BlockManagerId(driver, 192.168.145.1, 8710, None)
20:55:29.698 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.145.1, 8710, None)
20:55:29.698 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.145.1, 8710, None)
20:55:29.920 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@dffa30b{/metrics/json,null,AVAILABLE,@Spark}
20:55:29.975 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/H:/IdeaProject/release_test/target/classes/hive-site.xml
20:55:30.000 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
20:55:30.001 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/user/hive/warehouse'.
20:55:30.012 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f443fae{/SQL,null,AVAILABLE,@Spark}
20:55:30.012 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79ab34c1{/SQL/json,null,AVAILABLE,@Spark}
20:55:30.014 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@782168b7{/SQL/execution,null,AVAILABLE,@Spark}
20:55:30.015 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7435a578{/SQL/execution/json,null,AVAILABLE,@Spark}
20:55:30.018 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b214b94{/static/sql,null,AVAILABLE,@Spark}
20:55:30.873 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20:55:31.712 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20:55:31.742 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
20:55:32.772 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20:55:34.161 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
20:55:34.163 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
20:55:34.470 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
20:55:34.474 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
20:55:34.535 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
20:55:34.671 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
20:55:34.673 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
20:55:34.688 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=azkaban pat=*
20:55:34.688 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=azkaban pat=*	
20:55:34.739 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dim pat=*
20:55:34.739 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dim pat=*	
20:55:34.742 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dm pat=*
20:55:34.742 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dm pat=*	
20:55:34.746 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dwd pat=*
20:55:34.746 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dwd pat=*	
20:55:34.750 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dws pat=*
20:55:34.750 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dws pat=*	
20:55:34.753 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_ods pat=*
20:55:34.754 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_ods pat=*	
20:55:34.756 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
20:55:34.757 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
20:55:34.760 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_release pat=*
20:55:34.760 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_release pat=*	
20:55:34.763 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_shop pat=*
20:55:34.763 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_shop pat=*	
20:55:34.766 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
20:55:34.767 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
20:55:34.770 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_shop pat=*
20:55:34.770 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_shop pat=*	
20:55:34.774 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
20:55:34.774 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
20:55:34.777 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_shop pat=*
20:55:34.777 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_shop pat=*	
20:55:34.781 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_release pat=*
20:55:34.781 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_release pat=*	
20:55:34.783 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_shop pat=*
20:55:34.784 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_shop pat=*	
20:55:34.788 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
20:55:34.789 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
20:55:34.792 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_shop pat=*
20:55:34.792 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_shop pat=*	
20:55:34.795 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dim pat=*
20:55:34.795 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dim pat=*	
20:55:34.798 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dm pat=*
20:55:34.798 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dm pat=*	
20:55:34.802 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dw pat=*
20:55:34.802 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dw pat=*	
20:55:34.804 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_rds pat=*
20:55:34.805 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_rds pat=*	
20:55:34.808 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=te pat=*
20:55:34.808 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=te pat=*	
20:55:34.810 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test pat=*
20:55:34.810 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test pat=*	
20:55:34.814 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test4 pat=*
20:55:34.814 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test4 pat=*	
20:55:36.515 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/2fbb114a-5a94-413f-b96f-b04b551fc1b9_resources
20:55:36.558 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/2fbb114a-5a94-413f-b96f-b04b551fc1b9
20:55:36.560 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/2fbb114a-5a94-413f-b96f-b04b551fc1b9
20:55:36.568 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/2fbb114a-5a94-413f-b96f-b04b551fc1b9/_tmp_space.db
20:55:36.572 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
20:55:36.583 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for root: Unknown error.
20:55:36.594 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:55:36.594 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:55:36.600 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
20:55:36.600 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
20:55:36.604 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
20:55:36.825 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/d3ed76e3-5d1a-4a4e-b984-62333bd8ad5b_resources
20:55:36.832 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/d3ed76e3-5d1a-4a4e-b984-62333bd8ad5b
20:55:36.836 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/d3ed76e3-5d1a-4a4e-b984-62333bd8ad5b
20:55:36.844 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/d3ed76e3-5d1a-4a4e-b984-62333bd8ad5b/_tmp_space.db
20:55:36.847 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
20:55:36.882 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
20:55:38.510 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_exposure
20:55:38.724 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:55:38.724 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:55:38.732 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:55:38.733 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:55:38.868 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:55:38.869 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:55:38.898 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:55:38.911 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:55:38.911 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:55:38.912 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:55:38.912 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:55:38.912 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:55:38.912 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:55:38.912 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:55:38.926 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
20:55:39.259 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: idcard
20:55:39.274 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: age
20:55:39.275 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: gender
20:55:39.276 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: area_code
20:55:39.277 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: longitude
20:55:39.278 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: latitude
20:55:39.279 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: matter_id
20:55:39.280 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: model_code
20:55:39.281 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: model_version
20:55:39.282 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: aid
20:55:39.282 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
20:55:39.283 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
20:55:39.359 ERROR [main] com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$ - cannot resolve '`idcard`' given input columns: [bdp_day, device_num, sources, device_type, release_session, ct, channels, release_status]; line 1 pos 0;
'Project ['idcard, 'age, 'gender, 'area_code, 'longitude, 'latitude, 'matter_id, 'model_code, 'model_version, 'aid, ct#6L, bdp_day#7]
+- SubqueryAlias dw_release_exposure
   +- Relation[release_session#0,release_status#1,device_num#2,device_type#3,sources#4,channels#5,ct#6L,bdp_day#7] parquet

org.apache.spark.sql.AnalysisException: cannot resolve '`idcard`' given input columns: [bdp_day, device_num, sources, device_type, release_session, ct, channels, release_status]; line 1 pos 0;
'Project ['idcard, 'age, 'gender, 'area_code, 'longitude, 'latitude, 'matter_id, 'model_code, 'model_version, 'aid, ct#6L, bdp_day#7]
+- SubqueryAlias dw_release_exposure
   +- Relation[release_session#0,release_status#1,device_num#2,device_type#3,sources#4,channels#5,ct#6L,bdp_day#7] parquet

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2888)
	at org.apache.spark.sql.Dataset.select(Dataset.scala:1154)
	at org.apache.spark.sql.Dataset.selectExpr(Dataset.scala:1189)
	at com.qf.bigdata.release.util.SparkHelper$.readTableData(SparkHelper.scala:26)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$.handleReleaseJob(MIDReleaseExposure.scala:40)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$$anonfun$handleJobs$1.apply(MIDReleaseExposure.scala:87)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$$anonfun$handleJobs$1.apply(MIDReleaseExposure.scala:85)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$.handleJobs(MIDReleaseExposure.scala:85)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure$.main(MIDReleaseExposure.scala:107)
	at com.qf.bigdata.release.etl.release.mid.MIDReleaseExposure.main(MIDReleaseExposure.scala)
20:55:39.369 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@3fc08eec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20:55:39.372 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.145.1:4040
20:55:39.384 INFO  [dispatcher-event-loop-2] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
20:55:39.393 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
20:55:39.394 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
20:55:39.400 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
20:55:39.406 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
20:55:39.408 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
20:55:39.412 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
20:55:39.413 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\Yongning Y\AppData\Local\Temp\spark-6accad16-896d-4000-9a81-ead455d156ec
20:56:39.234 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
20:56:39.591 INFO  [main] org.apache.spark.SparkContext - Submitted application: mid_release_exposure_job
20:56:39.612 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: YongningY,root
20:56:39.612 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: YongningY,root
20:56:39.613 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
20:56:39.613 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
20:56:39.614 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(YongningY, root); groups with view permissions: Set(); users  with modify permissions: Set(YongningY, root); groups with modify permissions: Set()
20:56:40.233 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 8746.
20:56:40.249 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
20:56:40.268 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
20:56:40.271 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20:56:40.272 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
20:56:40.280 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\Yongning Y\AppData\Local\Temp\blockmgr-f3e31ff7-cf2f-481e-bf9c-31a47075d8d2
20:56:40.299 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1445.7 MB
20:56:40.347 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
20:56:40.441 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @3965ms
20:56:40.587 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
20:56:40.606 INFO  [main] org.spark_project.jetty.server.Server - Started @4132ms
20:56:40.631 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@3fc08eec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20:56:40.631 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
20:56:40.702 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@492fc69e{/jobs,null,AVAILABLE,@Spark}
20:56:40.706 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d2260db{/jobs/json,null,AVAILABLE,@Spark}
20:56:40.707 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49bf29c6{/jobs/job,null,AVAILABLE,@Spark}
20:56:40.713 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7668d560{/jobs/job/json,null,AVAILABLE,@Spark}
20:56:40.728 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@126be319{/stages,null,AVAILABLE,@Spark}
20:56:40.730 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c371e13{/stages/json,null,AVAILABLE,@Spark}
20:56:40.731 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e34c607{/stages/stage,null,AVAILABLE,@Spark}
20:56:40.735 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9257031{/stages/stage/json,null,AVAILABLE,@Spark}
20:56:40.737 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7726e185{/stages/pool,null,AVAILABLE,@Spark}
20:56:40.738 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@282308c3{/stages/pool/json,null,AVAILABLE,@Spark}
20:56:40.739 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1db0ec27{/storage,null,AVAILABLE,@Spark}
20:56:40.740 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d4ab71a{/storage/json,null,AVAILABLE,@Spark}
20:56:40.741 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1af05b03{/storage/rdd,null,AVAILABLE,@Spark}
20:56:40.741 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ad777f{/storage/rdd/json,null,AVAILABLE,@Spark}
20:56:40.742 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@438bad7c{/environment,null,AVAILABLE,@Spark}
20:56:40.743 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4fdf8f12{/environment/json,null,AVAILABLE,@Spark}
20:56:40.744 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54f5f647{/executors,null,AVAILABLE,@Spark}
20:56:40.745 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a6d5a8f{/executors/json,null,AVAILABLE,@Spark}
20:56:40.746 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@315ba14a{/executors/threadDump,null,AVAILABLE,@Spark}
20:56:40.747 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27f0ad19{/executors/threadDump/json,null,AVAILABLE,@Spark}
20:56:40.765 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38d5b107{/static,null,AVAILABLE,@Spark}
20:56:40.766 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77e2a6e2{/,null,AVAILABLE,@Spark}
20:56:40.768 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@199e4c2b{/api,null,AVAILABLE,@Spark}
20:56:40.769 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/job/kill,null,AVAILABLE,@Spark}
20:56:40.770 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e7095ac{/stages/stage/kill,null,AVAILABLE,@Spark}
20:56:40.775 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.145.1:4040
20:56:40.875 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
20:56:40.907 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 8767.
20:56:40.908 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.145.1:8767
20:56:40.910 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20:56:40.962 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.145.1, 8767, None)
20:56:40.967 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.145.1:8767 with 1445.7 MB RAM, BlockManagerId(driver, 192.168.145.1, 8767, None)
20:56:40.973 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.145.1, 8767, None)
20:56:40.974 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.145.1, 8767, None)
20:56:41.224 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@aa004a0{/metrics/json,null,AVAILABLE,@Spark}
20:56:41.275 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/H:/IdeaProject/release_test/target/classes/hive-site.xml
20:56:41.313 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
20:56:41.313 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/user/hive/warehouse'.
20:56:41.319 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49dbaaf3{/SQL,null,AVAILABLE,@Spark}
20:56:41.320 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@736f3e9e{/SQL/json,null,AVAILABLE,@Spark}
20:56:41.321 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1698d7c0{/SQL/execution,null,AVAILABLE,@Spark}
20:56:41.321 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@87abc48{/SQL/execution/json,null,AVAILABLE,@Spark}
20:56:41.323 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13047d7d{/static/sql,null,AVAILABLE,@Spark}
20:56:42.387 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20:56:43.265 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20:56:43.296 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
20:56:44.451 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20:56:45.800 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
20:56:45.803 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
20:56:46.132 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
20:56:46.135 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
20:56:46.188 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
20:56:46.309 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
20:56:46.311 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
20:56:46.325 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=azkaban pat=*
20:56:46.325 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=azkaban pat=*	
20:56:46.426 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dim pat=*
20:56:46.426 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dim pat=*	
20:56:46.451 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dm pat=*
20:56:46.451 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dm pat=*	
20:56:46.455 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dwd pat=*
20:56:46.455 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dwd pat=*	
20:56:46.459 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dws pat=*
20:56:46.459 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dws pat=*	
20:56:46.464 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_ods pat=*
20:56:46.464 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_ods pat=*	
20:56:46.468 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
20:56:46.468 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
20:56:46.472 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_release pat=*
20:56:46.472 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_release pat=*	
20:56:46.475 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_shop pat=*
20:56:46.476 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_shop pat=*	
20:56:46.479 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
20:56:46.480 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
20:56:46.483 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_shop pat=*
20:56:46.484 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_shop pat=*	
20:56:46.488 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
20:56:46.488 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
20:56:46.492 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_shop pat=*
20:56:46.492 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_shop pat=*	
20:56:46.495 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_release pat=*
20:56:46.496 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_release pat=*	
20:56:46.499 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_shop pat=*
20:56:46.499 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_shop pat=*	
20:56:46.503 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
20:56:46.504 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
20:56:46.507 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_shop pat=*
20:56:46.507 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_shop pat=*	
20:56:46.510 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dim pat=*
20:56:46.511 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dim pat=*	
20:56:46.514 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dm pat=*
20:56:46.514 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dm pat=*	
20:56:46.519 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dw pat=*
20:56:46.519 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dw pat=*	
20:56:46.522 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_rds pat=*
20:56:46.522 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_rds pat=*	
20:56:46.525 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=te pat=*
20:56:46.525 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=te pat=*	
20:56:46.529 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test pat=*
20:56:46.529 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test pat=*	
20:56:46.532 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test4 pat=*
20:56:46.533 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test4 pat=*	
20:56:48.268 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/8267a21c-9a80-442a-aa2f-d3a98bda7716_resources
20:56:48.282 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/8267a21c-9a80-442a-aa2f-d3a98bda7716
20:56:48.284 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/8267a21c-9a80-442a-aa2f-d3a98bda7716
20:56:48.290 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/8267a21c-9a80-442a-aa2f-d3a98bda7716/_tmp_space.db
20:56:48.294 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
20:56:48.305 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for root: Unknown error.
20:56:48.318 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:56:48.318 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20:56:48.324 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
20:56:48.324 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
20:56:48.327 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
20:56:48.510 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/6c7d6b45-adba-44ce-ad7f-8c210b25d42c_resources
20:56:48.518 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/6c7d6b45-adba-44ce-ad7f-8c210b25d42c
20:56:48.523 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/6c7d6b45-adba-44ce-ad7f-8c210b25d42c
20:56:48.533 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/6c7d6b45-adba-44ce-ad7f-8c210b25d42c/_tmp_space.db
20:56:48.538 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
20:56:48.582 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
20:56:50.047 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_exposure
20:56:50.259 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:56:50.260 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:56:50.267 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:56:50.268 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:56:50.494 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:56:50.494 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:56:50.528 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:56:50.542 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:56:50.543 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:56:50.543 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:56:50.543 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:56:50.543 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:56:50.543 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:56:50.544 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:56:50.558 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
20:56:50.880 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
20:56:50.893 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
20:56:51.707 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:56:51.708 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:56:51.714 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:56:51.714 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:56:51.741 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:56:51.741 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:56:51.769 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:56:51.769 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:56:51.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:56:51.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:56:51.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:56:51.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:56:51.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:56:51.771 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:56:51.809 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:56:51.810 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:56:51.813 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=dw_release tbl=dw_release_exposure
20:56:51.813 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=dw_release tbl=dw_release_exposure	
20:56:52.645 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#7),(bdp_day#7 = 2019-09-06)
20:56:52.648 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
20:56:52.650 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<ct: bigint>
20:56:52.692 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: 
20:56:52.716 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
20:56:53.424 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 268.3432 ms
20:56:53.829 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 33.8473 ms
20:56:54.038 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 307.7 KB, free 1445.4 MB)
20:56:54.171 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1445.4 MB)
20:56:54.183 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.145.1:8767 (size: 26.3 KB, free: 1445.7 MB)
20:56:54.206 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at MIDReleaseExposure.scala:51
20:56:54.257 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4765987 bytes, open cost is considered as scanning 4194304 bytes.
20:56:54.606 INFO  [main] org.apache.spark.SparkContext - Starting job: show at MIDReleaseExposure.scala:51
20:56:54.670 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 3 (show at MIDReleaseExposure.scala:51)
20:56:54.673 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at MIDReleaseExposure.scala:51) with 1 output partitions
20:56:54.674 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at MIDReleaseExposure.scala:51)
20:56:54.674 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
20:56:54.676 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
20:56:54.722 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at show at MIDReleaseExposure.scala:51), which has no missing parents
20:56:54.963 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 12.0 KB, free 1445.4 MB)
20:56:54.966 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KB, free 1445.4 MB)
20:56:54.966 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.145.1:8767 (size: 5.8 KB, free: 1445.7 MB)
20:56:54.967 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
20:56:54.998 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at show at MIDReleaseExposure.scala:51) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20:56:54.999 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 4 tasks
20:56:55.090 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5421 bytes)
20:56:55.092 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5421 bytes)
20:56:55.093 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5421 bytes)
20:56:55.094 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5421 bytes)
20:56:55.129 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
20:56:55.129 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
20:56:55.129 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
20:56:55.129 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
20:56:55.452 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00000-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, range: 0-571693, partition values: [2019-09-06]
20:56:55.452 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00002-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, range: 0-571654, partition values: [2019-09-06]
20:56:55.452 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00001-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, range: 0-571692, partition values: [2019-09-06]
20:56:55.452 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00003-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, range: 0-571693, partition values: [2019-09-06]
20:56:56.360 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
20:56:56.365 WARN  [Executor task launch worker for task 0] org.apache.parquet.CorruptStatistics - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
20:56:57.227 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1611 bytes result sent to driver
20:56:57.227 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1611 bytes result sent to driver
20:56:57.227 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1611 bytes result sent to driver
20:56:57.227 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1568 bytes result sent to driver
20:56:57.243 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 2149 ms on localhost (executor driver) (1/4)
20:56:57.248 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 2155 ms on localhost (executor driver) (2/4)
20:56:57.248 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 2156 ms on localhost (executor driver) (3/4)
20:56:57.249 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 2171 ms on localhost (executor driver) (4/4)
20:56:57.260 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
20:56:57.269 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (show at MIDReleaseExposure.scala:51) finished in 2.233 s
20:56:57.270 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
20:56:57.270 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
20:56:57.275 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
20:56:57.276 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
20:56:57.289 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[5] at show at MIDReleaseExposure.scala:51), which has no missing parents
20:56:57.298 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1445.4 MB)
20:56:57.300 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1445.4 MB)
20:56:57.301 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.145.1:8767 (size: 2.2 KB, free: 1445.7 MB)
20:56:57.302 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
20:56:57.307 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at MIDReleaseExposure.scala:51) (first 15 tasks are for partitions Vector(0))
20:56:57.307 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
20:56:57.310 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 4, localhost, executor driver, partition 0, ANY, 4726 bytes)
20:56:57.310 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 4)
20:56:57.338 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 non-empty blocks out of 4 blocks
20:56:57.340 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 13 ms
20:56:57.477 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 4). 1046 bytes result sent to driver
20:56:57.484 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 4) in 176 ms on localhost (executor driver) (1/1)
20:56:57.484 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
20:56:57.485 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at MIDReleaseExposure.scala:51) finished in 0.177 s
20:56:57.514 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at MIDReleaseExposure.scala:51, took 2.908347 s
20:56:57.592 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@3fc08eec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20:56:57.595 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.145.1:4040
20:56:57.607 INFO  [dispatcher-event-loop-3] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
20:56:57.628 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
20:56:57.629 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
20:56:57.634 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
20:56:57.636 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
20:56:57.641 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
20:56:57.644 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
20:56:57.645 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\Yongning Y\AppData\Local\Temp\spark-385b6555-353d-4b82-9dff-6413552f1868
21:04:48.545 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
21:04:48.998 INFO  [main] org.apache.spark.SparkContext - Submitted application: mid_release_exposure_job
21:04:49.021 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: YongningY,root
21:04:49.021 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: YongningY,root
21:04:49.022 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
21:04:49.022 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
21:04:49.023 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(YongningY, root); groups with view permissions: Set(); users  with modify permissions: Set(YongningY, root); groups with modify permissions: Set()
21:04:49.651 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 8832.
21:04:49.672 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
21:04:49.689 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
21:04:49.692 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21:04:49.692 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
21:04:49.701 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\Yongning Y\AppData\Local\Temp\blockmgr-26f1de12-5259-4637-bbf3-5bba74fbae91
21:04:49.719 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1445.7 MB
21:04:49.774 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
21:04:49.864 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @4193ms
21:04:49.957 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
21:04:49.974 INFO  [main] org.spark_project.jetty.server.Server - Started @4305ms
21:04:49.999 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@9b179e8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
21:04:50.000 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
21:04:50.028 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60b85ba1{/jobs,null,AVAILABLE,@Spark}
21:04:50.029 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b718392{/jobs/json,null,AVAILABLE,@Spark}
21:04:50.030 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f2d2181{/jobs/job,null,AVAILABLE,@Spark}
21:04:50.031 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fcdcf{/jobs/job/json,null,AVAILABLE,@Spark}
21:04:50.032 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46292372{/stages,null,AVAILABLE,@Spark}
21:04:50.033 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c44052e{/stages/json,null,AVAILABLE,@Spark}
21:04:50.034 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@530a8454{/stages/stage,null,AVAILABLE,@Spark}
21:04:50.036 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31198ceb{/stages/stage/json,null,AVAILABLE,@Spark}
21:04:50.037 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75201592{/stages/pool,null,AVAILABLE,@Spark}
21:04:50.038 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@aa5455e{/stages/pool/json,null,AVAILABLE,@Spark}
21:04:50.039 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5dda14d0{/storage,null,AVAILABLE,@Spark}
21:04:50.040 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d9fc57a{/storage/json,null,AVAILABLE,@Spark}
21:04:50.041 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b4ef7{/storage/rdd,null,AVAILABLE,@Spark}
21:04:50.042 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5987e932{/storage/rdd/json,null,AVAILABLE,@Spark}
21:04:50.043 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bbbdd4b{/environment,null,AVAILABLE,@Spark}
21:04:50.044 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25230246{/environment/json,null,AVAILABLE,@Spark}
21:04:50.046 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a8b5227{/executors,null,AVAILABLE,@Spark}
21:04:50.047 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6979efad{/executors/json,null,AVAILABLE,@Spark}
21:04:50.048 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a67318f{/executors/threadDump,null,AVAILABLE,@Spark}
21:04:50.048 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17f9344b{/executors/threadDump/json,null,AVAILABLE,@Spark}
21:04:50.090 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54e81b21{/static,null,AVAILABLE,@Spark}
21:04:50.094 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2237bada{/,null,AVAILABLE,@Spark}
21:04:50.096 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5710768a{/api,null,AVAILABLE,@Spark}
21:04:50.097 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bf61e67{/jobs/job/kill,null,AVAILABLE,@Spark}
21:04:50.098 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b273a59{/stages/stage/kill,null,AVAILABLE,@Spark}
21:04:50.100 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.145.1:4040
21:04:50.304 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
21:04:50.355 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 8853.
21:04:50.356 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.145.1:8853
21:04:50.359 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21:04:50.403 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.145.1, 8853, None)
21:04:50.407 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.145.1:8853 with 1445.7 MB RAM, BlockManagerId(driver, 192.168.145.1, 8853, None)
21:04:50.413 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.145.1, 8853, None)
21:04:50.415 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.145.1, 8853, None)
21:04:50.682 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f02251{/metrics/json,null,AVAILABLE,@Spark}
21:04:50.745 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/H:/IdeaProject/release_test/target/classes/hive-site.xml
21:04:50.792 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
21:04:50.793 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/user/hive/warehouse'.
21:04:50.802 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64138b0c{/SQL,null,AVAILABLE,@Spark}
21:04:50.803 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22d9c961{/SQL/json,null,AVAILABLE,@Spark}
21:04:50.804 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@79ab34c1{/SQL/execution,null,AVAILABLE,@Spark}
21:04:50.805 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@281f23f2{/SQL/execution/json,null,AVAILABLE,@Spark}
21:04:50.807 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6093d508{/static/sql,null,AVAILABLE,@Spark}
21:04:51.711 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
21:04:52.552 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
21:04:52.583 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
21:04:53.639 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
21:04:55.020 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
21:04:55.023 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
21:04:55.325 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
21:04:55.330 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
21:04:55.394 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
21:04:55.527 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
21:04:55.530 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
21:04:55.552 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=azkaban pat=*
21:04:55.552 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=azkaban pat=*	
21:04:55.618 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dim pat=*
21:04:55.618 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dim pat=*	
21:04:55.621 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dm pat=*
21:04:55.621 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dm pat=*	
21:04:55.626 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dwd pat=*
21:04:55.626 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dwd pat=*	
21:04:55.631 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dws pat=*
21:04:55.631 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dws pat=*	
21:04:55.635 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_ods pat=*
21:04:55.635 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_ods pat=*	
21:04:55.639 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
21:04:55.639 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
21:04:55.642 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_release pat=*
21:04:55.643 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_release pat=*	
21:04:55.645 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_shop pat=*
21:04:55.646 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_shop pat=*	
21:04:55.650 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
21:04:55.650 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
21:04:55.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_shop pat=*
21:04:55.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_shop pat=*	
21:04:55.657 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
21:04:55.657 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
21:04:55.660 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_shop pat=*
21:04:55.661 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_shop pat=*	
21:04:55.665 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_release pat=*
21:04:55.665 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_release pat=*	
21:04:55.668 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_shop pat=*
21:04:55.668 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_shop pat=*	
21:04:55.672 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
21:04:55.672 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
21:04:55.676 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_shop pat=*
21:04:55.676 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_shop pat=*	
21:04:55.679 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dim pat=*
21:04:55.679 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dim pat=*	
21:04:55.681 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dm pat=*
21:04:55.681 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dm pat=*	
21:04:55.685 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dw pat=*
21:04:55.685 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dw pat=*	
21:04:55.687 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_rds pat=*
21:04:55.688 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_rds pat=*	
21:04:55.691 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=te pat=*
21:04:55.691 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=te pat=*	
21:04:55.694 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test pat=*
21:04:55.694 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test pat=*	
21:04:55.697 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test4 pat=*
21:04:55.697 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test4 pat=*	
21:04:57.446 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/2242056a-60b9-42aa-ac80-7ff59e014069_resources
21:04:57.491 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/2242056a-60b9-42aa-ac80-7ff59e014069
21:04:57.493 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/2242056a-60b9-42aa-ac80-7ff59e014069
21:04:57.498 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/2242056a-60b9-42aa-ac80-7ff59e014069/_tmp_space.db
21:04:57.502 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
21:04:57.514 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for root: Unknown error.
21:04:57.527 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:04:57.528 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:04:57.535 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
21:04:57.535 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
21:04:57.538 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
21:04:57.737 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/788260d8-a524-4e2a-8168-c5dd2f0eaa31_resources
21:04:57.774 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/788260d8-a524-4e2a-8168-c5dd2f0eaa31
21:04:57.786 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/788260d8-a524-4e2a-8168-c5dd2f0eaa31
21:04:57.814 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/788260d8-a524-4e2a-8168-c5dd2f0eaa31/_tmp_space.db
21:04:57.818 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
21:04:57.846 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
21:04:59.302 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_exposure
21:04:59.489 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
21:04:59.490 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: dw_release	
21:04:59.497 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
21:04:59.497 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
21:04:59.621 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
21:04:59.621 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
21:04:59.649 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:04:59.662 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:04:59.662 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:04:59.662 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:04:59.663 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:04:59.663 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:04:59.663 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:04:59.663 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
21:04:59.678 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
21:04:59.986 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
21:05:00.003 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
21:05:00.004 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
21:05:00.005 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
21:05:00.006 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
21:05:00.006 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
21:05:00.007 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
21:05:00.007 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
21:05:00.322 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
21:05:00.322 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: dw_release	
21:05:00.327 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
21:05:00.327 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
21:05:00.351 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
21:05:00.351 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
21:05:00.374 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:00.374 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:00.375 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:00.375 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:00.375 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:00.375 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:00.376 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:00.376 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
21:05:00.396 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
21:05:00.397 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
21:05:00.400 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=dw_release tbl=dw_release_exposure
21:05:00.400 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=dw_release tbl=dw_release_exposure	
21:05:00.627 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#7),(bdp_day#7 = 2019-09-06)
21:05:00.629 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
21:05:00.633 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
21:05:00.642 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: 
21:05:00.648 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
21:05:01.078 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 273.3757 ms
21:05:01.320 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 50.4394 ms
21:05:01.405 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 310.4 KB, free 1445.4 MB)
21:05:01.475 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.7 KB, free 1445.4 MB)
21:05:01.477 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.145.1:8853 (size: 26.7 KB, free: 1445.7 MB)
21:05:01.480 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at MIDReleaseExposure.scala:51
21:05:01.488 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4765987 bytes, open cost is considered as scanning 4194304 bytes.
21:05:01.620 INFO  [main] org.apache.spark.SparkContext - Starting job: show at MIDReleaseExposure.scala:51
21:05:01.640 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 3 (show at MIDReleaseExposure.scala:51)
21:05:01.642 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at MIDReleaseExposure.scala:51) with 1 output partitions
21:05:01.643 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at MIDReleaseExposure.scala:51)
21:05:01.643 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
21:05:01.645 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
21:05:01.649 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at show at MIDReleaseExposure.scala:51), which has no missing parents
21:05:01.729 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 14.5 KB, free 1445.4 MB)
21:05:01.732 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.4 KB, free 1445.4 MB)
21:05:01.733 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.145.1:8853 (size: 6.4 KB, free: 1445.7 MB)
21:05:01.733 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
21:05:01.747 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at show at MIDReleaseExposure.scala:51) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
21:05:01.748 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 4 tasks
21:05:01.794 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5421 bytes)
21:05:01.797 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5421 bytes)
21:05:01.798 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5421 bytes)
21:05:01.799 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5421 bytes)
21:05:01.806 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
21:05:01.806 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
21:05:01.806 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
21:05:01.806 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
21:05:01.870 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
21:05:01.983 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00003-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, range: 0-571693, partition values: [2019-09-06]
21:05:01.983 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00001-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, range: 0-571692, partition values: [2019-09-06]
21:05:01.983 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00002-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, range: 0-571654, partition values: [2019-09-06]
21:05:01.983 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00000-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, range: 0-571693, partition values: [2019-09-06]
21:05:02.616 WARN  [Executor task launch worker for task 2] org.apache.parquet.CorruptStatistics - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
21:05:03.400 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1611 bytes result sent to driver
21:05:03.403 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1568 bytes result sent to driver
21:05:03.423 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 1623 ms on localhost (executor driver) (1/4)
21:05:03.427 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1653 ms on localhost (executor driver) (2/4)
21:05:03.439 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1568 bytes result sent to driver
21:05:03.443 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1568 bytes result sent to driver
21:05:03.446 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 1648 ms on localhost (executor driver) (3/4)
21:05:03.447 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 1651 ms on localhost (executor driver) (4/4)
21:05:03.449 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
21:05:03.450 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (show at MIDReleaseExposure.scala:51) finished in 1.683 s
21:05:03.451 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
21:05:03.451 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
21:05:03.452 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
21:05:03.452 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
21:05:03.456 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[5] at show at MIDReleaseExposure.scala:51), which has no missing parents
21:05:03.465 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1445.3 MB)
21:05:03.467 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1445.3 MB)
21:05:03.468 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.145.1:8853 (size: 2.2 KB, free: 1445.7 MB)
21:05:03.469 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
21:05:03.471 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at MIDReleaseExposure.scala:51) (first 15 tasks are for partitions Vector(0))
21:05:03.471 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
21:05:03.475 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 4, localhost, executor driver, partition 0, ANY, 4726 bytes)
21:05:03.475 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 4)
21:05:03.491 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 non-empty blocks out of 4 blocks
21:05:03.493 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
21:05:03.514 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 4). 1580 bytes result sent to driver
21:05:03.515 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 4) in 42 ms on localhost (executor driver) (1/1)
21:05:03.515 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
21:05:03.515 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at MIDReleaseExposure.scala:51) finished in 0.043 s
21:05:03.521 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at MIDReleaseExposure.scala:51, took 1.900834 s
21:05:03.543 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@9b179e8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
21:05:03.545 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.145.1:4040
21:05:03.556 INFO  [dispatcher-event-loop-0] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
21:05:03.577 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
21:05:03.577 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
21:05:03.578 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
21:05:03.581 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
21:05:03.584 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
21:05:03.586 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
21:05:03.587 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\Yongning Y\AppData\Local\Temp\spark-c4b758c2-60ab-4ceb-9d58-f70d8243f1ca
21:05:38.099 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
21:05:38.579 INFO  [main] org.apache.spark.SparkContext - Submitted application: mid_release_exposure_job
21:05:38.605 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: YongningY,root
21:05:38.606 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: YongningY,root
21:05:38.606 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
21:05:38.607 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
21:05:38.607 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(YongningY, root); groups with view permissions: Set(); users  with modify permissions: Set(YongningY, root); groups with modify permissions: Set()
21:05:39.274 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 8902.
21:05:39.291 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
21:05:39.308 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
21:05:39.311 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21:05:39.311 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
21:05:39.320 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\Yongning Y\AppData\Local\Temp\blockmgr-0d917be7-d3bd-4c18-adc2-7d051538220b
21:05:39.338 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1445.7 MB
21:05:39.391 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
21:05:39.483 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @3976ms
21:05:39.559 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
21:05:39.574 INFO  [main] org.spark_project.jetty.server.Server - Started @4069ms
21:05:39.596 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@3fc08eec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
21:05:39.596 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
21:05:39.617 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@492fc69e{/jobs,null,AVAILABLE,@Spark}
21:05:39.618 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d2260db{/jobs/json,null,AVAILABLE,@Spark}
21:05:39.618 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49bf29c6{/jobs/job,null,AVAILABLE,@Spark}
21:05:39.619 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7668d560{/jobs/job/json,null,AVAILABLE,@Spark}
21:05:39.620 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@126be319{/stages,null,AVAILABLE,@Spark}
21:05:39.620 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c371e13{/stages/json,null,AVAILABLE,@Spark}
21:05:39.621 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e34c607{/stages/stage,null,AVAILABLE,@Spark}
21:05:39.622 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9257031{/stages/stage/json,null,AVAILABLE,@Spark}
21:05:39.623 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7726e185{/stages/pool,null,AVAILABLE,@Spark}
21:05:39.623 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@282308c3{/stages/pool/json,null,AVAILABLE,@Spark}
21:05:39.624 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1db0ec27{/storage,null,AVAILABLE,@Spark}
21:05:39.625 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d4ab71a{/storage/json,null,AVAILABLE,@Spark}
21:05:39.626 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1af05b03{/storage/rdd,null,AVAILABLE,@Spark}
21:05:39.627 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ad777f{/storage/rdd/json,null,AVAILABLE,@Spark}
21:05:39.628 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@438bad7c{/environment,null,AVAILABLE,@Spark}
21:05:39.629 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4fdf8f12{/environment/json,null,AVAILABLE,@Spark}
21:05:39.630 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54f5f647{/executors,null,AVAILABLE,@Spark}
21:05:39.631 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a6d5a8f{/executors/json,null,AVAILABLE,@Spark}
21:05:39.632 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@315ba14a{/executors/threadDump,null,AVAILABLE,@Spark}
21:05:39.633 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27f0ad19{/executors/threadDump/json,null,AVAILABLE,@Spark}
21:05:39.647 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38d5b107{/static,null,AVAILABLE,@Spark}
21:05:39.648 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77e2a6e2{/,null,AVAILABLE,@Spark}
21:05:39.651 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@199e4c2b{/api,null,AVAILABLE,@Spark}
21:05:39.651 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/job/kill,null,AVAILABLE,@Spark}
21:05:39.652 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e7095ac{/stages/stage/kill,null,AVAILABLE,@Spark}
21:05:39.654 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.145.1:4040
21:05:39.741 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
21:05:39.775 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 8923.
21:05:39.776 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.145.1:8923
21:05:39.778 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21:05:39.804 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.145.1, 8923, None)
21:05:39.808 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.145.1:8923 with 1445.7 MB RAM, BlockManagerId(driver, 192.168.145.1, 8923, None)
21:05:39.810 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.145.1, 8923, None)
21:05:39.811 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.145.1, 8923, None)
21:05:40.026 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@dffa30b{/metrics/json,null,AVAILABLE,@Spark}
21:05:40.075 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/H:/IdeaProject/release_test/target/classes/hive-site.xml
21:05:40.102 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
21:05:40.103 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/user/hive/warehouse'.
21:05:40.110 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@736f3e9e{/SQL,null,AVAILABLE,@Spark}
21:05:40.112 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f443fae{/SQL/json,null,AVAILABLE,@Spark}
21:05:40.113 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@87abc48{/SQL/execution,null,AVAILABLE,@Spark}
21:05:40.114 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@782168b7{/SQL/execution/json,null,AVAILABLE,@Spark}
21:05:40.117 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65bb9029{/static/sql,null,AVAILABLE,@Spark}
21:05:40.982 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
21:05:41.753 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
21:05:41.782 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
21:05:42.779 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
21:05:44.203 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
21:05:44.205 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
21:05:44.460 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
21:05:44.464 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
21:05:44.514 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
21:05:44.624 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
21:05:44.626 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
21:05:44.640 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=azkaban pat=*
21:05:44.640 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=azkaban pat=*	
21:05:44.695 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dim pat=*
21:05:44.695 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dim pat=*	
21:05:44.698 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dm pat=*
21:05:44.699 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dm pat=*	
21:05:44.702 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dwd pat=*
21:05:44.702 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dwd pat=*	
21:05:44.706 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dws pat=*
21:05:44.706 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dws pat=*	
21:05:44.710 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_ods pat=*
21:05:44.711 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_ods pat=*	
21:05:44.715 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
21:05:44.715 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
21:05:44.719 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_release pat=*
21:05:44.719 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_release pat=*	
21:05:44.722 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_shop pat=*
21:05:44.722 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_shop pat=*	
21:05:44.726 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
21:05:44.726 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
21:05:44.730 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_shop pat=*
21:05:44.730 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_shop pat=*	
21:05:44.734 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
21:05:44.734 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
21:05:44.738 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_shop pat=*
21:05:44.739 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_shop pat=*	
21:05:44.743 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_release pat=*
21:05:44.743 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_release pat=*	
21:05:44.746 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_shop pat=*
21:05:44.746 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_shop pat=*	
21:05:44.750 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
21:05:44.750 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
21:05:44.754 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_shop pat=*
21:05:44.754 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_shop pat=*	
21:05:44.758 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dim pat=*
21:05:44.758 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dim pat=*	
21:05:44.761 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dm pat=*
21:05:44.761 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dm pat=*	
21:05:44.765 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dw pat=*
21:05:44.765 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dw pat=*	
21:05:44.768 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_rds pat=*
21:05:44.768 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_rds pat=*	
21:05:44.771 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=te pat=*
21:05:44.771 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=te pat=*	
21:05:44.774 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test pat=*
21:05:44.774 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test pat=*	
21:05:44.778 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test4 pat=*
21:05:44.778 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test4 pat=*	
21:05:46.454 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/8c989159-99ef-49c7-80ac-fad9d839da81_resources
21:05:46.497 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/8c989159-99ef-49c7-80ac-fad9d839da81
21:05:46.500 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/8c989159-99ef-49c7-80ac-fad9d839da81
21:05:46.514 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/8c989159-99ef-49c7-80ac-fad9d839da81/_tmp_space.db
21:05:46.518 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
21:05:46.530 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for root: Unknown error.
21:05:46.542 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:05:46.542 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:05:46.548 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
21:05:46.548 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
21:05:46.551 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
21:05:46.752 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/7030783e-b5d5-4fc2-a2f5-c73c8ef0ee18_resources
21:05:46.761 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/7030783e-b5d5-4fc2-a2f5-c73c8ef0ee18
21:05:46.763 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/7030783e-b5d5-4fc2-a2f5-c73c8ef0ee18
21:05:46.771 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/7030783e-b5d5-4fc2-a2f5-c73c8ef0ee18/_tmp_space.db
21:05:46.774 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
21:05:46.812 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
21:05:48.331 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_exposure
21:05:48.539 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
21:05:48.539 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: dw_release	
21:05:48.545 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
21:05:48.545 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
21:05:48.634 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
21:05:48.635 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
21:05:48.664 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:48.676 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:48.676 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:48.677 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:48.677 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:48.678 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:48.678 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:48.678 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
21:05:48.691 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
21:05:49.019 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
21:05:49.033 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
21:05:49.034 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
21:05:49.035 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
21:05:49.035 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
21:05:49.035 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
21:05:49.036 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
21:05:49.037 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
21:05:49.129 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
21:05:49.130 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
21:05:49.131 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
21:05:49.137 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
21:05:49.137 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
21:05:49.169 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
21:05:49.169 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
21:05:49.200 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:49.200 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:49.200 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:49.201 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:49.201 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:49.201 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:49.202 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:49.202 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:49.202 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:49.202 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
21:05:49.224 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
21:05:49.226 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
21:05:49.255 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: floor(datediff(from_unixtime(unix_timestamp(),'yyyy-MM-dd'),from_unixtime(unix_timestamp(substr(get_json_object(exts,'$.idcard'),7,8),'yyyyMMdd')))/365) as age
21:05:49.275 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: case subStr(get_json_object(exts,'$.idcard'),17,1)%2 when 1 then '男'  when 0 then '女' end as gender 
21:05:49.300 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
21:05:49.300 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
21:05:49.301 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
21:05:49.302 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
21:05:49.302 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
21:05:49.303 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_version
21:05:49.306 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') aid
21:05:49.310 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:05:49.310 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:05:49.317 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:05:49.317 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:05:49.326 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:05:49.326 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:05:49.333 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:05:49.334 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:05:49.341 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:05:49.341 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:05:49.347 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:05:49.347 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:05:49.352 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:05:49.352 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:05:49.357 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:05:49.357 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:05:49.362 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:05:49.362 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:05:49.367 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:05:49.367 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:05:49.372 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:05:49.373 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:05:49.378 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:05:49.378 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:05:49.381 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:05:49.382 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:05:49.385 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:05:49.385 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:05:49.389 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:05:49.389 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:05:49.393 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:05:49.393 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:05:49.397 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:05:49.397 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:05:49.401 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:05:49.401 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:05:49.405 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:05:49.405 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:05:49.408 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:05:49.408 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:05:49.848 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
21:05:49.848 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
21:05:49.853 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
21:05:49.853 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
21:05:49.880 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
21:05:49.881 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
21:05:49.902 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:49.903 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:49.903 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:49.904 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:49.904 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:49.904 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:49.904 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:49.904 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:49.904 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:05:49.904 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
21:05:49.927 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
21:05:49.928 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
21:05:49.931 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
21:05:49.931 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
21:05:50.133 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#45),(bdp_day#45 = 2019-09-06)
21:05:50.136 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#38),(release_status#38 = 01)
21:05:50.138 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, exts: string ... 1 more fields>
21:05:50.183 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
21:05:50.189 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
21:05:50.641 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 282.2867 ms
21:05:50.689 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
21:05:50.917 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 40.5339 ms
21:05:51.069 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 308.7 KB, free 1445.4 MB)
21:05:51.143 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.4 KB, free 1445.4 MB)
21:05:51.145 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.145.1:8923 (size: 26.4 KB, free: 1445.7 MB)
21:05:51.149 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at MIDReleaseExposure.scala:51
21:05:51.158 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 11968796 bytes, open cost is considered as scanning 4194304 bytes.
21:05:51.272 INFO  [main] org.apache.spark.SparkContext - Starting job: show at MIDReleaseExposure.scala:51
21:05:51.290 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at MIDReleaseExposure.scala:51)
21:05:51.292 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at MIDReleaseExposure.scala:51) with 1 output partitions
21:05:51.293 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at MIDReleaseExposure.scala:51)
21:05:51.293 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
21:05:51.295 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
21:05:51.299 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at show at MIDReleaseExposure.scala:51), which has no missing parents
21:05:51.467 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 22.5 KB, free 1445.4 MB)
21:05:51.470 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.2 KB, free 1445.3 MB)
21:05:51.472 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.145.1:8923 (size: 10.2 KB, free: 1445.7 MB)
21:05:51.472 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
21:05:51.483 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at show at MIDReleaseExposure.scala:51) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
21:05:51.484 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 4 tasks
21:05:51.521 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5390 bytes)
21:05:51.524 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5390 bytes)
21:05:51.524 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5390 bytes)
21:05:51.525 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5390 bytes)
21:05:51.532 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
21:05:51.532 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
21:05:51.532 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
21:05:51.532 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
21:05:51.853 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 89.8626 ms
21:05:51.877 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-06]
21:05:51.877 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-06]
21:05:51.877 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-06]
21:05:51.877 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-06]
21:05:52.471 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
21:05:52.472 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
21:05:52.472 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
21:05:52.654 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
21:05:52.745 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1611 bytes result sent to driver
21:05:52.745 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1654 bytes result sent to driver
21:05:52.745 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1611 bytes result sent to driver
21:05:52.768 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 1242 ms on localhost (executor driver) (1/4)
21:05:52.773 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 1249 ms on localhost (executor driver) (2/4)
21:05:52.773 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1261 ms on localhost (executor driver) (3/4)
21:05:57.277 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1740 bytes result sent to driver
21:05:57.279 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 5756 ms on localhost (executor driver) (4/4)
21:05:57.281 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
21:05:57.281 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (show at MIDReleaseExposure.scala:51) finished in 5.778 s
21:05:57.282 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
21:05:57.283 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
21:05:57.283 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
21:05:57.284 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
21:05:57.288 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at MIDReleaseExposure.scala:51), which has no missing parents
21:05:57.295 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1445.3 MB)
21:05:57.297 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1445.3 MB)
21:05:57.298 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.145.1:8923 (size: 2.2 KB, free: 1445.7 MB)
21:05:57.298 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
21:05:57.300 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at MIDReleaseExposure.scala:51) (first 15 tasks are for partitions Vector(0))
21:05:57.300 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
21:05:57.302 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 4, localhost, executor driver, partition 0, ANY, 4726 bytes)
21:05:57.302 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 4)
21:05:57.317 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
21:05:57.318 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
21:05:57.341 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 4). 2121 bytes result sent to driver
21:05:57.343 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 4) in 42 ms on localhost (executor driver) (1/1)
21:05:57.343 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
21:05:57.344 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at MIDReleaseExposure.scala:51) finished in 0.043 s
21:05:57.348 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at MIDReleaseExposure.scala:51, took 6.075774 s
21:05:57.367 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@3fc08eec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
21:05:57.369 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.145.1:4040
21:05:57.380 INFO  [dispatcher-event-loop-2] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
21:05:57.396 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
21:05:57.396 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
21:05:57.397 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
21:05:57.399 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
21:05:57.401 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
21:05:57.403 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
21:05:57.404 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\Yongning Y\AppData\Local\Temp\spark-ee2e16e0-16dd-41e6-854c-54f9f35df857
21:06:28.867 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
21:06:29.324 INFO  [main] org.apache.spark.SparkContext - Submitted application: mid_release_exposure_job
21:06:29.352 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: YongningY,root
21:06:29.352 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: YongningY,root
21:06:29.353 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
21:06:29.354 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
21:06:29.355 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(YongningY, root); groups with view permissions: Set(); users  with modify permissions: Set(YongningY, root); groups with modify permissions: Set()
21:06:29.982 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 8970.
21:06:30.001 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
21:06:30.018 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
21:06:30.020 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21:06:30.021 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
21:06:30.030 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\Yongning Y\AppData\Local\Temp\blockmgr-153cfb17-4440-4a88-b499-acc1ff5f50b2
21:06:30.049 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1445.7 MB
21:06:30.101 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
21:06:30.200 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @4094ms
21:06:30.293 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
21:06:30.314 INFO  [main] org.spark_project.jetty.server.Server - Started @4208ms
21:06:30.340 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@3fc08eec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
21:06:30.341 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
21:06:30.366 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@492fc69e{/jobs,null,AVAILABLE,@Spark}
21:06:30.367 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d2260db{/jobs/json,null,AVAILABLE,@Spark}
21:06:30.368 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49bf29c6{/jobs/job,null,AVAILABLE,@Spark}
21:06:30.369 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7668d560{/jobs/job/json,null,AVAILABLE,@Spark}
21:06:30.370 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@126be319{/stages,null,AVAILABLE,@Spark}
21:06:30.371 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c371e13{/stages/json,null,AVAILABLE,@Spark}
21:06:30.372 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e34c607{/stages/stage,null,AVAILABLE,@Spark}
21:06:30.373 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9257031{/stages/stage/json,null,AVAILABLE,@Spark}
21:06:30.374 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7726e185{/stages/pool,null,AVAILABLE,@Spark}
21:06:30.376 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@282308c3{/stages/pool/json,null,AVAILABLE,@Spark}
21:06:30.376 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1db0ec27{/storage,null,AVAILABLE,@Spark}
21:06:30.377 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d4ab71a{/storage/json,null,AVAILABLE,@Spark}
21:06:30.378 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1af05b03{/storage/rdd,null,AVAILABLE,@Spark}
21:06:30.379 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ad777f{/storage/rdd/json,null,AVAILABLE,@Spark}
21:06:30.380 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@438bad7c{/environment,null,AVAILABLE,@Spark}
21:06:30.381 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4fdf8f12{/environment/json,null,AVAILABLE,@Spark}
21:06:30.382 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54f5f647{/executors,null,AVAILABLE,@Spark}
21:06:30.383 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a6d5a8f{/executors/json,null,AVAILABLE,@Spark}
21:06:30.384 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@315ba14a{/executors/threadDump,null,AVAILABLE,@Spark}
21:06:30.385 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27f0ad19{/executors/threadDump/json,null,AVAILABLE,@Spark}
21:06:30.401 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38d5b107{/static,null,AVAILABLE,@Spark}
21:06:30.402 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77e2a6e2{/,null,AVAILABLE,@Spark}
21:06:30.407 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@199e4c2b{/api,null,AVAILABLE,@Spark}
21:06:30.410 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/job/kill,null,AVAILABLE,@Spark}
21:06:30.413 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e7095ac{/stages/stage/kill,null,AVAILABLE,@Spark}
21:06:30.417 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.145.1:4040
21:06:30.545 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
21:06:30.576 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 8991.
21:06:30.576 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.145.1:8991
21:06:30.578 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21:06:30.606 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.145.1, 8991, None)
21:06:30.609 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.145.1:8991 with 1445.7 MB RAM, BlockManagerId(driver, 192.168.145.1, 8991, None)
21:06:30.613 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.145.1, 8991, None)
21:06:30.613 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.145.1, 8991, None)
21:06:30.856 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@aa004a0{/metrics/json,null,AVAILABLE,@Spark}
21:06:30.904 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/H:/IdeaProject/release_test/target/classes/hive-site.xml
21:06:30.939 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
21:06:30.940 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/user/hive/warehouse'.
21:06:30.947 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49dbaaf3{/SQL,null,AVAILABLE,@Spark}
21:06:30.948 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@736f3e9e{/SQL/json,null,AVAILABLE,@Spark}
21:06:30.948 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1698d7c0{/SQL/execution,null,AVAILABLE,@Spark}
21:06:30.949 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@87abc48{/SQL/execution/json,null,AVAILABLE,@Spark}
21:06:30.951 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13047d7d{/static/sql,null,AVAILABLE,@Spark}
21:06:31.752 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
21:06:32.519 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
21:06:32.549 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
21:06:33.636 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
21:06:34.887 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
21:06:34.890 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
21:06:35.141 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
21:06:35.145 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
21:06:35.192 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
21:06:35.299 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
21:06:35.301 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
21:06:35.316 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=azkaban pat=*
21:06:35.316 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=azkaban pat=*	
21:06:35.371 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dim pat=*
21:06:35.371 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dim pat=*	
21:06:35.374 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dm pat=*
21:06:35.375 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dm pat=*	
21:06:35.380 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dwd pat=*
21:06:35.380 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dwd pat=*	
21:06:35.384 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dws pat=*
21:06:35.385 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dws pat=*	
21:06:35.388 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_ods pat=*
21:06:35.388 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_ods pat=*	
21:06:35.391 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
21:06:35.391 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
21:06:35.394 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_release pat=*
21:06:35.395 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_release pat=*	
21:06:35.399 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_shop pat=*
21:06:35.399 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_shop pat=*	
21:06:35.402 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
21:06:35.403 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
21:06:35.406 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_shop pat=*
21:06:35.406 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_shop pat=*	
21:06:35.410 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
21:06:35.410 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
21:06:35.414 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_shop pat=*
21:06:35.414 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_shop pat=*	
21:06:35.417 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_release pat=*
21:06:35.418 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_release pat=*	
21:06:35.421 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_shop pat=*
21:06:35.421 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_shop pat=*	
21:06:35.425 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
21:06:35.425 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
21:06:35.429 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_shop pat=*
21:06:35.429 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_shop pat=*	
21:06:35.432 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dim pat=*
21:06:35.432 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dim pat=*	
21:06:35.435 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dm pat=*
21:06:35.435 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dm pat=*	
21:06:35.439 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dw pat=*
21:06:35.439 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dw pat=*	
21:06:35.442 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_rds pat=*
21:06:35.443 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_rds pat=*	
21:06:35.447 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=te pat=*
21:06:35.447 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=te pat=*	
21:06:35.450 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test pat=*
21:06:35.450 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test pat=*	
21:06:35.453 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test4 pat=*
21:06:35.453 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test4 pat=*	
21:06:37.184 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/70c9aa9e-2d8a-42fb-9107-2352e65ce724_resources
21:06:37.197 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/70c9aa9e-2d8a-42fb-9107-2352e65ce724
21:06:37.234 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/70c9aa9e-2d8a-42fb-9107-2352e65ce724
21:06:37.248 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/70c9aa9e-2d8a-42fb-9107-2352e65ce724/_tmp_space.db
21:06:37.251 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
21:06:37.261 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for root: Unknown error.
21:06:37.274 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:06:37.274 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:06:37.280 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
21:06:37.280 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
21:06:37.283 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
21:06:37.484 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/689d9dc9-1101-48d2-b99e-bb668104aedb_resources
21:06:37.491 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/689d9dc9-1101-48d2-b99e-bb668104aedb
21:06:37.498 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/689d9dc9-1101-48d2-b99e-bb668104aedb
21:06:37.513 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/689d9dc9-1101-48d2-b99e-bb668104aedb/_tmp_space.db
21:06:37.516 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
21:06:37.551 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
21:06:39.021 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_exposure
21:06:39.242 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
21:06:39.242 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: dw_release	
21:06:39.250 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
21:06:39.250 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
21:06:39.340 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
21:06:39.341 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
21:06:39.370 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:39.382 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:39.383 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:39.383 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:39.383 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:39.383 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:39.383 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:39.384 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
21:06:39.399 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
21:06:39.709 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
21:06:39.726 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
21:06:39.726 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
21:06:39.728 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
21:06:39.729 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
21:06:39.729 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
21:06:39.730 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
21:06:39.730 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
21:06:39.824 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
21:06:39.825 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
21:06:39.826 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
21:06:39.835 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
21:06:39.835 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
21:06:39.870 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
21:06:39.870 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
21:06:39.904 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:39.904 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:39.905 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:39.905 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:39.905 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:39.906 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:39.906 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:39.906 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:39.906 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:39.907 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
21:06:39.927 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
21:06:39.928 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
21:06:39.952 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: floor(datediff(from_unixtime(unix_timestamp(),'yyyy-MM-dd'),from_unixtime(unix_timestamp(substr(get_json_object(exts,'$.idcard'),7,8),'yyyyMMdd')))/365) as age
21:06:39.966 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: case subStr(get_json_object(exts,'$.idcard'),17,1)%2 when 1 then '男'  when 0 then '女' end as gender 
21:06:39.988 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
21:06:39.989 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
21:06:39.991 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
21:06:39.991 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
21:06:39.992 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
21:06:39.996 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_version
21:06:39.996 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') aid
21:06:39.999 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:06:40.003 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:06:40.011 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:06:40.011 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:06:40.017 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:06:40.017 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:06:40.021 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:06:40.021 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:06:40.028 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:06:40.029 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:06:40.033 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:06:40.033 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:06:40.037 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:06:40.037 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:06:40.042 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:06:40.042 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:06:40.047 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:06:40.048 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:06:40.054 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:06:40.054 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:06:40.058 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:06:40.059 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:06:40.063 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:06:40.063 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:06:40.067 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:06:40.067 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:06:40.071 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:06:40.071 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:06:40.075 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:06:40.075 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:06:40.079 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:06:40.080 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:06:40.084 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:06:40.084 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:06:40.088 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:06:40.088 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:06:40.091 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:06:40.092 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:06:40.096 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
21:06:40.096 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21:06:40.709 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
21:06:40.709 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: dw_release	
21:06:40.714 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
21:06:40.714 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
21:06:40.738 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
21:06:40.738 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
21:06:40.760 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:40.760 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:40.761 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:40.761 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:40.761 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:40.761 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:40.761 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:40.762 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
21:06:40.784 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
21:06:40.785 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
21:06:40.788 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=dw_release tbl=dw_release_exposure
21:06:40.788 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=dw_release tbl=dw_release_exposure	
21:06:40.970 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
21:06:40.970 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_release	
21:06:40.975 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
21:06:40.976 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
21:06:41.003 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
21:06:41.003 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
21:06:41.027 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:41.028 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:41.028 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:41.028 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:41.028 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:41.028 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:41.028 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:41.028 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:41.028 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
21:06:41.028 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
21:06:41.030 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
21:06:41.030 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
21:06:41.031 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
21:06:41.031 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
21:06:41.150 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#7),(bdp_day#7 = 2019-09-06)
21:06:41.153 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_session#0)
21:06:41.155 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
21:06:41.166 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_session)
21:06:41.170 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
21:06:41.180 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#45),(bdp_day#45 = 2019-09-06)
21:06:41.181 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#38),(release_status#38 = 01),isnotnull(release_session#37)
21:06:41.182 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, exts: string ... 1 more fields>
21:06:41.185 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01),IsNotNull(release_session)
21:06:41.187 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
21:06:41.238 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(none),EqualTo(none,01),IsNotNull(none)
21:06:41.708 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 302.2267 ms
21:06:41.976 INFO  [broadcast-exchange-0] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 55.6326 ms
21:06:42.081 INFO  [broadcast-exchange-0] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 308.7 KB, free 1445.4 MB)
21:06:42.155 INFO  [broadcast-exchange-0] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.4 KB, free 1445.4 MB)
21:06:42.158 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.145.1:8991 (size: 26.4 KB, free: 1445.7 MB)
21:06:42.162 INFO  [broadcast-exchange-0] org.apache.spark.SparkContext - Created broadcast 0 from run at ThreadPoolExecutor.java:1149
21:06:42.169 INFO  [broadcast-exchange-0] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 11968796 bytes, open cost is considered as scanning 4194304 bytes.
21:06:42.434 INFO  [broadcast-exchange-0] org.apache.spark.SparkContext - Starting job: run at ThreadPoolExecutor.java:1149
21:06:42.456 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (run at ThreadPoolExecutor.java:1149)
21:06:42.459 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (run at ThreadPoolExecutor.java:1149) with 4 output partitions
21:06:42.460 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (run at ThreadPoolExecutor.java:1149)
21:06:42.460 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
21:06:42.464 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
21:06:42.472 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at run at ThreadPoolExecutor.java:1149), which has no missing parents
21:06:42.633 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 23.7 KB, free 1445.3 MB)
21:06:42.636 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.6 KB, free 1445.3 MB)
21:06:42.636 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.145.1:8991 (size: 10.6 KB, free: 1445.7 MB)
21:06:42.637 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
21:06:42.662 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
21:06:42.663 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 4 tasks
21:06:42.724 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5390 bytes)
21:06:42.729 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5390 bytes)
21:06:42.730 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5390 bytes)
21:06:42.731 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5390 bytes)
21:06:42.743 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
21:06:42.743 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
21:06:42.743 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
21:06:42.743 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
21:06:42.990 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 6
21:06:42.991 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
21:06:42.991 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 1
21:06:42.991 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 2
21:06:43.035 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 98.9143 ms
21:06:43.064 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-06]
21:06:43.065 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-06]
21:06:43.066 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-06]
21:06:43.066 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/ods/release_session/bdp_day=2019-09-06/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-06]
21:06:43.829 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(and(noteq(release_status, null), eq(release_status, Binary{"01"})), noteq(release_session, null))
21:06:43.829 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(and(noteq(release_status, null), eq(release_status, Binary{"01"})), noteq(release_session, null))
21:06:43.830 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(and(noteq(release_status, null), eq(release_status, Binary{"01"})), noteq(release_session, null))
21:06:43.860 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(and(noteq(release_status, null), eq(release_status, Binary{"01"})), noteq(release_session, null))
21:06:43.943 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1654 bytes result sent to driver
21:06:43.943 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1654 bytes result sent to driver
21:06:43.945 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1654 bytes result sent to driver
21:06:43.959 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 1224 ms on localhost (executor driver) (1/4)
21:06:43.963 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 1232 ms on localhost (executor driver) (2/4)
21:06:43.963 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1256 ms on localhost (executor driver) (3/4)
21:06:48.049 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1783 bytes result sent to driver
21:06:48.051 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 5322 ms on localhost (executor driver) (4/4)
21:06:48.052 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
21:06:48.055 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (run at ThreadPoolExecutor.java:1149) finished in 5.365 s
21:06:48.056 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
21:06:48.057 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
21:06:48.058 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
21:06:48.058 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
21:06:48.063 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at run at ThreadPoolExecutor.java:1149), which has no missing parents
21:06:48.069 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.8 KB, free 1445.3 MB)
21:06:48.071 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1445.3 MB)
21:06:48.071 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.145.1:8991 (size: 2.3 KB, free: 1445.7 MB)
21:06:48.072 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
21:06:48.074 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
21:06:48.074 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 4 tasks
21:06:48.076 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 4, localhost, executor driver, partition 0, ANY, 4726 bytes)
21:06:48.076 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 1.0 (TID 5, localhost, executor driver, partition 1, ANY, 4726 bytes)
21:06:48.077 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 1.0 (TID 6, localhost, executor driver, partition 2, ANY, 4726 bytes)
21:06:48.077 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 1.0 (TID 7, localhost, executor driver, partition 3, ANY, 4726 bytes)
21:06:48.077 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 1.0 in stage 1.0 (TID 5)
21:06:48.077 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 4)
21:06:48.077 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 2.0 in stage 1.0 (TID 6)
21:06:48.077 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 3.0 in stage 1.0 (TID 7)
21:06:48.093 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
21:06:48.093 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
21:06:48.093 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
21:06:48.093 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
21:06:48.095 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 6 ms
21:06:48.095 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 6 ms
21:06:48.095 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 6 ms
21:06:48.095 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 6 ms
21:06:48.196 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.memory.MemoryStore - Block taskresult_5 stored as bytes in memory (estimated size 1638.7 KB, free 1442.1 MB)
21:06:48.198 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added taskresult_5 in memory on 192.168.145.1:8991 (size: 1638.7 KB, free: 1444.1 MB)
21:06:48.199 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 1.0 in stage 1.0 (TID 5). 1677994 bytes result sent via BlockManager)
21:06:48.199 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.memory.MemoryStore - Block taskresult_6 stored as bytes in memory (estimated size 1642.5 KB, free 1440.5 MB)
21:06:48.201 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.memory.MemoryStore - Block taskresult_4 stored as bytes in memory (estimated size 1645.5 KB, free 1440.5 MB)
21:06:48.204 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added taskresult_6 in memory on 192.168.145.1:8991 (size: 1642.5 KB, free: 1442.5 MB)
21:06:48.204 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 2.0 in stage 1.0 (TID 6). 1681966 bytes result sent via BlockManager)
21:06:48.205 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added taskresult_4 in memory on 192.168.145.1:8991 (size: 1645.5 KB, free: 1440.9 MB)
21:06:48.206 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 4). 1684985 bytes result sent via BlockManager)
21:06:48.213 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.memory.MemoryStore - Block taskresult_7 stored as bytes in memory (estimated size 1638.0 KB, free 1438.9 MB)
21:06:48.215 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added taskresult_7 in memory on 192.168.145.1:8991 (size: 1638.0 KB, free: 1439.3 MB)
21:06:48.215 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 3.0 in stage 1.0 (TID 7). 1677317 bytes result sent via BlockManager)
21:06:48.401 INFO  [task-result-getter-1] org.apache.spark.network.client.TransportClientFactory - Successfully created connection to /192.168.145.1:8991 after 122 ms (0 ms spent in bootstraps)
21:06:48.560 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 1.0 (TID 6) in 483 ms on localhost (executor driver) (1/4)
21:06:48.560 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 1.0 (TID 7) in 483 ms on localhost (executor driver) (2/4)
21:06:48.569 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 4) in 494 ms on localhost (executor driver) (3/4)
21:06:48.571 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 1.0 (TID 5) in 495 ms on localhost (executor driver) (4/4)
21:06:48.571 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
21:06:48.572 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (run at ThreadPoolExecutor.java:1149) finished in 0.497 s
21:06:48.580 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed taskresult_7 on 192.168.145.1:8991 in memory (size: 1638.0 KB, free: 1440.9 MB)
21:06:48.581 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed taskresult_5 on 192.168.145.1:8991 in memory (size: 1638.7 KB, free: 1442.5 MB)
21:06:48.583 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed taskresult_6 on 192.168.145.1:8991 in memory (size: 1642.5 KB, free: 1444.1 MB)
21:06:48.584 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed taskresult_4 on 192.168.145.1:8991 in memory (size: 1645.5 KB, free: 1445.7 MB)
21:06:48.588 INFO  [broadcast-exchange-0] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: run at ThreadPoolExecutor.java:1149, took 6.154145 s
21:06:48.673 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.145.1:8991 in memory (size: 2.3 KB, free: 1445.7 MB)
21:06:48.744 INFO  [broadcast-exchange-0] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.2815 ms
21:06:48.835 INFO  [broadcast-exchange-0] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 36.0 MB, free 1409.3 MB)
21:06:48.940 INFO  [broadcast-exchange-0] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.0 MB, free 1405.3 MB)
21:06:48.941 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.145.1:8991 (size: 4.0 MB, free: 1441.7 MB)
21:06:48.944 INFO  [broadcast-exchange-0] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece1 stored as bytes in memory (estimated size 3.5 MB, free 1401.9 MB)
21:06:48.944 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece1 in memory on 192.168.145.1:8991 (size: 3.5 MB, free: 1438.2 MB)
21:06:48.945 INFO  [broadcast-exchange-0] org.apache.spark.SparkContext - Created broadcast 3 from run at ThreadPoolExecutor.java:1149
21:06:48.992 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 31.3635 ms
21:06:49.023 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 25.1231 ms
21:06:49.034 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 310.4 KB, free 1401.6 MB)
21:06:49.056 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 26.7 KB, free 1401.5 MB)
21:06:49.057 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.145.1:8991 (size: 26.7 KB, free: 1438.2 MB)
21:06:49.059 INFO  [main] org.apache.spark.SparkContext - Created broadcast 4 from show at MIDReleaseExposure.scala:50
21:06:49.059 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4765987 bytes, open cost is considered as scanning 4194304 bytes.
21:06:49.086 INFO  [main] org.apache.spark.SparkContext - Starting job: show at MIDReleaseExposure.scala:50
21:06:49.088 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 10 (show at MIDReleaseExposure.scala:50)
21:06:49.088 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 14 (show at MIDReleaseExposure.scala:50)
21:06:49.088 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (show at MIDReleaseExposure.scala:50) with 1 output partitions
21:06:49.088 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 4 (show at MIDReleaseExposure.scala:50)
21:06:49.088 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 3)
21:06:49.088 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 3)
21:06:49.089 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[10] at show at MIDReleaseExposure.scala:50), which has no missing parents
21:06:49.095 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 16.6 KB, free 1401.5 MB)
21:06:49.096 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.1 KB, free 1401.5 MB)
21:06:49.097 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.145.1:8991 (size: 7.1 KB, free: 1438.1 MB)
21:06:49.097 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1004
21:06:49.098 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[10] at show at MIDReleaseExposure.scala:50) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
21:06:49.098 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 4 tasks
21:06:49.099 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 8, localhost, executor driver, partition 0, ANY, 5421 bytes)
21:06:49.100 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 9, localhost, executor driver, partition 1, ANY, 5421 bytes)
21:06:49.100 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 2.0 (TID 10, localhost, executor driver, partition 2, ANY, 5421 bytes)
21:06:49.100 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 2.0 (TID 11, localhost, executor driver, partition 3, ANY, 5421 bytes)
21:06:49.100 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 8)
21:06:49.100 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 9)
21:06:49.100 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 2.0 in stage 2.0 (TID 10)
21:06:49.102 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 3.0 in stage 2.0 (TID 11)
21:06:49.112 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00003-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, range: 0-571693, partition values: [2019-09-06]
21:06:49.116 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00002-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, range: 0-571654, partition values: [2019-09-06]
21:06:49.118 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00001-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, range: 0-571692, partition values: [2019-09-06]
21:06:49.122 INFO  [Executor task launch worker for task 8] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://mycluster/data/release/dw/release_exposure/bdp_day=2019-09-06/part-00000-acbec4ec-87d5-4bd1-a83c-1baf5ee4d914.c000, range: 0-571693, partition values: [2019-09-06]
21:06:49.136 WARN  [Executor task launch worker for task 11] org.apache.parquet.CorruptStatistics - Ignoring statistics because this file was created prior to 1.8.0, see PARQUET-251
21:06:49.136 INFO  [Executor task launch worker for task 11] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(release_session, null)
21:06:49.159 INFO  [Executor task launch worker for task 9] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(release_session, null)
21:06:49.190 INFO  [Executor task launch worker for task 10] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(release_session, null)
21:06:49.248 INFO  [Executor task launch worker for task 8] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(release_session, null)
21:06:49.607 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 3.0 in stage 2.0 (TID 11). 1587 bytes result sent to driver
21:06:49.608 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 2.0 (TID 11) in 508 ms on localhost (executor driver) (1/4)
21:06:49.615 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 1.0 in stage 2.0 (TID 9). 1544 bytes result sent to driver
21:06:49.616 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 9) in 517 ms on localhost (executor driver) (2/4)
21:06:49.620 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 2.0 in stage 2.0 (TID 10). 1587 bytes result sent to driver
21:06:49.621 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 2.0 (TID 10) in 521 ms on localhost (executor driver) (3/4)
21:06:49.625 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 8). 1544 bytes result sent to driver
21:06:49.626 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 8) in 527 ms on localhost (executor driver) (4/4)
21:06:49.626 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
21:06:49.628 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (show at MIDReleaseExposure.scala:50) finished in 0.528 s
21:06:49.628 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
21:06:49.628 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
21:06:49.628 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 3, ResultStage 4)
21:06:49.628 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
21:06:49.628 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 3 (MapPartitionsRDD[14] at show at MIDReleaseExposure.scala:50), which has no missing parents
21:06:49.632 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 16.4 KB, free 1401.5 MB)
21:06:49.634 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 6.9 KB, free 1401.5 MB)
21:06:49.635 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 192.168.145.1:8991 (size: 6.9 KB, free: 1438.1 MB)
21:06:49.636 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1004
21:06:49.636 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[14] at show at MIDReleaseExposure.scala:50) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
21:06:49.636 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 4 tasks
21:06:49.637 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 12, localhost, executor driver, partition 0, ANY, 4715 bytes)
21:06:49.638 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 13, localhost, executor driver, partition 1, ANY, 4715 bytes)
21:06:49.638 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 14, localhost, executor driver, partition 2, ANY, 4715 bytes)
21:06:49.639 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 3.0 (TID 15, localhost, executor driver, partition 3, ANY, 4715 bytes)
21:06:49.639 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 12)
21:06:49.639 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 13)
21:06:49.641 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 14)
21:06:49.641 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Running task 3.0 in stage 3.0 (TID 15)
21:06:49.643 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 non-empty blocks out of 4 blocks
21:06:49.643 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
21:06:49.648 INFO  [Executor task launch worker for task 15] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 non-empty blocks out of 4 blocks
21:06:49.648 INFO  [Executor task launch worker for task 15] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
21:06:49.649 INFO  [Executor task launch worker for task 13] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 non-empty blocks out of 4 blocks
21:06:49.649 INFO  [Executor task launch worker for task 13] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
21:06:49.651 INFO  [Executor task launch worker for task 14] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 non-empty blocks out of 4 blocks
21:06:49.651 INFO  [Executor task launch worker for task 14] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
21:06:49.889 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 12). 1740 bytes result sent to driver
21:06:49.889 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Finished task 3.0 in stage 3.0 (TID 15). 1697 bytes result sent to driver
21:06:49.892 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 12) in 255 ms on localhost (executor driver) (1/4)
21:06:49.893 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 3.0 (TID 15) in 255 ms on localhost (executor driver) (2/4)
21:06:49.901 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 13). 1697 bytes result sent to driver
21:06:49.904 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 13) in 267 ms on localhost (executor driver) (3/4)
21:06:49.920 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 14). 1697 bytes result sent to driver
21:06:49.920 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 14) in 282 ms on localhost (executor driver) (4/4)
21:06:49.920 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
21:06:49.921 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 3 (show at MIDReleaseExposure.scala:50) finished in 0.284 s
21:06:49.921 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
21:06:49.921 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
21:06:49.921 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 4)
21:06:49.921 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
21:06:49.922 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 4 (MapPartitionsRDD[16] at show at MIDReleaseExposure.scala:50), which has no missing parents
21:06:49.925 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 3.7 KB, free 1401.5 MB)
21:06:49.927 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1401.5 MB)
21:06:49.928 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on 192.168.145.1:8991 (size: 2.2 KB, free: 1438.1 MB)
21:06:49.928 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1004
21:06:49.928 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[16] at show at MIDReleaseExposure.scala:50) (first 15 tasks are for partitions Vector(0))
21:06:49.928 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
21:06:49.929 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 16, localhost, executor driver, partition 0, ANY, 4726 bytes)
21:06:49.929 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 16)
21:06:49.932 INFO  [Executor task launch worker for task 16] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 non-empty blocks out of 4 blocks
21:06:49.932 INFO  [Executor task launch worker for task 16] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
21:06:49.940 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 16). 2468 bytes result sent to driver
21:06:49.944 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 16) in 16 ms on localhost (executor driver) (1/1)
21:06:49.944 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
21:06:49.945 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 4 (show at MIDReleaseExposure.scala:50) finished in 0.017 s
21:06:49.946 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: show at MIDReleaseExposure.scala:50, took 0.859936 s
21:06:49.969 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@3fc08eec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
21:06:49.972 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.145.1:4040
21:06:49.985 INFO  [dispatcher-event-loop-0] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
21:06:50.035 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
21:06:50.035 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
21:06:50.036 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
21:06:50.040 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
21:06:50.043 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
21:06:50.050 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
21:06:50.051 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\Yongning Y\AppData\Local\Temp\spark-e46b099f-782f-4c4c-bd77-8e9d102f9ad8
