12:50:23.332 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
12:50:24.963 INFO  [main] org.apache.spark.SparkContext - Submitted application: ods_user_job
12:50:25.104 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: YongningY,root
12:50:25.104 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: YongningY,root
12:50:25.108 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
12:50:25.108 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
12:50:25.116 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(YongningY, root); groups with view permissions: Set(); users  with modify permissions: Set(YongningY, root); groups with modify permissions: Set()
12:50:26.306 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 4547.
12:50:26.482 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
12:50:26.669 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
12:50:26.689 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
12:50:26.689 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
12:50:26.732 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\Yongning Y\AppData\Local\Temp\blockmgr-f7b00478-94be-44a8-b573-3e7774ee97a8
12:50:26.943 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1445.7 MB
12:50:27.189 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
12:50:27.481 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @8286ms
12:50:27.712 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
12:50:27.731 INFO  [main] org.spark_project.jetty.server.Server - Started @8544ms
12:50:27.774 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@4d4d48a6{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
12:50:27.778 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
12:50:27.813 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51bde877{/jobs,null,AVAILABLE,@Spark}
12:50:27.813 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2bffa76d{/jobs/json,null,AVAILABLE,@Spark}
12:50:27.813 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d2260db{/jobs/job,null,AVAILABLE,@Spark}
12:50:27.813 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ee55e70{/jobs/job/json,null,AVAILABLE,@Spark}
12:50:27.817 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7668d560{/stages,null,AVAILABLE,@Spark}
12:50:27.817 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@126be319{/stages/json,null,AVAILABLE,@Spark}
12:50:27.817 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c371e13{/stages/stage,null,AVAILABLE,@Spark}
12:50:27.821 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36b6964d{/stages/stage/json,null,AVAILABLE,@Spark}
12:50:27.821 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9257031{/stages/pool,null,AVAILABLE,@Spark}
12:50:27.825 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7726e185{/stages/pool/json,null,AVAILABLE,@Spark}
12:50:27.825 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@282308c3{/storage,null,AVAILABLE,@Spark}
12:50:27.825 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1db0ec27{/storage/json,null,AVAILABLE,@Spark}
12:50:27.829 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d4ab71a{/storage/rdd,null,AVAILABLE,@Spark}
12:50:27.829 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1af05b03{/storage/rdd/json,null,AVAILABLE,@Spark}
12:50:27.829 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ad777f{/environment,null,AVAILABLE,@Spark}
12:50:27.833 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@438bad7c{/environment/json,null,AVAILABLE,@Spark}
12:50:27.833 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4fdf8f12{/executors,null,AVAILABLE,@Spark}
12:50:27.833 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54f5f647{/executors/json,null,AVAILABLE,@Spark}
12:50:27.833 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a6d5a8f{/executors/threadDump,null,AVAILABLE,@Spark}
12:50:27.837 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@315ba14a{/executors/threadDump/json,null,AVAILABLE,@Spark}
12:50:27.868 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27f0ad19{/static,null,AVAILABLE,@Spark}
12:50:27.868 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6105f8a3{/,null,AVAILABLE,@Spark}
12:50:27.868 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77e2a6e2{/api,null,AVAILABLE,@Spark}
12:50:27.872 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bb3d42d{/jobs/job/kill,null,AVAILABLE,@Spark}
12:50:27.872 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/stages/stage/kill,null,AVAILABLE,@Spark}
12:50:27.872 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.145.1:4040
12:50:28.368 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
12:50:28.450 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 4568.
12:50:28.450 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.145.1:4568
12:50:28.465 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
12:50:28.524 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.145.1, 4568, None)
12:50:28.539 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.145.1:4568 with 1445.7 MB RAM, BlockManagerId(driver, 192.168.145.1, 4568, None)
12:50:28.547 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.145.1, 4568, None)
12:50:28.547 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.145.1, 4568, None)
12:50:28.977 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@392a04e7{/metrics/json,null,AVAILABLE,@Spark}
12:50:29.188 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/H:/IdeaProject/release_test/target/classes/hive-site.xml
12:50:29.273 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
12:50:29.273 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/user/hive/warehouse'.
12:50:29.309 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@736f3e9e{/SQL,null,AVAILABLE,@Spark}
12:50:29.309 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f443fae{/SQL/json,null,AVAILABLE,@Spark}
12:50:29.309 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@87abc48{/SQL/execution,null,AVAILABLE,@Spark}
12:50:29.309 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@782168b7{/SQL/execution/json,null,AVAILABLE,@Spark}
12:50:29.320 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65bb9029{/static/sql,null,AVAILABLE,@Spark}
12:50:31.526 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
12:50:32.612 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
12:50:32.662 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
12:50:34.361 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
12:50:36.051 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
12:50:36.063 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
12:50:36.441 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
12:50:36.445 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
12:50:36.519 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
12:50:36.726 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
12:50:36.736 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
12:50:36.769 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=azkaban pat=*
12:50:36.769 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=azkaban pat=*	
12:50:36.832 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dim pat=*
12:50:36.832 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dim pat=*	
12:50:36.836 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dm pat=*
12:50:36.836 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dm pat=*	
12:50:36.838 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dwd pat=*
12:50:36.838 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dwd pat=*	
12:50:36.842 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_dws pat=*
12:50:36.842 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_dws pat=*	
12:50:36.844 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=bbp_ods pat=*
12:50:36.846 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=bbp_ods pat=*	
12:50:36.848 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
12:50:36.848 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
12:50:36.852 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_release pat=*
12:50:36.852 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_release pat=*	
12:50:36.856 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dim_shop pat=*
12:50:36.856 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dim_shop pat=*	
12:50:36.858 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
12:50:36.858 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
12:50:36.862 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_shop pat=*
12:50:36.862 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dm_shop pat=*	
12:50:36.864 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
12:50:36.866 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
12:50:36.870 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_shop pat=*
12:50:36.870 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=dw_shop pat=*	
12:50:36.872 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_release pat=*
12:50:36.872 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_release pat=*	
12:50:36.876 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=mid_shop pat=*
12:50:36.876 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=mid_shop pat=*	
12:50:36.880 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
12:50:36.880 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
12:50:36.882 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_shop pat=*
12:50:36.884 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_shop pat=*	
12:50:36.886 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_test pat=*
12:50:36.886 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=ods_test pat=*	
12:50:36.888 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dim pat=*
12:50:36.888 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dim pat=*	
12:50:36.892 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dm pat=*
12:50:36.892 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dm pat=*	
12:50:36.895 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_dw pat=*
12:50:36.895 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_dw pat=*	
12:50:36.899 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sales_rds pat=*
12:50:36.899 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=sales_rds pat=*	
12:50:36.901 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=te pat=*
12:50:36.901 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=te pat=*	
12:50:36.905 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test pat=*
12:50:36.905 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test pat=*	
12:50:36.907 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test4 pat=*
12:50:36.907 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=test4 pat=*	
12:50:43.901 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/1aac43f1-ffe6-4f94-b3a5-e56cb8e62fd7_resources
12:50:43.931 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/1aac43f1-ffe6-4f94-b3a5-e56cb8e62fd7
12:50:43.937 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/1aac43f1-ffe6-4f94-b3a5-e56cb8e62fd7
12:50:43.946 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/1aac43f1-ffe6-4f94-b3a5-e56cb8e62fd7/_tmp_space.db
12:50:43.960 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
12:50:44.019 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for root: Unknown error.
12:50:44.088 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
12:50:44.088 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
12:50:44.096 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
12:50:44.096 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
12:50:44.102 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
12:50:44.474 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/cfb85baa-0e7e-4de1-a862-d6fca17600f8_resources
12:50:44.480 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/cfb85baa-0e7e-4de1-a862-d6fca17600f8
12:50:44.484 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/YONGNI~1/AppData/Local/Temp/YongningY/cfb85baa-0e7e-4de1-a862-d6fca17600f8
12:50:44.492 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/root/cfb85baa-0e7e-4de1-a862-d6fca17600f8/_tmp_space.db
12:50:44.494 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse
12:50:44.872 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
12:50:46.766 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_test.ods_user_action_log
12:50:47.338 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_test
12:50:47.338 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_database: ods_test	
12:50:47.360 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_test tbl=ods_user_action_log
12:50:47.360 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_test tbl=ods_user_action_log	
12:50:47.582 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_test tbl=ods_user_action_log
12:50:47.582 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=ods_test tbl=ods_user_action_log	
12:50:47.623 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
12:50:47.634 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
12:50:47.637 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
12:50:47.637 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
12:50:47.637 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
12:50:47.708 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
12:50:48.663 ERROR [main] com.qf.bigdata.release.etl.release.dm.DMExposureSources$ - cannot resolve '`2019-09-12`' given input columns: [action_target, action, ct, bdp_day, users];;
'Filter ('2019-09-12 = 2019-09-12)
+- SubqueryAlias ods_user_action_log
   +- HiveTableRelation `ods_test`.`ods_user_action_log`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [users#0, action#1, action_target#2, ct#3, bdp_day#4]

org.apache.spark.sql.AnalysisException: cannot resolve '`2019-09-12`' given input columns: [action_target, action, ct, bdp_day, users];;
'Filter ('2019-09-12 = 2019-09-12)
+- SubqueryAlias ods_user_action_log
   +- HiveTableRelation `ods_test`.`ods_user_action_log`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [users#0, action#1, action_target#2, ct#3, bdp_day#4]

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:165)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:171)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:62)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:2893)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1305)
	at org.apache.spark.sql.Dataset.where(Dataset.scala:1333)
	at com.qf.bigdata.release.etl.release.ods.ODSUser$.handleReleaseJob(ODSUser.scala:39)
	at com.qf.bigdata.release.etl.release.ods.ODSUser$$anonfun$handleJobs$1.apply(ODSUser.scala:80)
	at com.qf.bigdata.release.etl.release.ods.ODSUser$$anonfun$handleJobs$1.apply(ODSUser.scala:78)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at com.qf.bigdata.release.etl.release.ods.ODSUser$.handleJobs(ODSUser.scala:78)
	at com.qf.bigdata.release.etl.release.ods.ODSUser$.main(ODSUser.scala:100)
	at com.qf.bigdata.release.etl.release.ods.ODSUser.main(ODSUser.scala)
12:50:48.711 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@4d4d48a6{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
12:50:48.722 INFO  [main] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.145.1:4040
12:50:48.818 INFO  [dispatcher-event-loop-1] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
12:50:48.837 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
12:50:48.840 INFO  [main] org.apache.spark.storage.BlockManager - BlockManager stopped
12:50:48.848 INFO  [main] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
12:50:48.851 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
12:50:48.855 INFO  [main] org.apache.spark.SparkContext - Successfully stopped SparkContext
12:50:48.874 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
12:50:48.874 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\Yongning Y\AppData\Local\Temp\spark-f849281c-8678-4a5d-9d1d-595379fe4516
